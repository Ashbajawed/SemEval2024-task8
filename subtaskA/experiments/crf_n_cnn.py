# -*- coding: utf-8 -*-
"""baseline_with_CRF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sax--EyAw-mSYDNGcqnKKVScs-yO5UHl
"""



# # Commented out IPython magic to ensure Python compatibility.
# !git clone https://github.com/mbzuai-nlp/SemEval2024-task8.git
# # %cd SemEval2024-task8
# !pip install gdown

# !gdown --folder https://drive.google.com/drive/folders/1CAbb3DjrOPBNm0ozVBfhvrEh9P9rAppc

# !pip install datasets  evaluate transformers[torch] nltk

# import nltk
# nltk.download('stopwords')
# nltk.download('wordnet')

# !pip install pytorch-crf







import torch.nn.functional as F
import torch.nn as nn
import torch
from datasets import Dataset
import pandas as pd
import evaluate
import numpy as np
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed
import os
from sklearn.model_selection import train_test_split
from scipy.special import softmax
import argparse
import logging
import time
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
stopwords = stopwords.words('english')
import time
from torchcrf import CRF

def preprocess_function(examples, **fn_kwargs):
    return fn_kwargs['tokenizer'](examples["text"], truncation=True)
def lemmatize(text):
  lemmatizer = WordNetLemmatizer()
  r = re.sub('[^a-zA-Z]', ' ', text)
  r = r.lower()
  r= r.split()
  r = [word for word in r if word not in stopwords]
  r = [lemmatizer.lemmatize(word) for word in r]
  r = ' '.join(r)
  return r


def add_features(df):
  # df['char_count'] = df['text'].apply(len) # Number of characters in the string
  # df['word_count'] = df['text'].apply(lambda x: len(x.split())) # Number of words in the string
  # df['word_density'] = df['char_count'] / (df['word_count']+1) # Density of word (in char)
  # df['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))
  df['text']= df['text'].apply(lemmatize)
  # df['modified_word_count'] = df['text'].apply(lambda x: len(x.split())) # Number of words in the string
  return df

def get_data(train_path, test_path, random_seed):
    """
    function to read dataframe with columns
    """
    train_df = pd.read_json(train_path, lines=True)
    test_df = pd.read_json(test_path, lines=True)
    train_df, test_df= add_features(train_df), add_features(test_df)

    train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=random_seed)

    return train_df, val_df, test_df

def compute_metrics(eval_pred):

    f1_metric = evaluate.load("f1")

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    results = {}
    results.update(f1_metric.compute(predictions=predictions, references = labels, average="micro"))

    return results


def test(test_df, model_path, id2label, label2id):

    # load tokenizer from saved model
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # load best model
    model = AutoModelForSequenceClassification.from_pretrained(
       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id
    )

    test_dataset = Dataset.from_pandas(test_df)

    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # create Trainer
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    # get logits from predictions and evaluate results using classification report
    predictions = trainer.predict(tokenized_test_dataset)
    preds = np.argmax(predictions.predictions, axis=-1)
    metric = evaluate.load("bstrai/classification_report")
    results = metric.compute(predictions=preds, references=predictions.label_ids)

    # return dictionary of classification report
    return results, preds

"""
# Define a custom CRF model with CNN
class CRFModelWithCNN(AutoModelForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = len(label2id)
        self.convs = nn.ModuleList([
             nn.Conv1d(config.hidden_size, 128, kernel_size=3, padding=1),
            nn.Conv1d(config.hidden_size, 128, kernel_size=5, padding=2),
        ])
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.dropout = nn.Dropout(0.2)



#        self.conv1d = nn.Conv1d(in_channels=config.hidden_size, out_channels=256, kernel_size=3, padding=1)
#        self.relu = nn.ReLU()
 #       self.pooling = nn.MaxPool1d(kernel_size=2)
        self.crf = CRF(num_tags=len(label2id))

    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = super().forward(input_ids, attention_mask=attention_mask)
        hidden_states = outputs[0]

        conved_outputs = []
        for conv in self.convs:
            conv_output = conv(hidden_states.transpose(1, 2))
            conved_outputs.append(conv_output.transpose(1, 2))

        pooled_outputs = []
        for conv_output in conved_outputs:
            pooled_output = self.pool(self.dropout(conv_output))
            pooled_outputs.append(pooled_output)

        conv_out = torch.cat(pooled_outputs, dim=1)

   #     outputs = self.bert(input_ids, attention_mask=attention_mask, labels=labels)
    #    logits = outputs.logits

#        conv_out = self.conv1d(logits)
 #       conv_out = self.relu(conv_out)
  #      conv_out = self.pooling(conv_out)
   #     conv_out = conv_out.permute(0, 2, 1)  # Change shape back
        # Apply CRF layer
        loss = self.crf(conv_out, labels, attention_mask)

        return {"loss": loss}

"""

class CRFModelWithCNN(AutoModelForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = len(label2id)
        self.convs = nn.ModuleList([
            nn.Conv1d(config.hidden_size, 128, kernel_size=3, padding=1),
            nn.Conv1d(config.hidden_size, 128, kernel_size=5, padding=2),
        ])
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.dropout = nn.Dropout(0.2)
        self.crf = CRF(self.num_labels)  # Add CRF layer
        self.classifier = nn.Linear(128 * 2, self.num_labels)

    def forward(self, input_ids, attention_mask=None):
        outputs = super().forward(input_ids, attention_mask=attention_mask)
        hidden_states = outputs[0]

        conved_outputs = []
        for conv in self.convs:
            conv_output = conv(hidden_states.transpose(1, 2))
            conved_outputs.append(conv_output.transpose(1, 2))

        pooled_outputs = []
        for conv_output in conved_outputs:
            pooled_output = self.pool(self.dropout(conv_output))
            pooled_outputs.append(pooled_output)

        pooled_output = torch.cat(pooled_outputs, dim=1)
        emissions = self.classifier(pooled_output)

        # Use CRF layer
        mask = attention_mask.bool() if attention_mask is not None else None
        tags = self.crf.decode(emissions, mask=mask)
        
        return {"logits": emissions, "tags": tags}
def fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model):
    # pandas dataframe to huggingface Dataset
    train_dataset = Dataset.from_pandas(train_df)
    valid_dataset = Dataset.from_pandas(valid_df)
    if os.path.exists(model):
        # If it exists, add a timestamp or a unique identifier
        timestamp = time.strftime("%Y%m%d%H%M%S")
        os.rename(model,  f"{model}_{timestamp}")
    # get tokenizer and model from huggingface
    tokenizer = AutoTokenizer.from_pretrained(model)     # put your model here


    # model = AutoModelForSequenceClassification.from_pretrained(
    #    model, num_labels=len(label2id), id2label=id2label, label2id=label2id
    # )    # put your model here

    model = CRFModelWithCNN.from_pretrained(model)

    # tokenize data for train/valid
    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})
    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})


    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


    # create Trainer
    training_args = TrainingArguments(
        output_dir=checkpoints_path,
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        weight_decay=0.01,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )


    # create Trainer with CRF model
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_dataset,
        eval_dataset=tokenized_valid_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    # save best model
    best_model_path = checkpoints_path+'/best/'

    if not os.path.exists(best_model_path):
        os.makedirs(best_model_path)


    trainer.save_model(best_model_path)

train_path =train_file_path= '/home/tooba.sheikh/ashba/SemEval2024-task8/jsons/SubtaskA/subtaskA_train_monolingual.jsonl'
test_path= test_file_path='/home/tooba.sheikh/ashba/SemEval2024-task8/jsons/SubtaskA/subtaskA_dev_monolingual.jsonl'
subtask='A'
model='xlm-roberta-base'
prediction_path= prediction_file_path = 'pred_files/crf_n_cnn.json'
random_seed = 0

id2label = {0: "human", 1: "machine"}
label2id = {"human": 0, "machine": 1}

train_df, valid_df, test_df = get_data(train_path, test_path, random_seed)


PRED= prediction_path.split(".")[0].split("/")[-1]
fine_tune(train_df, valid_df, f"checkpoints/{model}_{PRED}/subtask{subtask}/{random_seed}", id2label, label2id, model)
# test detector model
results, predictions = test(test_df, f"checkpoints/{model}_{PRED}/subtask{subtask}/{random_seed}/best/", id2label, label2id)

logging.info(results)
predictions_df = pd.DataFrame({'id': test_df['id'], 'label': predictions})
predictions_df.to_json(prediction_path, lines=True, orient='records')

