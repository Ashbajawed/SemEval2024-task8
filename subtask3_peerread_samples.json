[
    {
        "id": "104",
        "source": "PeerRead/acl_2017/",
        "title": "Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding",
        "human_review": "- Strengths:\n* Outperforms ALIGN in supervised entity linking task which suggests that the\nproposed framework improves representations of text and knowledge that are\nlearned jointly.\n* Direct comparison with closely related approach using very similar input\ndata.\n* Analysis of the smoothing parameter provides useful analysis since impact of\npopularity is a persistent issue in entity linking.\n\n- Weaknesses:\n* Comparison with ALIGN could be better. ALIGN used content window size 10 vs\nthis paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear\nto me whether N(e_j) includes only entities that link to e_j. The graph is\ndirected and consists of wikipedia outlinks, but is adjacency defined as it\nwould be for an undirected graph? For ALIGN, the context of an entity is the\nset of entities that link to that entity. If N(e_j) is different, we cannot\ntell how much impact this change has on the learned vectors, and this could\ncontribute to the difference in scores on the entity similarity task. \n* It is sometimes difficult to follow whether \"mention\" means a string type, or\na particular mention in a particular document. The phrase \"mention embedding\"\nis used, but it appears that embeddings are only learned for mention senses.\n* It is difficult to determine the impact of sense disambiguation order without\ncomparison to other unsupervised entity linking methods. \n\n- General Discussion:\nThis paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nWikipedia).  (Its title and introduction are a little overblown/misleading,\nsince there is a lot more to bridging text and knowledge than the EDL task, but\nEDL is a core part of the overall task nonetheless.)  The method is to perform\nthis bridging via an intermediate layer of representation, namely mention\nsenses, thus following two steps: (1) mention to mention sense, and (2) mention\nsense to entity.  Various embedding representations are learned for the words,\nthe mention senses, and the entities, which are then jointly trained to\nmaximize a single overall objective function that maximizes all three types of\nembedding equally.  \n\nTechnically the approach is fairly clear and conforms to the current deep\nprocessing fashion and known best practices regarding embeddings; while one can\nsuggest all kinds of alternatives, it\u2019s not clear they would make a material\ndifference.  Rather, my comments focus on the basic approach.  It is not\nexplained, however, exactly why a two-step process, involving the mention\nsenses, is better than a simple direct one-step mapping from word mentions to\ntheir entities.  (This is the approach of Yamada et al., in what is called here\nthe ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its\nsimplification SPME) do better.  By why, exactly?  What is the exact\ndifference, and additional information, that the mention senses have compare4ed\nto the entities?  To understand, please check if the following is correct (and\nperhaps update the paper to make it exactly clear what is going on).  \n\nFor entities: their profiles consist of neighboring entities in a relatedness\ngraph.                    This graph is built (I assume) by looking at word-level\nrelatedness of\nthe entity definitions (pages in Wikipedia).  The profiles are (extended\nskip-gram-based) embeddings.  \n\nFor words: their profiles are the standard distributional semantics approach,\nwithout sense disambiguation.  \n\nFor mention senses: their profiles are the standard distributional semantics\napproach, but WITH sense disambiguation.  Sense disambiguation is performed\nusing a sense-based profile (\u2018language model\u2019) from local context words and\nneighboring mentions, as mentioned briefly just before Section 4, but without\ndetails.  This is a problem point in the approach.  How exactly are the senses\ncreated and differentiated?  Who defines how many senses a mention string can\nhave?  If this is done by looking at the knowledge base, then we get a\nbijective mapping between mention senses and entities -\u2013 that is, there is\nexactly one entity for each mention sense (even if there may be more entities).\n In that case, are the sense collection\u2019s definitional profiles built\nstarting with entity text as \u2018seed words\u2019?                    If so, what\ninformation\nis used\nat the mention sense level that is NOT used at the entity level?  Just and\nexactly the words in the texts that reliably associate with the mention sense,\nbut that do NOT occur in the equivalent entity webpage in Wikipedia?  How many\nsuch words are there, on average, for a mention sense?                    That is,\nhow\npowerful/necessary is it to keep this extra differentiation information in a\nseparate space (the mention sense space) as opposed to just loading these\nadditional words into the Entity space (by adding these words into the\nWikipedia entity pages)?  \n\nIf the above understanding is essentially correct, please update Section 5 of\nthe paper to say so, for (to me) it is the main new information in the paper.  \n\nIt is not true, as the paper says in Section 6, that \u201c\u2026this is the first\nwork to deal with mention ambiguity in the integration of text and knowledge\nrepresentations, so there is no exact baselines for comparison\u201d.  The TAC KBP\nevaluations for the past two years have hosted EDL tasks, involving eight or\nnine systems, all performing exactly this task, albeit against Freebase, which\nis considerably larger and more noisy than Wikipedia.  Please see\nhttp://nlp.cs.rpi.edu/kbp/2016/ .  \n\nOn a positive note: I really liked the idea of the smoothing parameter in\nSection 6.4.2.\n\nPost-response: I have read the authors' responses.  I am not really satisfied\nwith their reply about the KBP evaluation not being relevant, but that they are\ninterested in the goodness of the embeddings instead.  In fact, the only way to\nevaluate such 'goodness' is through an application.  No-one really cares how\nconceptually elegant an embedding is, the question is: does it perform better?\n- Strengths:\nGood ideas, simple neural learning, interesting performance (altough not\nstriking) and finally large set of applications.\n\n- Weaknesses: amount of novel content. Clarity in some sections. \n\nThe paper presents a neural learning method for entity disambiguation and\nlinking. It introduces a good idea to integrate entity, mention and sense\nmodeling within the smame neural language modeling technique. The simple\ntraining procedure connected with the modeling allows to support a large set of\napplication.\n\nThe paper is clear formally, but the discussion is not always at the same level\nof the technical ideas.\n\nThe empirical evaluation is good although not striking improvements of the\nperformance are reported. Although it seems an extension of (Yamada et al.,\nCoNLL 2016), it adds novel ideas and it is of a releant interest.\n\nThe weaker points of the paper are:\n\n- The prose is not always clear. I found Section 3 not as clear. Some details\nof Figure 2 are not explained and the terminology is somehow redundant: for\nexample, why do you refer to the dictionary of mentions? or the dictionary of\nentity-mention pairs? are these different from text anchors and types for\nannotated text anchors?\n- Tha paper is quite close in nature to Yamada et al., 2016) and the authors\nshould at least outline the differences.\n\nOne general observation on the current version is:\nThe paper tests the Multiple Embedding model against entity\nlinking/disambiguation tasks. However, word embeddings are not only used to\nmodel such tasks, but also some processes not directly depending on entities of\nthe KB, e.g. parsing, coreference or semantic role labeling. \nThe authors should show that the word embeddings provided by the proposed MPME\nmethod are not weaker wrt to simpler wordspaces in such other semantic tasks,\ni.e. those involving directly entity mentions.\n\nI did read the author's response.",
        "partial_review": "- Strengths:\n* Outperforms ALIGN in supervised entity linking task which suggests that the\nproposed framework improves representations of text and knowledge that are\nlearned jointly.\n* Direct comparison with closely related approach using very similar input\ndata.\n* Analysis of the smoothing parameter provides useful analysis since impact of\npopularity is a persistent issue in entity linking.\n\n- Weaknesses:\n* Comparison with ALIGN could be better. ALIGN used content window size 10 vs\nthis paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear\nto me whether N(e_j) includes only entities that link to e_j. The graph is\ndirected and consists of wikipedia outlinks, but is adjacency defined as it\nwould be for an undirected graph? For ALIGN, the context of an entity is the\nset of entities that link to that entity. If N(e_j) is different, we cannot\ntell how much impact this change has on the learned vectors, and this could\ncontribute to the difference in scores on the entity similarity task. \n* It is sometimes difficult to follow whether \"mention\" means a string type, or\na particular mention in a particular document. The phrase \"mention embedding\"\nis used, but it appears that embeddings are only learned for mention senses.\n* It is difficult to determine the impact of sense disambiguation order without\ncomparison to other unsupervised entity linking methods. \n\n- General Discussion:\nThis paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nWikipedia).  (Its title and introduction are a little overblown/misleading,\nsince there is a lot more to bridging text and knowledge than the EDL task, but\nEDL is a core part of the overall task nonetheless.)  The method is to perform\nthis bridging via an intermediate layer of representation, namely mention\nsenses, thus following two steps: (1) mention to mention sense, and (2) mention\nsense to entity.  Various embedding representations are learned for the words,\nthe mention senses, and the entities, which are then jointly trained to\nmaximize a single overall objective function that maximizes all three types of\nembedding equally.  \n\nTechnically the approach is fairly clear and conforms to the current deep\nprocessing fashion and known best practices regarding embeddings; while one can\nsuggest all kinds of alternatives, it\u2019s not clear they would make a material\ndifference.  Rather, my comments focus on the basic approach.  It is not\nexplained, however, exactly why a two-step process, involving the mention\nsenses, is better than a simple direct one-step mapping from word mentions to\ntheir entities.  (This is the approach of Yamada et al., in what is called here\nthe ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its\nsimplification SPME) do better.  By why, exactly?  What is the exact\ndifference, and additional information, that the mention senses have compare4ed\nto the entities?  To understand, please check if the following is correct (and\nperhaps update the paper to make it exactly clear what is going on).  \n\nFor entities: their profiles consist of neighboring entities in a relatedness\ngraph.                    This graph is built (I assume) by looking at word-level\nrelatedness of\nthe entity definitions (pages in Wikipedia).  The profiles are (extended\nskip-gram-based) embeddings.  \n\nFor words: their profiles are the standard distributional semantics approach,\nwithout sense disambiguation.  \n\nFor mention senses: their profiles are the standard distributional semantics\napproach, but WITH sense disambiguation.  Sense disambiguation is performed\nusing a sense-based profile (\u2018language model\u2019) from local context words and\nneighboring mentions, as mentioned briefly just before Section 4, but without\ndetails.  This is a problem point in the approach.  How exactly are the senses\nc",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding \n\nHere is the abstract of the paper: Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance. \n\nHere is the partial review: - Strengths:\n* Outperforms ALIGN in supervised entity linking task which suggests that the\nproposed framework improves representations of text and knowledge that are\nlearned jointly.\n* Direct comparison with closely related approach using very similar input\ndata.\n* Analysis of the smoothing parameter provides useful analysis since impact of\npopularity is a persistent issue in entity linking.\n\n- Weaknesses:\n* Comparison with ALIGN could be better. ALIGN used content window size 10 vs\nthis paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear\nto me whether N(e_j) includes only entities that link to e_j. The graph is\ndirected and consists of wikipedia outlinks, but is adjacency defined as it\nwould be for an undirected graph? For ALIGN, the context of an entity is the\nset of entities that link to that entity. If N(e_j) is different, we cannot\ntell how much impact this change has on the learned vectors, and this could\ncontribute to the difference in scores on the entity similarity task. \n* It is sometimes difficult to follow whether \"mention\" means a string type, or\na particular mention in a particular document. The phrase \"mention embedding\"\nis used, but it appears that embeddings are only learned for mention senses.\n* It is difficult to determine the impact of sense disambiguation order without\ncomparison to other unsupervised entity linking methods. \n\n- General Discussion:\nThis paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nWikipedia).  (Its title and introduction are a little overblown/misleading,\nsince there is a lot more to bridging text and knowledge than the EDL task, but\nEDL is a core part of the overall task nonetheless.)  The method is to perform\nthis bridging via an intermediate layer of representation, namely mention\nsenses, thus following two steps: (1) mention to mention sense, and (2) mention\nsense to entity.  Various embedding representations are learned for the words,\nthe mention senses, and the entities, which are then jointly trained to\nmaximize a single overall objective function that maximizes all three types of\nembedding equally.  \n\nTechnically the approach is fairly clear and conforms to the current deep\nprocessing fashion and known best practices regarding embeddings; while one can\nsuggest all kinds of alternatives, it\u2019s not clear they would make a material\ndifference.  Rather, my comments focus on the basic approach.  It is not\nexplained, however, exactly why a two-step process, involving the mention\nsenses, is better than a simple direct one-step mapping from word mentions to\ntheir entities.  (This is the approach of Yamada et al., in what is called here\nthe ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its\nsimplification SPME) do better.  By why, exactly?  What is the exact\ndifference, and additional information, that the mention senses have compare4ed\nto the entities?  To understand, please check if the following is correct (and\nperhaps update the paper to make it exactly clear what is going on).  \n\nFor entities: their profiles consist of neighboring entities in a relatedness\ngraph.                    This graph is built (I assume) by looking at word-level\nrelatedness of\nthe entity definitions (pages in Wikipedia).  The profiles are (extended\nskip-gram-based) embeddings.  \n\nFor words: their profiles are the standard distributional semantics approach,\nwithout sense disambiguation.  \n\nFor mention senses: their profiles are the standard distributional semantics\napproach, but WITH sense disambiguation.  Sense disambiguation is performed\nusing a sense-based profile (\u2018language model\u2019) from local context words and\nneighboring mentions, as mentioned briefly just before Section 4, but without\ndetails.  This is a problem point in the approach.  How exactly are the senses\nc \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The strengths of this paper include the improvement in supervised entity linking task compared to the ALIGN algorithm, the direct comparison with a closely related approach, and the analysis of the smoothing parameter to address the issue of popularity in entity linking. However, there are some weaknesses that need to be addressed. Firstly, the comparison with ALIGN could be better by providing more details about the differences in the input data and the impact of those changes on the learned vectors. It is also unclear whether N(e_j) includes only entities that link to e_j and how the adjacency for the graph is defined. The meaning of 'mention' is sometimes confusing, as it is not clear whether it represents a string type or a particular mention in a document. Additionally, it is difficult to understand the impact of sense disambiguation order without comparing it to other unsupervised entity linking methods. The general discussion on the approach is clear and the technical aspects conform to current best practices. However, it is not explained why the two-step process involving mention senses is better than a direct one-step mapping from word mentions to entities. More clarity on the exact difference and additional information that mention senses provide compared to entities is needed. The paper also lacks details on how sense disambiguation is performed using the sense-based profile from local context words and neighboring mentions. Overall, the paper has strengths but needs some improvements in terms of comparisons, clarity, and providing more details on sense disambiguation.\"\n}",
        "human_end_index": 3889
    },
    {
        "id": 105,
        "source": "PeerRead/acl_2017/",
        "title": "Morphological Inflection Generation with Hard Monotonic Attention",
        "human_review": "- Strengths:\nThe idea of hard monotonic attention is new and substantially different from\nothers.\n\n- Weaknesses:\nThe experiment results on morphological inflection generation is somewhat\nmixed. The proposed model is effective if the amount of training data is small\n(such as CELEX). It is also effective if the alignment is mostly monotonic and\nless context sensitive (such as Russian, German and Spanish).\n\n- General Discussion:\n\nThe authors proposed a novel neural model for morphological inflection\ngeneration which uses \"hard attention\", character alignments separately\nobtained by using a Bayesian method for transliteration. It is substantially\ndifferent from the previous state of the art neural model for the task which\nuses \"soft attention\", where character alignment and conversion are solved\njointly in the probabilistic model.\n\nThe idea is novel and sound. The paper is clearly written. The experiment is\ncomprehensive. The only concern is that the proposed method is not necessarily\nthe state of the art in all conditions. It is suitable for the task with mostly\nmonotonic alignment and with less context sensitive phenomena. The paper would\nbe more convincing if it describe the practical merits of the proposed method,\nsuch as the ease of implementation and computational cost.\n- Strengths: A new encoder-decoder model is proposed that explicitly takes \ninto account monotonicity.\n\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\nde-coupled.\nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n\n- General Discussion:\n\nThe authors propose a novel encoder-decoder neural network architecture with\n\"hard monotonic attention\". They evaluate it on three morphology datasets.\n\nThis paper is a tough one. One the one hand it is well-written, mostly very\nclear and also presents a novel idea, namely including monotonicity in\nmorphology tasks. \n\nThe reason for including such monotonicity is pretty obvious: Unlike machine\ntranslation, many seq2seq tasks are monotone, and therefore general\nencoder-decoder models should not be used in the first place. That they still\nperform reasonably well should be considered a strong argument for neural\ntechniques, in general. The idea of this paper is now to explicity enforce a\nmonotonic output character generation. They do this by decoupling alignment and\ntransduction and first aligning input-output sequences monotonically and\nthen training to generate outputs in agreement with the monotone alignments.\nHowever, the authors are unclear on this point. I have a few questions:\n\n1) How do your alignments look like? On the one hand, the alignments seem to\nbe of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\ncharacter can be aligned with zero, 1, or several output characters. However,\nthis seems to contrast with the description given in lines 311-312 where the\nauthors speak of several input characters aligned to 1 output character. That\nis, do you use 1-to-many, many-to-1 or many-to-many alignments?\n\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\nstage, align input and output characters monotonically with a 1-to-many\nconstraint (one can use any monotone aligner, such as the toolkit of\nJiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\npredict exactly these 1-to-many alignments. For example, flog->fliege (your\nexample on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4)\nfrom \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested\nin multiple papers, one reference could be [*, Section 4.2] below. \nMy two questions here are: \n\n2a) How does your approach differ from this rather simple idea?\n\n2b) Why did you not include it as a baseline?\n\nFurther issues:\n\n3) It's really a pitty that you only tested on morphology, because there are\nmany other interesting monotonic seq2seq tasks, and you could have shown your\nsystem's superiority by evaluating on these, given that you explicitly model\nmonotonicity (cf. also [*]).\n\n4) You perform \"on par or better\" (l.791). There seems to be a general\ncognitive bias among NLP researchers to map instances where they perform worse\nto\n\"on par\" and all the rest to \"better\". I think this wording should be\ncorrected, but otherwise I'm fine with the experimental results.\n\n5) You say little about your linguistic features: From Fig. 1, I infer that\nthey include POS, etc. \n\n5a) Where did you take these features from?\n\n5b) Is it possible that these are responsible for your better performance in\nsome cases, rather than the monotonicity constraints?\n\nMinor points:\n\n6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar\n\n7) l.231 \"Where\" should be lower case\n\n8) l.237 and many more: $x_1\\ldots x_n$. As far as I know, the math community\nrecommends to write $x_1,\\ldots,x_n$ but $x_1\\cdots x_n$. That is, dots should\nbe on the same level as surrounding symbols.\n\n9) Figure 1: is it really necessary to use cyrillic font? I can't even address\nyour example here, because I don't have your fonts.\n\n10) l.437: should be \"these\"\n\n[*] \n\n@InProceedings{schnober-EtAl:2016:COLING, \n\n  author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh,\nErik-L\\^{a}n  and  Gurevych, Iryna},\n  title     = {Still not there? Comparing Traditional Sequence-to-Sequence\nModels to Encoder-Decoder Neural Networks on Monotone String Translation\nTasks},\n  booktitle = {Proceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers},\n  month     = {December},\n  year                                                      = {2016},\n  address   = {Osaka, Japan},\n  publisher = {The COLING 2016 Organizing Committee},\n  pages     = {1703--1714},\n  url                                               =\n{http://aclweb.org/anthology/C16-1160}\n\n}\n\nAFTER AUTHOR RESPONSE\n\nThanks for the clarifications. I think your alignments got mixed up in the\nresponse somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,\n1-1, and later make many-to-many alignments from these. \nI know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question\nwould have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober\net al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much\nyour results would have differed from such a rather simple baseline. (A tagger\nis a monotone model to start with and given the monotone alignments, everything\nstays monotone. In contrast, you start out with a more general model and then\nput hard monotonicity constraints on this ...)\n\nNOTES FROM AC\n\nAlso quite relevant is Cohn et al. (2016),\nhttp://www.aclweb.org/anthology/N16-1102 .\n\nIsn't your architecture also related to methods like the Stack LSTM, which\nsimilarly predicts a sequence of actions that modify or annotate an input?  \n\nDo you think you lose anything by using a greedy alignment, in contrast to\nRastogi et al. (2016), which also has hard monotonic attention but sums over\nall alignments?",
        "partial_review": "- Strengths:\nThe idea of hard monotonic attention is new and substantially different from\nothers.\n\n- Weaknesses:\nThe experiment results on morphological inflection generation is somewhat\nmixed. The proposed model is effective if the amount of training data is small\n(such as CELEX). It is also effective if the alignment is mostly monotonic and\nless context sensitive (such as Russian, German and Spanish).\n\n- General Discussion:\n\nThe authors proposed a novel neural model for morphological inflection\ngeneration which uses \"hard attention\", character alignments separately\nobtained by using a Bayesian method for transliteration. It is substantially\ndifferent from the previous state of the art neural model for the task which\nuses \"soft attention\", where character alignment and conversion are solved\njointly in the probabilistic model.\n\nThe idea is novel and sound. The paper is clearly written. The experiment is\ncomprehensive. The only concern is that the proposed method is not necessarily\nthe state of the art in all conditions. It is suitable for the task with mostly\nmonotonic alignment and with less context sensitive phenomena. The paper would\nbe more convincing if it describe the practical merits of the proposed method,\nsuch as the ease of implementation and computational cost.\n- Strengths: A new encoder-decoder model is proposed that explicitly takes \ninto account monotonicity.\n\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\nde-coupled.\nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n\n- General Discussion:\n\nThe authors propose a novel encoder-decoder neural network architecture with\n\"hard monotonic attention\". They evaluate it on three morphology datasets.\n\nThis paper is a tough one. One the one hand it is well-written, mostly very\nclear and also presents a novel idea, namely including monotonicity in\nmorphology tasks. \n\nThe reason for including such monotonicity is pretty obvious: Unlike machine\ntranslation, many seq2seq tasks are monotone, and therefore general\nencoder-decoder models should not be used in the first place. That they still\nperform reasonably well should be considered a strong argument for neural\ntechniques, in general. The idea of this paper is now to explicity enforce a\nmonotonic output character generation. They do this by decoupling alignment and\ntransduction and first aligning input-output sequences monotonically and\nthen training to generate outputs in agreement with the monotone alignments.\nHowever, the authors are unclear on this point. I have a few questions:\n\n1) How do your alignments look like? On the one hand, the alignments seem to\nbe of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\ncharacter can be aligned with zero, 1, or several output characters. However,\nthis seems to contrast with the description given in lines 311-312 where the\nauthors speak of several input characters aligned to 1 output character. That\nis, do you use 1-to-many, many-to-1 or many-to-many alignments?\n\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\nstage, align input and output characters monotonically with a 1-to-many\nconstraint (one can use any monotone aligner, such as the toolkit of\nJiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\npredict exactly these 1-to-many alignments. For example, flog->fliege (your\nexample on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Morphological Inflection Generation with Hard Monotonic Attention \n\nHere is the abstract of the paper: We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such models extract. \n\nHere is the partial review: - Strengths:\nThe idea of hard monotonic attention is new and substantially different from\nothers.\n\n- Weaknesses:\nThe experiment results on morphological inflection generation is somewhat\nmixed. The proposed model is effective if the amount of training data is small\n(such as CELEX). It is also effective if the alignment is mostly monotonic and\nless context sensitive (such as Russian, German and Spanish).\n\n- General Discussion:\n\nThe authors proposed a novel neural model for morphological inflection\ngeneration which uses \"hard attention\", character alignments separately\nobtained by using a Bayesian method for transliteration. It is substantially\ndifferent from the previous state of the art neural model for the task which\nuses \"soft attention\", where character alignment and conversion are solved\njointly in the probabilistic model.\n\nThe idea is novel and sound. The paper is clearly written. The experiment is\ncomprehensive. The only concern is that the proposed method is not necessarily\nthe state of the art in all conditions. It is suitable for the task with mostly\nmonotonic alignment and with less context sensitive phenomena. The paper would\nbe more convincing if it describe the practical merits of the proposed method,\nsuch as the ease of implementation and computational cost.\n- Strengths: A new encoder-decoder model is proposed that explicitly takes \ninto account monotonicity.\n\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\nde-coupled.\nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n\n- General Discussion:\n\nThe authors propose a novel encoder-decoder neural network architecture with\n\"hard monotonic attention\". They evaluate it on three morphology datasets.\n\nThis paper is a tough one. One the one hand it is well-written, mostly very\nclear and also presents a novel idea, namely including monotonicity in\nmorphology tasks. \n\nThe reason for including such monotonicity is pretty obvious: Unlike machine\ntranslation, many seq2seq tasks are monotone, and therefore general\nencoder-decoder models should not be used in the first place. That they still\nperform reasonably well should be considered a strong argument for neural\ntechniques, in general. The idea of this paper is now to explicity enforce a\nmonotonic output character generation. They do this by decoupling alignment and\ntransduction and first aligning input-output sequences monotonically and\nthen training to generate outputs in agreement with the monotone alignments.\nHowever, the authors are unclear on this point. I have a few questions:\n\n1) How do your alignments look like? On the one hand, the alignments seem to\nbe of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\ncharacter can be aligned with zero, 1, or several output characters. However,\nthis seems to contrast with the description given in lines 311-312 where the\nauthors speak of several input characters aligned to 1 output character. That\nis, do you use 1-to-many, many-to-1 or many-to-many alignments?\n\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\nstage, align input and output characters monotonically with a 1-to-many\nconstraint (one can use any monotone aligner, such as the toolkit of\nJiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\npredict exactly these 1-to-many alignments. For example, flog->fliege (your\nexample on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"I have a few comments on the strengths and weaknesses of the paper. \\n\\n- Strengths: The idea of hard monotonic attention is new and provides a different approach to the task of morphological inflection generation. The paper is well-written and the experiments are comprehensive. \\n\\n- Weaknesses: The experiment results on morphological inflection generation show mixed results. The proposed model is effective with small amounts of training data and in cases where the alignment is mostly monotonic and less context-sensitive. It would have been helpful if the authors discussed the practical merits of the proposed method, such as ease of implementation and computational cost. Additionally, the model is not evaluated on other monotone Seq2Seq tasks, limiting the generalizability of the findings.\\n\\n- General Discussion: The authors propose a novel neural model for morphological inflection generation that incorporates hard monotonic attention. This is an interesting idea, as it takes into account the nearly-monotonic alignment commonly found between characters in a word and its inflection. While the paper presents a comprehensive evaluation on three morphological inflection generation datasets, the proposed method is not necessarily the state-of-the-art in all conditions. It is more suitable for tasks with mostly monotonic alignment and less context-sensitive phenomena. Addressing these concerns would strengthen the paper and contribute to its overall impact.\"\n}",
        "human_end_index": 3543
    },
    {
        "id": "107",
        "source": "PeerRead/acl_2017/",
        "title": "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection",
        "human_review": "This paper presents several weakly supervised methods for developing NERs. The\nmethods rely on some form of projection from English into another language. The\noverall approach is not new and the individual methods proposed are\nimprovements of existing methods. For an ACL paper I would have expected more\nnovel approaches.\n\nOne of the contributions of the paper is the data selection scheme. The formula\nused to calculate the quality score is quite straightforward and this is not a\nbad thing. However, it is unclear how the thresholds were calculated for Table\n2. The paper says only that different thresholds were tried. Was this done on a\ndevelopment set? There is no mention of this in the paper. The evaluation\nresults show clearly that data selection is very important, but one may not\nknow how to tune the parameters for a new data set or a new language pair. \n\nAnother contribution of the paper is the combination of the outputs of the two\nsystems developed in the paper. I tried hard to understand how it works, but\nthe description provided is not clear. \n\nThe paper presents a number of variants for each of the methods proposed. Does\nit make sense to combine more than two weakly supervised systems? Did the\nauthors try anything in this direction.\n\nIt would be good to know a bit more about the types of texts that are in the\n\"in-house\" dataset.\nThis paper describes a model for cross-lingual named entity recognition (NER).\nThe authors employ conditional random fields, maximum entropy Markov, and\nneural network-based NER methods. In addition, authors propose two methods to\ncombine the output of those methods (probability-based and ranking-based), and\na method to select the best training instances from cross-lingual comparable\ncorpora. The cross-lingual projection is done using a variant of Mikolov\u2019s\nproposal. In general, the paper is easy to follow, well-structured, and the\nEnglish quality is also correct. The results of the combined annotations are\ninteresting.\n\nDetailed comments:\n\nI was wondering which is the motivation behind proposing a Continuous\nBag-of-word (CBOW) model variation. You don\u2019t give much details about this\n(or the parameters employed). Was the original model (or the Continuous\nSkip-gram model) offering low results? I suggest to include also the results\nwith the CBOW model, so readers can analyse the improvements of your approach.\nSince you use a decay factor for the surrounding embeddings, I suggest to take\na look to the exponential decay used in [1].\n\nSimilarly to the previous comment, I would like to look at the differences\nbetween the original Mikolov\u2019s cross-lingual projections and your frequency\nweighted projections. These contributions are more valuable if readers can see\nthat your method is really superior.\n\n\u201cthe proposed data selection scheme is very effective in selecting\ngood-quality projection-labeled data and the improvement is significant\u201d \u2190\nHave you conducted a test of statistical significance? I would like to know if\nthe differences between result in this work are significant. \n\nI suggest to integrate the text of Section 4.4 at the beginning of Section 4.2.\nIt would look cleaner. I also recommend to move the evaluation of Table 2 to\nthe evaluation section.\n\nI miss a related work section. Your introduction includes part of that\ninformation. I suggest to divide the introduction in two sections.\n\nThe evaluation is quite short (1.5 pages with conclusion section there). You\nobtain state-of-the-art results, and I would appreciate more discussion and\nanalysis of the results.\n\nSuggested references:\n\n[1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word\nsense disambiguation: An evaluation study. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907).",
        "partial_review": "This paper presents several weakly supervised methods for developing NERs. The\nmethods rely on some form of projection from English into another language. The\noverall approach is not new and the individual methods proposed are\nimprovements of existing methods. For an ACL paper I would have expected more\nnovel approaches.\n\nOne of the contributions of the paper is the data selection scheme. The formula\nused to calculate the quality score is quite straightforward and this is not a\nbad thing. However, it is unclear how the thresholds were calculated for Table\n2. The paper says only that different thresholds were tried. Was this done on a\ndevelopment set? There is no mention of this in the paper. The evaluation\nresults show clearly that data selection is very important, but one may not\nknow how to tune the parameters for a new data set or a new language pair. \n\nAnother contribution of the paper is the combination of the outputs of the two\nsystems developed in the paper. I tried hard to understand how it works, but\nthe description provided is not clear. \n\nThe paper presents a number of variants for each of the methods proposed. Does\nit make sense to combine more than two weakly supervised systems? Did the\nauthors try anything in this direction.\n\nIt would be good to know a bit more about the types of texts that are in the\n\"in-house\" dataset.\nThis paper describes a model for cross-lingual named entity recognition (NER).\nThe authors employ conditional random fields, maximum entropy Markov, and\nneural network-based NER methods. In addition, authors propose two methods to\ncombine the output of those methods (probability-based and ranking-based), and\na method to select the best training instances from cross-lingual comparable\ncorpora. The cross-lingual projection is done using a variant of Mikolov\u2019s\nproposal. In general, the paper is easy to follow, well-structured, and the\nEngli",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection \n\nHere is the abstract of the paper: The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data. \n\nHere is the partial review: This paper presents several weakly supervised methods for developing NERs. The\nmethods rely on some form of projection from English into another language. The\noverall approach is not new and the individual methods proposed are\nimprovements of existing methods. For an ACL paper I would have expected more\nnovel approaches.\n\nOne of the contributions of the paper is the data selection scheme. The formula\nused to calculate the quality score is quite straightforward and this is not a\nbad thing. However, it is unclear how the thresholds were calculated for Table\n2. The paper says only that different thresholds were tried. Was this done on a\ndevelopment set? There is no mention of this in the paper. The evaluation\nresults show clearly that data selection is very important, but one may not\nknow how to tune the parameters for a new data set or a new language pair. \n\nAnother contribution of the paper is the combination of the outputs of the two\nsystems developed in the paper. I tried hard to understand how it works, but\nthe description provided is not clear. \n\nThe paper presents a number of variants for each of the methods proposed. Does\nit make sense to combine more than two weakly supervised systems? Did the\nauthors try anything in this direction.\n\nIt would be good to know a bit more about the types of texts that are in the\n\"in-house\" dataset.\nThis paper describes a model for cross-lingual named entity recognition (NER).\nThe authors employ conditional random fields, maximum entropy Markov, and\nneural network-based NER methods. In addition, authors propose two methods to\ncombine the output of those methods (probability-based and ranking-based), and\na method to select the best training instances from cross-lingual comparable\ncorpora. The cross-lingual projection is done using a variant of Mikolov\u2019s\nproposal. In general, the paper is easy to follow, well-structured, and the\nEngli \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"sh and grammar are good. However, there are several areas of improvement in this paper. Firstly, the paper lacks novelty as the overall approach is not new and the individual methods proposed are improvements of existing methods. For an ACL paper, more novel approaches were expected. Additionally, the calculation of the quality score for data selection is straightforward but it is unclear how the thresholds in Table 2 were determined. The paper does not mention if different thresholds were tried on a development set. This lack of information makes it difficult for readers to understand how to tune the parameters for a new data set or language pair. The combination of the outputs of the two systems is also not well-described, and the paper could benefit from a clearer explanation. Another point to consider is if it makes sense to combine more than two weakly supervised systems, and if the authors have explored this direction. Lastly, more information about the types of texts in the \\\"in-house\\\" dataset would be valuable. Overall, while the paper is well-structured and easy to follow, there are areas where more clarity and information are needed.\"\n}",
        "human_end_index": 1900
    },
    {
        "id": 108,
        "source": "PeerRead/acl_2017/",
        "title": "A Multigraph-based Model for Overlapping Entity Recognition",
        "human_review": "- Strengths: the paper is well-written, except for a few places as described\nbelow. The problem the paper tackles is useful. The proposed approach,\nmultigraph-based model, is a variant of MH. The empirical result is solid.\n\n- Weaknesses: Clarification is needed in several places.\n\n1. In section 3, in addition to the description of the previous model, MH, you\nneed point out the issues of MH which motivate you to propose a new model.\n\n2. In section 4, I don't see the reason why separators are introduced. what\nadditional info they convene beyond T/I/O?\n\n3. section 5.1 does not seem to provide useful info regarding why the new model\nis superior.\n\n4. the discussion in section 5.2 is so abstract that I don't get the insights\nwhy the new model is better than MH. can you provide examples of spurious\nstructures? \n\n- General Discussion: The paper presents a new model for detecting overlapping\nentities in text. The new model improves the previous state-of-the-art, MH, in\nthe experiments on a few benchmark datasets. But it is not clear why and how\nthe new model works better.\nThe paper suggests an approach based on multigraphs (several edges may link two\nnodes) for detecting potentially overlapping entities.\n\nStrengths:\nThe problem itself could be rather interesting especially for crossing entities\nto decide which one might actually be mentioned in some text. The technique\nseems to work although the empirically results do not show some \"dramatic\"\neffect. I like that some words are spent on efficiency compared to a previous\nsystem. The paper in general is well-written but also needs some further\npolishing in some details (see minor remarks below).\n\nWeaknesses:\nThe problem itself is not really well motivated. Why is it important to detect\nChina as an entity within the entity Bank of China, to stay with the example in\nthe introduction? I do see a point for crossing entities but what is the use\ncase for nested entities? This could be much more motivated to make the reader\ninterested. As for the approach itself, some important details are missing in\nmy opinion: What is the decision criterion to include an edge or not? In lines\n229--233 several different options for the I^k_t nodes are mentioned but it is\nnever clarified which edges should be present!\n\nAs for the empirical evaluation, the achieved results are better than some\nprevious approaches but not really by a large margin. I would not really call\nthe slight improvements as \"outperformed\" as is done in the paper. What is the\neffect size? Does it really matter to some user that there is some improvement\nof two percentage points in F_1? What is the actual effect one can observe? How\nmany \"important\" entities are discovered, that have not been discovered by\nprevious methods? Furthermore, what performance would some simplistic\ndictionary-based method achieve that could also be used to find overlapping\nthings? And in a similar direction: what would some commercial system like\nGoogle's NLP cloud that should also be able to detect and link entities would\nhave achieved on the datasets. Just to put the results also into contrast of\nexisting \"commercial\" systems.\n\nAs for the result discussion, I would have liked to see some more emphasis on\nactual crossing entities. How is the performance there? This in my opinion is\nthe more interesting subset of overlapping entities than the nested ones. How\nmany more crossing entities are detected than were possible before? Which ones\nwere missed and maybe why? Is the performance improvement due to better nested\ndetection only or also detecting crossing entities? Some general error\ndiscussion comparing errors made by the suggested system and previous ones\nwould also strengthen that part.\n\nGeneral Discussion:\nI like the problems related to named entity recognition and see a point for\nrecognizing crossing entities. However, why is one interested in nested\nentities? The paper at hand does not really motivate the scenario and also\nsheds no light on that point in the evaluation. Discussing errors and maybe\nadvantages with some example cases and an emphasis on the results on crossing\nentities compared to other approaches would possibly have convinced me more.\nSo, I am only lukewarm about the paper with maybe a slight tendency to\nrejection. It just seems yet another try without really emphasizing the in my\nopinion important question of crossing entities.\n\nMinor remarks:\n\n- first mention of multigraph: some readers may benefit if the notion of a\nmultigraph would get a short description\n\n- previously noted by ... many previous: sounds a little odd\n\n- Solving this task: which one?\n\n- e.g.: why in italics?\n\n- time linear in n: when n is sentence length, does it really matter whether it\nis linear or cubic?\n\n- spurious structures: in the introduction it is not clear, what is meant\n\n- regarded as _a_ chunk\n\n- NP chunking: noun phrase chunking?\n\n- Since they set: who?\n\n- pervious -> previous\n\n- of Lu and Roth~(2015)\n\n- the following five types: in sentences with no large numbers, spell out the\nsmall ones, please\n\n- types of states: what is a state in a (hyper-)graph? later state seems to be\nused analogous to node?!\n\n- I would place commas after the enumeration items at the end of page 2 and a\nperiod after the last one\n\n- what are child nodes in a hypergraph?\n\n- in Figure 2 it was not obvious at first glance why this is a hypergraph.\ncolors are not visible in b/w printing. why are some nodes/edges in gray. it is\nalso not obvious how the highlighted edges were selected and why the others are\nin gray ...\n\n- why should both entities be detected in the example of Figure 2? what is the\ndifference to \"just\" knowing the long one?\n\n- denoting ...: sometimes in brackets, sometimes not ... why?\n\n- please place footnotes not directly in front of a punctuation mark but\nafterwards\n\n- footnote 2: due to the missing edge: how determined that this one should be\nmissing?\n\n- on whether the separator defines ...: how determined?\n\n- in _the_ mention hypergraph\n\n- last paragraph before 4.1: to represent the entity separator CS: how is the\nCS-edge chosen algorithmically here?\n\n- comma after Equation 1?\n\n- to find out: sounds a little odd here\n\n- we extract entities_._\\footnote\n\n- we make two: sounds odd; we conduct or something like that?\n\n- nested vs. crossing remark in footnote 3: why is this good? why not favor\ncrossing? examples to clarify?\n\n- the combination of states alone do_es_ not?\n\n- the simple first order assumption: that is what?\n\n- In _the_ previous section\n\n- we see that our model: demonstrated? have shown?\n\n- used in this experiments: these\n\n- each of these distinct interpretation_s_\n\n- published _on_ their website\n\n- The statistics of each dataset _are_ shown\n\n- allows us to use to make use: omit \"to use\"\n\n- tried to follow as close ... : tried to use the features suggested in\nprevious works as close as possible?\n\n- Following (Lu and Roth, 2015): please do not use references as nouns:\nFollowing Lu and Roth (2015)\n\n- using _the_ BILOU scheme\n\n- highlighted in bold: what about the effect size?\n\n- significantly better: in what sense? effect size?\n\n- In GENIA dataset: On the GENIA dataset\n\n- outperforms by about 0.4 point_s_: I would not call that \"outperform\"\n\n- that _the_ GENIA dataset\n\n- this low recall: which one?\n\n- due to _an_ insufficient\n\n- Table 5: all F_1 scores seems rather similar to me ... again, \"outperform\"\nseems a bit of a stretch here ...\n\n- is more confident: why does this increase recall?\n\n- converge _than_ the mention hypergraph\n\n- References: some paper titles are lowercased, others not, why?",
        "partial_review": "- Strengths: the paper is well-written, except for a few places as described\nbelow. The problem the paper tackles is useful. The proposed approach,\nmultigraph-based model, is a variant of MH. The empirical result is solid.\n\n- Weaknesses: Clarification is needed in several places.\n\n1. In section 3, in addition to the description of the previous model, MH, you\nneed point out the issues of MH which motivate you to propose a new model.\n\n2. In section 4, I don't see the reason why separators are introduced. what\nadditional info they convene beyond T/I/O?\n\n3. section 5.1 does not seem to provide useful info regarding why the new model\nis superior.\n\n4. the discussion in section 5.2 is so abstract that I don't get the insights\nwhy the new model is better than MH. can you provide examples of spurious\nstructures? \n\n- General Discussion: The paper presents a new model for detecting overlapping\nentities in text. The new model improves the previous state-of-the-art, MH, in\nthe experiments on a few benchmark datasets. But it is not clear why and how\nthe new model works better.\nThe paper suggests an approach based on multigraphs (several edges may link two\nnodes) for detecting potentially overlapping entities.\n\nStrengths:\nThe problem itself could be rather interesting especially for crossing entities\nto decide which one might actually be mentioned in some text. The technique\nseems to work although the empirically results do not show some \"dramatic\"\neffect. I like that some words are spent on efficiency compared to a previous\nsystem. The paper in general is well-written but also needs some further\npolishing in some details (see minor remarks below).\n\nWeaknesses:\nThe problem itself is not really well motivated. Why is it important to detect\nChina as an entity within the entity Bank of China, to stay with the example in\nthe introduction? I do see a point for crossing entities but what is the use\ncase for nested entities? This could be much more motivated to make the reader\ninterested. As for the approach itself, some important details are missing in\nmy opinion: What is the decision criterion to include an edge or not? In lines\n229--233 several different options for the I^k_t nodes are mentioned but it is\nnever clarified which edges should be present!\n\nAs for the empirical evaluation, the achieved results are better than some\nprevious approaches but not really by a large margin. I would not really call\nthe slight improvements as \"outperformed\" as is done in the paper. What is the\neffect size? Does it really matter to some user that there is some improvement\nof two percentage points in F_1? What is the actual effect one can observe? How\nmany \"important\" entities are discovered, that have not been discovered by\nprevious methods? Furthermore, what performance would some simplistic\ndictionary-based method achieve that could also be used to find overlapping\nthings? And in a similar direction: what would some commercial system like\nGoogle's NLP cloud that should also be able to detect and link entities would\nhave achieved on the datasets. Just to put the results also into contrast of\nexisting \"commercial\" systems.\n\nAs for the result discussion, I would have liked to see some more emphasis on\nactual crossing entities. How is the performance there? This in my opinion is\nthe more interesting subset of overlapping entities than the nested ones. How\nmany more crossing entities are detected than were possible before? Which ones\nwere missed and maybe why? Is the performance improvement due to better nested\ndetection only or also detecting crossing entities? Some general error\ndiscussion comparing errors made by the suggested system and previous ones\nwould also strengthen that part.\n\nGeneral Discussion:\nI like the problems related to named entity recogni",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A Multigraph-based Model for Overlapping Entity Recognition \n\nHere is the abstract of the paper: In this paper, we propose a new model for predicting overlapping entities based on multigraphs, as opposed to simple graphs commonly used in graphical models for structured predictions. Through experiments in standard datasets containing overlapping and non-overlapping entities, we demonstrate that our model outperforms previous models. We also present some analysis on the differences between our model and the previous models and discuss the possible implications of the differences. To the best of our knowledge, this is the first structured prediction model utilizing multigraphs to predict overlapping structures. \n\nHere is the partial review: - Strengths: the paper is well-written, except for a few places as described\nbelow. The problem the paper tackles is useful. The proposed approach,\nmultigraph-based model, is a variant of MH. The empirical result is solid.\n\n- Weaknesses: Clarification is needed in several places.\n\n1. In section 3, in addition to the description of the previous model, MH, you\nneed point out the issues of MH which motivate you to propose a new model.\n\n2. In section 4, I don't see the reason why separators are introduced. what\nadditional info they convene beyond T/I/O?\n\n3. section 5.1 does not seem to provide useful info regarding why the new model\nis superior.\n\n4. the discussion in section 5.2 is so abstract that I don't get the insights\nwhy the new model is better than MH. can you provide examples of spurious\nstructures? \n\n- General Discussion: The paper presents a new model for detecting overlapping\nentities in text. The new model improves the previous state-of-the-art, MH, in\nthe experiments on a few benchmark datasets. But it is not clear why and how\nthe new model works better.\nThe paper suggests an approach based on multigraphs (several edges may link two\nnodes) for detecting potentially overlapping entities.\n\nStrengths:\nThe problem itself could be rather interesting especially for crossing entities\nto decide which one might actually be mentioned in some text. The technique\nseems to work although the empirically results do not show some \"dramatic\"\neffect. I like that some words are spent on efficiency compared to a previous\nsystem. The paper in general is well-written but also needs some further\npolishing in some details (see minor remarks below).\n\nWeaknesses:\nThe problem itself is not really well motivated. Why is it important to detect\nChina as an entity within the entity Bank of China, to stay with the example in\nthe introduction? I do see a point for crossing entities but what is the use\ncase for nested entities? This could be much more motivated to make the reader\ninterested. As for the approach itself, some important details are missing in\nmy opinion: What is the decision criterion to include an edge or not? In lines\n229--233 several different options for the I^k_t nodes are mentioned but it is\nnever clarified which edges should be present!\n\nAs for the empirical evaluation, the achieved results are better than some\nprevious approaches but not really by a large margin. I would not really call\nthe slight improvements as \"outperformed\" as is done in the paper. What is the\neffect size? Does it really matter to some user that there is some improvement\nof two percentage points in F_1? What is the actual effect one can observe? How\nmany \"important\" entities are discovered, that have not been discovered by\nprevious methods? Furthermore, what performance would some simplistic\ndictionary-based method achieve that could also be used to find overlapping\nthings? And in a similar direction: what would some commercial system like\nGoogle's NLP cloud that should also be able to detect and link entities would\nhave achieved on the datasets. Just to put the results also into contrast of\nexisting \"commercial\" systems.\n\nAs for the result discussion, I would have liked to see some more emphasis on\nactual crossing entities. How is the performance there? This in my opinion is\nthe more interesting subset of overlapping entities than the nested ones. How\nmany more crossing entities are detected than were possible before? Which ones\nwere missed and maybe why? Is the performance improvement due to better nested\ndetection only or also detecting crossing entities? Some general error\ndiscussion comparing errors made by the suggested system and previous ones\nwould also strengthen that part.\n\nGeneral Discussion:\nI like the problems related to named entity recogni \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a new model for overlapping entity recognition based on multigraphs. The paper is well-written and the problem the paper tackles is useful. The proposed approach, multigraph-based model, is a variant of MH and the empirical results are solid. However, there are some weaknesses that need to be addressed.\\n\\nFirst, in section 3, the paper needs to clarify the issues with MH that motivate the proposed new model. This will help the reader understand the motivation for the new approach.\\n\\nSecond, in section 4, the paper should provide a clear explanation for the introduction of separators and what additional information they convey beyond T/I/O. This clarification will enhance the reader's understanding of the model.\\n\\nThird, in section 5.1, the paper does not provide useful information regarding why the new model is superior. The authors should provide more details and analysis to support their claim.\\n\\nLastly, the discussion in section 5.2 is too abstract and does not provide insights into why the new model is better than MH. The authors should consider providing examples of spurious structures to better illustrate the advantages of their model.\\n\\nFurthermore, the motivation for detecting nested entities and the importance of the problem could be more clearly explained. The paper should also include more details about the decision criterion for including edges in the model. Additionally, the empirical results should be presented in the context of effect size and compared to existing commercial systems. Lastly, the discussion should focus more on crossing entities and provide a comparison of errors made by the proposed system and previous ones. Addressing these weaknesses will strengthen the paper overall.\"\n}",
        "human_end_index": 3791
    },
    {
        "id": "117",
        "source": "PeerRead/acl_2017/",
        "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
        "human_review": "- Strengths: The paper addresses a relevant topic: learning the mapping between\nnatural language and KB relations, in the context of QA (where we have only\npartial information for one of the arguments), and in the case of having a very\nlarge number of possible target relations.\n\nThe proposal consists in a new method to combine two different representations\nof the input text: a word level representation (i.e. with segmentation of the\ntarget relation names and also the input text), and relations as a single token\n(i.e without segmentation of relation names nor input text). \n\nIt seems, that the main contribution in QA is the ability to re-rank entities\nafter the Entity Linking step.\n\nResults show an improvement compared with the state of the art. \n\n- Weaknesses:\nThe approach has been evaluated in a limited dataset. \n\n- General Discussion:\n\nI think, section 3.1 fits better inside related work, so the 3.2 can become\nsection 3 with the proposal. Thus, new section 3 can be splitted more properly.",
        "partial_review": "- Strengths: The paper addresses a relevant topic: learning the mapping between\nnatural language and KB relations, in the context of QA (where we have only\npartial information for one of the arguments), and in the case of having a very\nlarge number of possible target relations.\n\nThe proposal consists in a new method to combine two different representations\nof the input text: a word level representation (i.e. with segmentation of the\ntarget relation names and also the input text), and relations as a single token\n(i.e without segmentation of relation names nor input tex",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Improved Neural Relation Detection for Knowledge Base Question Answering \n\nHere is the abstract of the paper: Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. \n\nHere is the partial review: - Strengths: The paper addresses a relevant topic: learning the mapping between\nnatural language and KB relations, in the context of QA (where we have only\npartial information for one of the arguments), and in the case of having a very\nlarge number of possible target relations.\n\nThe proposal consists in a new method to combine two different representations\nof the input text: a word level representation (i.e. with segmentation of the\ntarget relation names and also the input text), and relations as a single token\n(i.e without segmentation of relation names nor input tex \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n\"complete_review\": \"Strengths: The paper addresses a relevant topic and introduces a new method for learning the mapping between natural language and KB relations in the context of QA. The proposed method combines two different representations of the input text, improving the model's ability to detect relations. The experimental results show that the approach achieves outstanding relation detection performance and state-of-the-art accuracy in both single-relation and multi-relation QA benchmarks. \n\nWeaknesses: The paper lacks a detailed explanation of the proposed method. While the use of deep residual bidirectional LSTMs is mentioned, it would be beneficial to provide more information about the model architecture and how it compares questions and relation names. Additionally, the integration of entity linking and the proposed relation detector is briefly mentioned but not elaborated upon. It would be helpful to include more details about how this integration is achieved and how it enhances the performance of the KBQA system.\n\nSuggestions for improvement: Provide a more thorough explanation of the proposed method, including the model architecture and the comparison between questions and relation names. Expand on the integration of entity linking and the relation detector, explaining in more detail how they enhance each other. Additionally, include more insights into the experimental results, discussing the strengths and limitations of the approach and potential future directions for improvement.\"\n}",
        "human_end_index": 574
    },
    {
        "id": "12",
        "source": "PeerRead/acl_2017/",
        "title": "Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules",
        "human_review": "This paper describes a rule based approach to time expression extraction. Its\nkey insights are time expressions typically are short and contain at least 1\ntime token. It first recognizes the time token through a combination of\ndictionary lookup, regular expression match with POS tagging information. It\nthen expands the time segment from either direction of the time token until it\nreaches based on a set of heuristic rules. Finally, it merges the time segments\ninto a single time expression based on another set of rules. Evaluation of this\napproach with both rule based & ML based systems on 3 data sets show\nsignificant improvements.\n\n- Strengths:\n\nIt's well written and clearly presented. The rules are motivated by empirical\nobservations of the data, and seems to be well justified as evidenced by the\nevaluation. \n\n- Weaknesses:\n\n There are some underspecification in the paper that makes it difficult to\nreproduce the results. See below for details.\n\n- General Discussion:\n\n* Section 4.1: what are there 5 seasons? What about things such as Ramadan\nmonth or Holiday Season?\n* Section 5.1: \"two benchmark datasets\" => \"three datasets\"?\n* Section 5.2: an example without time token will be helpful.\n* Section 5.2: given this approach is close to the ceiling of performance since\n93% expressions contain time token, and the system has achieved 92% recall, how\ndo you plan to improve further?\n* Is there any plan to release the full set of rules/software used?\nThe paper proposes a method to recognize time expressions from text. It is a\nsimple rule-based method, which is a strong advantage as an analysis tool since\ntime expression recognition should be a basic process in applications.\nExperiments results show that the proposed method outperforms the\nstate-of-the-art rule-based methods and machine learning based method for time\nexpression recognition. \n\nIt is great, but my concern is generality of the method. The rules in the\nmethod were designed based on observations of corpora that are used for\nevaluation as well. Hence I\u2019m afraid that the rules over-fit to these\ncorpora. Similarly, domains of these corpora may have affected the rule design.\nThere is no statistic nor discussion to show overlaps in time expressions in\nthe observed corpora. If it was shown that time expressions in these corpora\nare mostly overlap, the fact should have supported generality of the rules. \n\nAnyway, it was better that the experiments have been conducted using a new\ncorpus that was distinct from rule design process in order to show that the\nproposed method is widely effective.",
        "partial_review": "This paper describes a rule based approach to time expression extraction. Its\nkey insights are time expressions typically are short and contain at least 1\ntime token. It first recognizes the time token through a combination of\ndictionary lookup, regular expression match with POS tagging information. It\nthen expands the time segment from either direction of the time token until it\nreaches based on a set of heuristic rules. Finally, it merges the time segments\ninto a single time expression based on another set of rules. Evaluation of this\napproach with both rule based & ML based systems on 3 data sets show\nsignificant improvements.\n\n- Strengths:\n\nIt's well written and clearly presented. The rules are motivated by empirical\nobservations of the data, and seems to be well justified as evidenced by the\nevaluation. \n\n- Weaknesses:\n\n There are some underspecification in the paper that makes it difficult to\nreproduce the results. See below for details.\n\n- General Discussion:\n\n* Section 4.1: what are there 5 seasons? What about things such as Ramadan\nmonth or Holiday Season?\n* Section 5.1: \"two benchmark datasets\" => \"three datasets\"?\n* Section 5.2: an example without time token will be helpful.\n* Section 5.2: given this approach is close to the ceiling of performance since\n93% expressions contain time token, and the system has achieved 92% recall, how\ndo you plan t",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules \n\nHere is the abstract of the paper: Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and find that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment on benchmark datasets and tweets data shows that SynTime outperforms state-of-the-art methods. \n\nHere is the partial review: This paper describes a rule based approach to time expression extraction. Its\nkey insights are time expressions typically are short and contain at least 1\ntime token. It first recognizes the time token through a combination of\ndictionary lookup, regular expression match with POS tagging information. It\nthen expands the time segment from either direction of the time token until it\nreaches based on a set of heuristic rules. Finally, it merges the time segments\ninto a single time expression based on another set of rules. Evaluation of this\napproach with both rule based & ML based systems on 3 data sets show\nsignificant improvements.\n\n- Strengths:\n\nIt's well written and clearly presented. The rules are motivated by empirical\nobservations of the data, and seems to be well justified as evidenced by the\nevaluation. \n\n- Weaknesses:\n\n There are some underspecification in the paper that makes it difficult to\nreproduce the results. See below for details.\n\n- General Discussion:\n\n* Section 4.1: what are there 5 seasons? What about things such as Ramadan\nmonth or Holiday Season?\n* Section 5.1: \"two benchmark datasets\" => \"three datasets\"?\n* Section 5.2: an example without time token will be helpful.\n* Section 5.2: given this approach is close to the ceiling of performance since\n93% expressions contain time token, and the system has achieved 92% recall, how\ndo you plan t \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"o address those 7% cases? Is this an acceptable performance for practical use?\"\n}",
        "human_end_index": 1378
    },
    {
        "id": 122,
        "source": "PeerRead/acl_2017/",
        "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
        "human_review": "- Strengths:\nThis paper proposes a novel approach for dialogue state tracking that benefits\nfrom representing slot values with pre-trained embeddings and learns to compose\nthem into distributed representations of user utterances and dialogue context.\nExperiments performed on two datasets show consistent and significant\nimprovements over the baseline of previous delexicalization based approach.\nAlternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training\nword embeddings have been investigated.\n\n- Weaknesses:\nAlthough one of the main motivations for using embeddings is to generalize to\nmore complex dialogue domains where delexicalization may not scale for, the\ndatasets used seem limited.    I wonder how the approach would compare with and\nwithout a separate slot tagging component on more complex dialogues. For\nexample, when computing similarity between the utterance and slot value pairs,\none can actually limit the estimation to the span of the slot values. This\nshould be applicable even when the values do not match.\n\nI think the examples in the intro is misleading, shouldn\u2019t the dialogue state\nalso include \u201crestaurant_name=The House\u201d? This brings another question, how\ndoes resolution of coreferences impact this task?\n\n- General Discussion:\nOn the overall, use of pre-trained word embeddings is a great idea, and the\nspecific approach for using them is exciting.\nThis paper presents a neural network-based framework for dialogue state\ntracking.\nThe main contribution of this work is on learning representations of user\nutterances, system outputs, and also ontology entries, all of which are based\non pre-trained word vectors.\nParticularly for the utterance representation, the authors compared two\ndifferent neural network models: NBT-DNN and NBT-CNN.\nThe learned representations are combined with each other and finally used in\nthe downstream network to make binary decision for a given slot value pair.\nThe experiment shows that the proposed framework achieved significant\nperformance improvements compared to the baseline with the delexicalized\napproach.\n\nIt's generally a quality work with clear goal, reasonable idea, and improved\nresults from previous studies.\nBut the paper itself doesn't seem to be very well organized to effectively\ndeliver the details especially to readers who are not familiar with this area.\n\nFirst of all, more formal definition of DST needs to be given at the beginning\nof this paper.\nIt is not clear enough and could be more confusing after coupling with SLU.\nMy suggestion is to provide a general architecture of dialogue system described\nin Section 1 rather than Section 2, followed by the problem definition of DST\nfocusing on its relationships to other components including ASR, SLU, and\npolicy learning.\n\nAnd it would also help to improve the readability if all the notations used\nthroughout the paper are defined in an earlier section.\nSome symbols (e.g. t_q, t_s, t_v) are used much earlier than their\ndescriptions.\n\nBelow are other comments or questions:\n\n- Would it be possible to perform the separate SLU with this model? If no, the\nterm 'joint' could be misleading that this model is able to handle both tasks.\n\n- Could you please provide some statistics about how many errors were corrected\nfrom the original DSTC2 dataset?\nIf it is not very huge, the experiment could include the comparisons also with\nother published work including DSTC2 entries using the same dataset.\n\n- What do you think about using RNNs or LSTMs to learn the sequential aspects\nin learning utterance representations?\nConsidering the recent successes of these recurrent networks in SLU problems,\nit could be effective to DST as well.\n\n- Some more details about the semantic dictionary used with the baseline would\nhelp to imply the cost for building this kind of resources manually.\n\n- It would be great if you could give some samples which were not correctly\npredicted by the baseline but solved with your proposed models.",
        "partial_review": "- Strengths:\nThis paper proposes a novel approach for dialogue state tracking that benefits\nfrom representing slot values with pre-trained embeddings and learns to compose\nthem into distributed representations of user utterances and dialogue context.\nExperiments performed on two datasets show consistent and significant\nimprovements over the baseline of previous delexicalization based approach.\nAlternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training\nword embeddings have been investigated.\n\n- Weaknesses:\nAlthough one of the main motivations for using embeddings is to generalize to\nmore complex dialogue domains where delexicalization may not scale for, the\ndatasets used seem limited.    I wonder how the approach would compare with and\nwithout a separate slot tagging component on more complex dialogues. For\nexample, when computing similarity between the utterance and slot value pairs,\none can actually limit the estimation to the span of the slot values. This\nshould be applicable even when the values do not match.\n\nI think the examples in the intro is misleading, shouldn\u2019t the dialogue state\nalso include \u201crestaurant_name=The House\u201d? This brings another question, how\ndoes resolution of coreferences impact this task?\n\n- General Discussion:\nOn the overall, use of pre-trained word embeddings is a great idea, and the\nspecific approach for using them is exciting.\nThis paper presents a neural network-based framework for dialogue state\ntracking.\nThe main contribution of this work is on learning representations of user\nutterances, system outputs, and also ontology entries, all of which are based\non pre-trained word vectors.\nParticularly for the utterance representation, the authors compared two\ndifferent neural network models: NBT-DNN and NBT-CNN.\nThe learned representations are combined with each other and finally used in\nthe downstream network to make binary decision for a giv",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Neural Belief Tracker: Data-Driven Dialogue State Tracking \n\nHere is the abstract of the paper: One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided. \n\nHere is the partial review: - Strengths:\nThis paper proposes a novel approach for dialogue state tracking that benefits\nfrom representing slot values with pre-trained embeddings and learns to compose\nthem into distributed representations of user utterances and dialogue context.\nExperiments performed on two datasets show consistent and significant\nimprovements over the baseline of previous delexicalization based approach.\nAlternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training\nword embeddings have been investigated.\n\n- Weaknesses:\nAlthough one of the main motivations for using embeddings is to generalize to\nmore complex dialogue domains where delexicalization may not scale for, the\ndatasets used seem limited.    I wonder how the approach would compare with and\nwithout a separate slot tagging component on more complex dialogues. For\nexample, when computing similarity between the utterance and slot value pairs,\none can actually limit the estimation to the span of the slot values. This\nshould be applicable even when the values do not match.\n\nI think the examples in the intro is misleading, shouldn\u2019t the dialogue state\nalso include \u201crestaurant_name=The House\u201d? This brings another question, how\ndoes resolution of coreferences impact this task?\n\n- General Discussion:\nOn the overall, use of pre-trained word embeddings is a great idea, and the\nspecific approach for using them is exciting.\nThis paper presents a neural network-based framework for dialogue state\ntracking.\nThe main contribution of this work is on learning representations of user\nutterances, system outputs, and also ontology entries, all of which are based\non pre-trained word vectors.\nParticularly for the utterance representation, the authors compared two\ndifferent neural network models: NBT-DNN and NBT-CNN.\nThe learned representations are combined with each other and finally used in\nthe downstream network to make binary decision for a giv \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"en dialogue state tracking, representing slot values with pre-trained embeddings and learning to compose them into distributed representations of user utterances and dialogue context is a novel approach proposed in this paper. The experiments on two datasets show consistent and significant improvements over previous delexicalization based approaches. The paper investigates alternative approaches for pre-training word embeddings such as XAVIER, GloVe, and Program-SL999. However, the datasets used in the experiments may be limited compared to more complex dialogue domains. It would be interesting to see how the proposed approach performs without a separate slot tagging component on more complex dialogues. Furthermore, the paper raises questions about the impact of coreference resolution on the task and suggests the inclusion of additional dialogue state information. Overall, the use of pre-trained word embeddings is a promising idea, and the specific approach presented in this paper demonstrates exciting potential. The paper provides a detailed neural network-based framework for dialogue state tracking, particularly focusing on learning representations of utterances, system outputs, and ontology entries based on pre-trained word vectors. It compares two different neural network models for utterance representation and combines learned representations in a downstream network for making binary decisions. However, the review is incomplete and additional information is required to provide a comprehensive assessment.\"\n}",
        "human_end_index": 1918
    },
    {
        "id": 128,
        "source": "PeerRead/acl_2017/",
        "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks",
        "human_review": "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of the substructure embeddings\nare fed in a RNN at each time step as additional context for labeling.\n\n-----Strengths-----\n\nI think the main contribution of this paper is a simple way to \"flatten\"\nstructured information to an array of vectors (the memory), which is then\nconnected to the tagger as additional knowledge. The idea is similar to\nstructured / syntax-based attention (i.e. attention over nodes from treeLSTM);\nrelated work includes Zhao et al on textual entailment, Liu et al. on natural\nlanguage inference, and Eriguchi et al. for machine translation. The proposed\nsubstructure encoder is similar to DCNN (Ma et al.): each node is embedded from\na sequence of ancestor words. The architecture does not look entirely novel,\nbut I kind of like the simple and practical approach compared to prior work.\n\n-----Weaknesses-----\n\nI'm not very convinced by the empirical results, mostly due to the lack of\ndetails of the baselines. Comments below are ranked by decreasing importance.\n\n-  The proposed model has two main parts: sentence embedding and substructure\nembedding. In Table 1, the baseline models are TreeRNN and DCNN, they are\noriginally used for sentence embedding but one can easily take the\nnode/substructure embedding from them too. It's not clear how they are used to\ncompute the two parts.\n\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The\nonly difference in the knowledge-guided RNN is the addition of a \"knowledge\"\nvector from the memory in the RNN input (Eqn 5 and 8). It seems completely\nunnecessary to me to have separate weights for the two RNNs. The only advantage\nof using two is an increase of model capacity, i.e. more parameters.\nFurthermore, what are the hyper-parameters / size of the baseline neural\nnetworks? They should have comparable numbers of parameters.\n\n- I also think it is reasonable to include a baseline that just input\nadditional knowledge as features to the RNN, e.g. the head of each word, NER\nresults etc.\n\n- Any comments / results on the model's sensitivity to parser errors?\n\nComments on the model:\n\n- After computing the substructure embeddings, it seems very natural to compute\nan attention over them at each word. Is there any reason to use a static\nattention for all words? I guess as it is, the \"knowledge\" is acting more like\na filter to mark important words. Then it is reasonable to include the baseline\nsuggest above, i.e. input additional features.\n\n- Since the weight on a word is computed by inner product of the sentence\nembedding and the substructure embedding, and the two embeddings are computed\nby the same RNN/CNN, doesn't it means nodes / phrases similar to the whole\nsentence gets higher weights, i.e. all leaf nodes?\n\n- The paper claims the model generalizes to different knowledge but I think the\nsubstructure has to be represented as a sequence of words, e.g. it doesn't seem\nstraightforward for me to use constituent parse as knowledge here.\n\nFinally, I'm hesitating to call it \"knowledge\". This is misleading as usually\nit is used to refer to world / external knowledge such as a knowledge base of\nentities, whereas here it is really just syntax, or arguably semantics if AMR\nparsing is used.\n\n-----General Discussion-----\n\nThis paper proposes a practical model which seems working well on one dataset,\nbut the main ideas are not very novel (see comments in Strengths). I think as\nan ACL paper there should be more takeaways. More importantly, the experiments\nare not convincing as it is presented now. Will need some clarification to\nbetter judge the results.\n\n-----Post-rebuttal-----\n\nThe authors did not address my main concern, which is whether the baselines\n(e.g. TreeRNN) are used to compute substructure embeddings independent of the\nsentence embedding and the joint tagger. Another major concern is the use of\ntwo separate RNNs which gives the proposed model more parameters than the\nbaselines. Therefore I'm not changing my scores.",
        "partial_review": "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of the substructure embeddings\nare fed in a RNN at each time step as additional context for labeling.\n\n-----Strengths-----\n\nI think the main contribution of this paper is a simple way to \"flatten\"\nstructured information to an array of vectors (the memory), which is then\nconnected to the tagger as additional knowledge. The idea is similar to\nstructured / syntax-based attention (i.e. attention over nodes from treeLSTM);\nrelated work includes Zhao et al on textual entailment, Liu et al. on natural\nlanguage inference, and Eriguchi et al. for machine translation. The proposed\nsubstructure encoder is similar to DCNN (Ma et al.): each node is embedded from\na sequence of ancestor words. The architecture does not look entirely novel,\nbut I kind of like the simple and practical approach compared to prior work.\n\n-----Weaknesses-----\n\nI'm not very convinced by the empirical results, mostly due to the lack of\ndetails of the baselines. Comments below are ranked by decreasing importance.\n\n-  The proposed model has two main parts: sentence embedding and substructure\nembedding. In Table 1, the baseline models are TreeRNN and DCNN, they are\noriginally used for sentence embedding but one can easily take the\nnode/substructure embedding from them too. It's not clear how they are used to\ncompute the two parts.\n\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The\nonly difference in the knowledge-guided RNN is the addition of a \"knowledge\"\nvector from the memory in the RNN input (Eqn 5 and 8). It seems completely\nunnecessary to me to have separate weights for the two RNNs. The only advantage\nof using two is an increase of model capacity, i.e. more parameters.\nFurthermore, what are the hyper-parameters / size of the baseline neural\nnetworks? They should have comparab",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks \n\nHere is the abstract of the paper: Natural language understanding (NLU) is a core component of a dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time.  Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved.  The experiments on the benchmark ATIS data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the state-of-the-art neural network based frameworks. \n\nHere is the partial review: This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of the substructure embeddings\nare fed in a RNN at each time step as additional context for labeling.\n\n-----Strengths-----\n\nI think the main contribution of this paper is a simple way to \"flatten\"\nstructured information to an array of vectors (the memory), which is then\nconnected to the tagger as additional knowledge. The idea is similar to\nstructured / syntax-based attention (i.e. attention over nodes from treeLSTM);\nrelated work includes Zhao et al on textual entailment, Liu et al. on natural\nlanguage inference, and Eriguchi et al. for machine translation. The proposed\nsubstructure encoder is similar to DCNN (Ma et al.): each node is embedded from\na sequence of ancestor words. The architecture does not look entirely novel,\nbut I kind of like the simple and practical approach compared to prior work.\n\n-----Weaknesses-----\n\nI'm not very convinced by the empirical results, mostly due to the lack of\ndetails of the baselines. Comments below are ranked by decreasing importance.\n\n-  The proposed model has two main parts: sentence embedding and substructure\nembedding. In Table 1, the baseline models are TreeRNN and DCNN, they are\noriginally used for sentence embedding but one can easily take the\nnode/substructure embedding from them too. It's not clear how they are used to\ncompute the two parts.\n\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The\nonly difference in the knowledge-guided RNN is the addition of a \"knowledge\"\nvector from the memory in the RNN input (Eqn 5 and 8). It seems completely\nunnecessary to me to have separate weights for the two RNNs. The only advantage\nof using two is an increase of model capacity, i.e. more parameters.\nFurthermore, what are the hyper-parameters / size of the baseline neural\nnetworks? They should have comparab \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts. The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a 'knowledge' vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters. Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparab\"\n}",
        "human_end_index": 2143
    },
    {
        "id": "130",
        "source": "PeerRead/acl_2017/",
        "title": "Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts",
        "human_review": "- Strengths:\n\nThis paper proposes to apply NLP to speech transcripts (narratives and\ndescriptions) in order to identify patients with MCI (mild cognitive\nimpairment, ICD-10 code F06.7). The authors claim that they were able to\ndistinguish between healthy control participants and patients with MCI (lines\n141-144). However in the conclusion, lines 781-785, they say that \u201c\u2026\naccuracy ranging from 60% to 85% \u2026. means that it is not easy to distinguish\nbetween healthy subjects and those with cognitive impairments\u201d. So the paper\nbeginning is more optimistic than the conclusion but anyway the message is\nencouraging and the reader becomes curious to see more details about what has\nbeen actually done.\n\nThe corpus submitted in the dataset is constructed for 20 healthy patients and\n20 control participants only (20+20), and it is non-understandable for people\nwho do not speak Portuguese. It would be good to incorporate more technological\ndetails in the article and probably to include at least one example of a short\ntranscript that is translated to English, and eventually a (part of a) sample\nnetwork with embeddings for this transcript.\n\n- Weaknesses:\n\nThe paper starts with a detailed introduction and review of relevant work. Some\nof the cited references are more or less NLP background so they can be omitted\ne.g. (Salton 1989) in section 4.2.3. Other references are not directly related\nto the topic e.g. \u201csentiment classification\u201d and \u201cpedestrian detection in\nimages\u201d, lines 652-654, and they can be omitted too. In general lines\n608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to\ncompress the first 5 pages, focusing the review strictly on the paper topic,\nand consider the technological innovation in more detail, incl. samples of\nEnglish translations of the ABCD and/or Cindarela narratives.\n\nThe relatively short narratives in Portuguese esp. in ABCD dataset open the\nquestion how the similarities between words have been found, in order to\nconstruct word embeddings. In lines 272-289 the authors explain that they\ngenerate word-level networks from continuous word representations. What is the\nsource for learning the continuous word representations; are these the datasets\nABCD+Cinderella only, or external corpora were used? In lines 513-525 it is\nwritten that sub-word level (n-grams) networks were used to generate word\nembeddings. Again, what is the source for the training? Are we sure that the\ntwo kinds of networks together provide better accuracy? And what are the\n\u201cout-of-vocabulary words\u201d (line 516), from where they come?\n\n- General Discussion:\n\nIt is important to study how NLP can help to discover cognitive impairments;\nfrom this perspective the paper is interesting. Another interesting aspect is\nthat it deals with NLP for Portuguese, and it is important to explain how one\ncomputes embeddings for a language with relatively fewer resources (compared to\nEnglish). \n\nThe text needs revision: shortening sections 1-3, compressing 4.1 and adding\nmore explanations about the experiments. Some clarification about the NURC/SP\nN. 338 EF and 331 D2 transcription norms can be given.\n\nTechnical comments:\n\nLine 029: \u2018\u2026 as it a lightweight \u2026\u2019 -> shouldn\u2019t this be \u2018\u2026 as in\na lightweight \u2026\u2019\n\nLine 188: PLN -> NLP\n\nLine 264: \u2018out of cookie out of the cookie\u2019 \u2013 some words are repeated\ntwice \n\nTable 3, row 2, column 3: 72,0 -> 72.0\n\nLines 995-996: the DOI number is the same as the one at lines 1001-1002; the\nlink behind the title at lines 992-993 points to the next paper in the list\n- Strengths:\nThis paper explores is problem of identifying patients with Mild Cognitive\nImpairment (MCI) by analyzing speech transcripts available from three different\ndatasets. A graph based method leveraging co-occurrence information between\nwords found in transcripts is described. Features are encoded using different\ncharacteristics of the graph lexical, syntactic properties, and many others. \nResults are reported using 5 fold cross validation using a number of\nclassifiers. Different models exhibit different performance across the three\ndatasets. This work targets a well defined problem and uses appropriate\ndatasets. \n\n- Weaknesses:\nThe paper suffers from several drawbacks\n1. The paper is hard to read due to incorrect usage of English. The current\nmanuscript would benefit a  lot from a review grammar and spellings. \n2. The main machine learning problem being addressed is poorly described. What\nwas a single instance of classification? It seems every transcripts was\nclassified as MCI or No MCI. If this is the case, the dataset descriptions\nshould describe the numbers at a transcript level. Tables 1,2, and 3 should\ndescribe the data not the study that produced the transcripts. The age of the\npatients is irrelevant for the classification task. A lot of text (2 pages) is\nconsumed in simply describing the datasets with details that do not affect the\nend classification task. Also, I was unsure why numbers did not add up. For\ne.g.: in section 4.1.1 the text says 326 people were involved. But the total\nnumber of males and females in Table 1 are less than 100?\n3. What is the motivation behind enriching the graph? Why not represent each\nword by a node in the graph and connect them by the similarity between their\nvectors, irrespective of co-occurrence?\n4. The datsets are from a biomedical domain. No domain specific tools have been\nleveraged.\n5. Since dataset class distribution is unclear, it is unclear to determine if\naccuracy is a good measure for evaluation. In either case, since it is a binary\nclassification task, F1 would have been a desirable metric. \n6. Results are reported unto 4 decimal places on very small datasets (43\ntranscripts) without statistical tests over increments. Therefore, it is\nunclear if the gains are significant.\nThe paper describes a novel application of mostly existing representations,\nfeatures sets, and methods: namely, detecting Mild Cognitive Impairment (MCI) \nin speech narratives. The nature of the problem, datasets, and domain are\nthoroughly described. While missing some detail, the proposed solution and\nexperiments sound reasonable. Overall, I found the study interesting and\ninformative.\n\nIn terms of drawbacks, the paper needs some considerable editing to improve\nreadability. Details on some key concepts appear to be missing. For example, \ndetails on the multi-view learning used are omitted; the set of \u201clinguistic\nfeatures\u201d needs to be clarified; it is not entirely clear what datasets were\nused to generate the word embeddings (presumably the 3 datasets described in\nthe paper, which appear to be too small for that purpose\u2026). It is also not\nclear why disfluencies (filled pauses, false starts, repetitions, etc.) were\nremoved from the dataset. One might suggest that they are important features in\nthe context of MCI. It is also not clear why the most popular tf-idf weighting\nscheme was not used for the BoW classifications. In addition, tests for\nsignificance are not provided to substantiate the conclusions from the\nexperiments. Lastly, the related work is described a bit superficially. \n\nDetailed comments are provided below:\n\nAbstract: The abstract needs to be shortened. See detailed notes below.\n\nLines 22,23 need rephrasing.            \u201cHowever, MCI disfluencies produce\nagrammatical speech impacting in parsing results\u201d \u2192 impacting the parsing\nresults?\n\nLines 24,25: You mean correct grammatical errors in transcripts manually? It is\nnot clear why this should be performed, doesn\u2019t the fact that grammatical\nerrors are present indicate MCI? \u2026 Only after reading the Introduction and\nRelated Work sections it becomes clear what you mean. Perhaps include some\nexamples of disfluencies.\n\nLines 29,30 need rephrasing: \u201cas it a lightweight and language  independent\nrepresentation\u201d\n\nLines 34-38 need rephrasing: it is not immediately clear which exactly are the\n3 datasets. Maybe: \u201cthe other two: Cinderella and \u2026 \u201c            \n\nLine 70: \u201c15% a year\u201d \u2192 Not sure what exactly \u201cper year\u201d means\u2026\n\nLine 73 needs rephrasing.\n\nLines 115 - 117: It is not obvious why BoW will also have problems with\ndisfluencies, some explanation will be helpful.\n\nLines 147 - 149: What do you mean by \u201cthe best scenario\u201d?\n\nLine 157: \u201cin public corpora of Dementia Bank\u201d \u2192 a link or citation to\nDementia Bank will be helpful. \n\nLine 162: A link or citation describing the \u201cPicnic picture of the Western\nAphasia Battery\u201d will be helpful.\n\nLine 170: An explanation as to what the WML subtest is will be helpful.\n\nLine 172 is missing citations.\n\nLines 166 - 182: This appears to be the core of the related work and it is\ndescribed a bit superficially. For example, it will be helpful to know\nprecisely what methods were used to achieve these tasks and how they compare to\nthis study.\n\nLine 185: Please refer to the conference citation guidelines. I believe they\nare something along these lines: \u201cAluisio et al. (2016)  used\u2026\u201d\n\nLine 188: The definition of \u201cPLN\u201d appears to be missing.\n\nLines 233 - 235 could you some rephrasing. Lemmatization is not necessarily a\nlast step in text pre-processing and normalization, in fact there are also\nadditional common normalization/preprocessing steps omitted. \n\nLines 290-299: Did you create the word embeddings using the MCI datasets or\nexternal datasets?\n\nLine 322: consisted of \u2192 consist of\n\nLines 323: 332 need to be rewritten. ... \u201cmanually segmented of the\nDementiaBank and Cinderella\u201d \u2192  What do you mean by segmented, segmented\ninto sentences? Why weren\u2019t all datasets automatically segmented?; \u201cABCD\u201d\nis not defined; You itemized the datasets in i) and ii), but subsequently  you\nrefer to 3 dataset, which is a bit confusing. Maybe one could explicitly name\nthe datasets, as opposed to referring to them as \u201cfirst\u201d, \u201csecond\u201d,\n\u201cthird\u201d.\n\nTable 1 Caption: The demographic information is present, but there are no any\nadditional statistics of the dataset, as described.\n\nLines 375 - 388:  It is not clear why filled pauses, false starts, repetitions,\netc. were removed. One might suggest that they are important features in the\ncontext of MCI \u2026.\n\nLine 399: \u2026 multidisciplinary team with psychiatrists ... \u2192 consisting of\npsychiatrists\u2026\n\nLines 340-440: A link or citation describing the transcription norms will be\nhelpful.\n\nSection 4.2.1. It is not clear what dataset was used to generate the word\nembeddings. \n\nLine 560. The shortest path as defined in feature 6?\n\nSection \u201c4.2.2 Linguistic Features\u201d needs to be significantly expanded for\nclarity. Also, please check the conference guidelines regarding additional\npages (\u201cSupplementary Material\u201d).\n\nLine 620: \u201cIn this work term frequency was \u2026\u201d \u2192 \u201cIn this work, term\nfrequency was \u2026\u201d Also, why not tf-idf, as it seems to be the most common\nweighting scheme? \n\nThe sentence on lines 641-645 needs to be rewritten.\n\nLine 662: What do you mean by \u201cthe threshold parameter\u201d? The threshold for\nthe word embedding cosine distance?\n\nLine 735 is missing a period.\n\nSection 4.3 Classification Algorithms: Details on exactly what scheme of\nmulti-view learning was used are entirely omitted. Statistical significance of\nresult differences is not provided.",
        "partial_review": "- Strengths:\n\nThis paper proposes to apply NLP to speech transcripts (narratives and\ndescriptions) in order to identify patients with MCI (mild cognitive\nimpairment, ICD-10 code F06.7). The authors claim that they were able to\ndistinguish between healthy control participants and patients with MCI (lines\n141-144). However in the conclusion, lines 781-785, they say that \u201c\u2026\naccuracy ranging from 60% to 85% \u2026. means that it is not easy to distinguish\nbetween healthy subjects and those with cognitive impairments\u201d. So the paper\nbeginning is more optimistic than the conclusion but anyway the message is\nencouraging and the reader becomes curious to see more details about what has\nbeen actually done.\n\nThe corpus submitted in the dataset is constructed for 20 healthy patients and\n20 control participants only (20+20), and it is non-understandable for people\nwho do not speak Portuguese. It would be good to incorporate more technological\ndetails in the article and probably to include at least one example of a short\ntranscript that is translated to English, and eventually a (part of a) sample\nnetwork with embeddings for this transcript.\n\n- Weaknesses:\n\nThe paper starts with a detailed introduction and review of relevant work. Some\nof the cited references are more or less NLP background so they can be omitted\ne.g. (Salton 1989) in section 4.2.3. Other references are not directly related\nto the topic e.g. \u201csentiment classification\u201d and \u201cpedestrian detection in\nimages\u201d, lines 652-654, and they can be omitted too. In general lines\n608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to\ncompress the first 5 pages, focusing the review strictly on the paper topic,\nand consider the technological innovation in more detail, incl. samples of\nEnglish translations of the ABCD and/or Cindarela narratives.\n\nThe relatively short narratives in Portuguese esp. in ABCD dataset open the\nquestion how the similarities between words have been found, in order to\nconstruct word embeddings. In lines 272-289 the authors explain that they\ngenerate word-level networks from continuous word representations. What is the\nsource for learning the continuous word representations; are these the datasets\nABCD+Cinderella only, or external corpora were used? In lines 513-525 it is\nwritten that sub-word level (n-grams) networks were used to generate word\nembeddings. Again, what is the source for the training? Are we sure that the\ntwo kinds of networks together provide better accuracy? And what are the\n\u201cout-of-vocabulary words\u201d (line 516), from where they come?\n\n- General Discussion:\n\nIt is important to study how NLP can help to discover cognitive impairments;\nfrom this perspective the paper is interesting. Another interesting aspect is\nthat it deals with NLP for Portuguese, and it is important to explain how one\ncomputes embeddings for a language with relatively fewer resources (compared to\nEnglish). \n\nThe text needs revision: shortening sections 1-3, compressing 4.1 and adding\nmore explanations about the experiments. Some clarification about the NURC/SP\nN. 338 EF and 331 D2 transcription norms can be given.\n\nTechnical comments:\n\nLine 029: \u2018\u2026 as it a lightweight \u2026\u2019 -> shouldn\u2019t this be \u2018\u2026 as in\na lightweight \u2026\u2019\n\nLine 188: PLN -> NLP\n\nLine 264: \u2018out of cookie out of the cookie\u2019 \u2013 some words are repeated\ntwice \n\nTable 3, row 2, column 3: 72,0 -> 72.0\n\nLines 995-996: the DOI number is the same as the one at lines 1001-1002; the\nlink behind the title at lines 992-993 points to the next paper in the list\n- Strengths:\nThis paper explores is problem of identifying patients with Mild Cognitive\nImpairment (MCI) by analyzing speech transcripts available from three different\ndatasets. A graph based method leveraging co-occurrence information between\nwords found in transcripts is described. Features are encoded using different\ncharacteristics of the graph lexical, syntactic properties, and many others. \nResults are reported using 5 fold cross validation using a number of\nclassifiers. Different models exhibit different performance across the three\ndatasets. This work targets a well defined problem and uses appropriate\ndatasets. \n\n- Weaknesses:\nThe paper suffers from several drawbacks\n1. The paper is hard to read due to incorrect usage of English. The current\nmanuscript would benefit a  lot from a review grammar and spellings. \n2. The main machine learning problem being addressed is poorly described. What\nwas a single instance of classification? It seems every transcripts was\nclassified as MCI or No MCI. If this is the case, the dataset descriptions\nshould describe the numbers at a transcript level. Tables 1,2, and 3 should\ndescribe the data not the study that produced the transcripts. The age of the\npatients is irrelevant for the classification task. A lot of text (2 pages) is\nconsumed in simply describing the datasets with details that do not affect the\nend classification task. Also, I was unsure why numbers did not add up. For\ne.g.: in section 4.1.1 the text says 326 people were involved. But the total\nnumber of males and females in Table 1 are less than 100?\n3. What is the motivation behind enriching the graph? Why not represent each\nword by a node in the graph and connect them by the similarity between their\nvectors, irrespective of co-occurrence?\n4. The datsets are from a biomedical domain. No domain specific tools have been\nleveraged.\n5. Since dataset class distribution is unclear, it is unclear to determine if\naccuracy is a good measure for evaluation. In either case, since it is a b",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts \n\nHere is the abstract of the paper: Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts.  In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments. \n\nHere is the partial review: - Strengths:\n\nThis paper proposes to apply NLP to speech transcripts (narratives and\ndescriptions) in order to identify patients with MCI (mild cognitive\nimpairment, ICD-10 code F06.7). The authors claim that they were able to\ndistinguish between healthy control participants and patients with MCI (lines\n141-144). However in the conclusion, lines 781-785, they say that \u201c\u2026\naccuracy ranging from 60% to 85% \u2026. means that it is not easy to distinguish\nbetween healthy subjects and those with cognitive impairments\u201d. So the paper\nbeginning is more optimistic than the conclusion but anyway the message is\nencouraging and the reader becomes curious to see more details about what has\nbeen actually done.\n\nThe corpus submitted in the dataset is constructed for 20 healthy patients and\n20 control participants only (20+20), and it is non-understandable for people\nwho do not speak Portuguese. It would be good to incorporate more technological\ndetails in the article and probably to include at least one example of a short\ntranscript that is translated to English, and eventually a (part of a) sample\nnetwork with embeddings for this transcript.\n\n- Weaknesses:\n\nThe paper starts with a detailed introduction and review of relevant work. Some\nof the cited references are more or less NLP background so they can be omitted\ne.g. (Salton 1989) in section 4.2.3. Other references are not directly related\nto the topic e.g. \u201csentiment classification\u201d and \u201cpedestrian detection in\nimages\u201d, lines 652-654, and they can be omitted too. In general lines\n608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to\ncompress the first 5 pages, focusing the review strictly on the paper topic,\nand consider the technological innovation in more detail, incl. samples of\nEnglish translations of the ABCD and/or Cindarela narratives.\n\nThe relatively short narratives in Portuguese esp. in ABCD dataset open the\nquestion how the similarities between words have been found, in order to\nconstruct word embeddings. In lines 272-289 the authors explain that they\ngenerate word-level networks from continuous word representations. What is the\nsource for learning the continuous word representations; are these the datasets\nABCD+Cinderella only, or external corpora were used? In lines 513-525 it is\nwritten that sub-word level (n-grams) networks were used to generate word\nembeddings. Again, what is the source for the training? Are we sure that the\ntwo kinds of networks together provide better accuracy? And what are the\n\u201cout-of-vocabulary words\u201d (line 516), from where they come?\n\n- General Discussion:\n\nIt is important to study how NLP can help to discover cognitive impairments;\nfrom this perspective the paper is interesting. Another interesting aspect is\nthat it deals with NLP for Portuguese, and it is important to explain how one\ncomputes embeddings for a language with relatively fewer resources (compared to\nEnglish). \n\nThe text needs revision: shortening sections 1-3, compressing 4.1 and adding\nmore explanations about the experiments. Some clarification about the NURC/SP\nN. 338 EF and 331 D2 transcription norms can be given.\n\nTechnical comments:\n\nLine 029: \u2018\u2026 as it a lightweight \u2026\u2019 -> shouldn\u2019t this be \u2018\u2026 as in\na lightweight \u2026\u2019\n\nLine 188: PLN -> NLP\n\nLine 264: \u2018out of cookie out of the cookie\u2019 \u2013 some words are repeated\ntwice \n\nTable 3, row 2, column 3: 72,0 -> 72.0\n\nLines 995-996: the DOI number is the same as the one at lines 1001-1002; the\nlink behind the title at lines 992-993 points to the next paper in the list\n- Strengths:\nThis paper explores is problem of identifying patients with Mild Cognitive\nImpairment (MCI) by analyzing speech transcripts available from three different\ndatasets. A graph based method leveraging co-occurrence information between\nwords found in transcripts is described. Features are encoded using different\ncharacteristics of the graph lexical, syntactic properties, and many others. \nResults are reported using 5 fold cross validation using a number of\nclassifiers. Different models exhibit different performance across the three\ndatasets. This work targets a well defined problem and uses appropriate\ndatasets. \n\n- Weaknesses:\nThe paper suffers from several drawbacks\n1. The paper is hard to read due to incorrect usage of English. The current\nmanuscript would benefit a  lot from a review grammar and spellings. \n2. The main machine learning problem being addressed is poorly described. What\nwas a single instance of classification? It seems every transcripts was\nclassified as MCI or No MCI. If this is the case, the dataset descriptions\nshould describe the numbers at a transcript level. Tables 1,2, and 3 should\ndescribe the data not the study that produced the transcripts. The age of the\npatients is irrelevant for the classification task. A lot of text (2 pages) is\nconsumed in simply describing the datasets with details that do not affect the\nend classification task. Also, I was unsure why numbers did not add up. For\ne.g.: in section 4.1.1 the text says 326 people were involved. But the total\nnumber of males and females in Table 1 are less than 100?\n3. What is the motivation behind enriching the graph? Why not represent each\nword by a node in the graph and connect them by the similarity between their\nvectors, irrespective of co-occurrence?\n4. The datsets are from a biomedical domain. No domain specific tools have been\nleveraged.\n5. Since dataset class distribution is unclear, it is unclear to determine if\naccuracy is a good measure for evaluation. In either case, since it is a b \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Strengths:\\nThis paper explores is problem of identifying patients with Mild Cognitive\\nImpairment (MCI) by analyzing speech transcripts available from three different\\ndatasets. A graph based method leveraging co-occurrence information between\\nwords found in transcripts is described. Features are encoded using different\\ncharacteristics of the graph lexical, syntactic properties, and many others.\\nResults are reported using 5 fold cross validation using a number of\\nclassifiers. Different models exhibit different performance across the three\\ndatasets. This work targets a well defined problem and uses appropriate\\ndatasets.\\n\\nWeaknesses:\\nThe paper suffers from several drawbacks\\n1. The paper is hard to read due to incorrect usage of English. The current\\nmanuscript would benefit a  lot from a review grammar and spellings.\\n2. The main machine learning problem being addressed is poorly described. What\\nwas a single instance of classification? It seems every transcripts was\\nclassified as MCI or No MCI. If this is the case, the dataset descriptions\\nshould describe the numbers at a transcript level. Tables 1,2, and 3 should\\ndescribe the data not the study that produced the transcripts. The age of the\\npatients is irrelevant for the classification task. A lot of text (2 pages) is\\nconsumed in simply describing the datasets with details that do not affect the\\nend classification task. Also, I was unsure why numbers did not add up. For\\ne.g.: in section 4.1.1 the text says 326 people were involved. But the total\\nnumber of males and females in Table 1 are less than 100?\\n3. What is the motivation behind enriching the graph? Why not represent each\\nword by a node in the graph and connect them by the similarity between their\\nvectors, irrespective of co-occurrence?\\n4. The datsets are from a biomedical domain. No domain specific tools have been\\nleveraged.\\n5. Since dataset class distribution is unclear, it is unclear to determine if\\naccuracy is a good measure for evaluation. In either case, since it is a b\"\n}",
        "human_end_index": 5550
    },
    {
        "id": 134,
        "source": "PeerRead/acl_2017/",
        "title": "Neural End-to-End Learning for Computational Argumentation Mining",
        "human_review": "- Strengths:\n\nThe paper is well-written and easy to understand. The methods and results are\ninteresting.\n\n- Weaknesses:\n\nThe evaluation and the obtained results might be problematic (see my comments\nbelow).\n\n- General Discussion:\n\nThis paper proposes a system for end-to-end argumentation mining using neural\nnetworks. The authors model the problem using two approaches: (1) sequence\nlabeling (2) dependency parsing. The paper also includes the results of\nexperimenting with a multitask learning setting for the sequence labeling\napproach. The paper clearly explains the motivation behind the proposed model.\nExisting methods are based on ILP, manual feature engineering and manual design\nof ILP constraints. However, the proposed model avoids such manual effort.\nMoreover, the model jointly learns the subtasks in argumentation mining and\ntherefore, avoids the error back propagation problem in pipeline methods.\nExcept a few missing details (mentioned below), the methods are explained\nclearly.\n\nThe experiments are substantial, the comparisons are performed properly, and\nthe results are interesting. My main concern about this paper is the small size\nof the dataset and the large capacity of the used (Bi)LSTM-based recurrent\nneural networks (BLC and BLCC). The dataset includes only around 320 essays for\ntraining and 80 essays for testing. The size of the development set, however,\nis not mentioned in the paper (and also the supplementary materials). This is\nworrying because very few number of essays are left for training, which is a\ncrucial problem. The total number of tags in the training data is probably only\na few thousand. Compare it to the standard sequence labeling tasks, where\nhundreds of thousands (sometimes millions) of tags are available. For this\nreason, I am not sure if the model parameters are trained properly. The paper\nalso does not analyze the overfitting problem. It would be interesting to see\nthe training and development \"loss\" values during training (after each\nparameter update or after each epoch). The authors have also provided some\ninformation that can be seen as the evidence for overfitting: Line 622 \"Our\nexplanation is that taggers are simpler local models, and thus need less\ntraining data and are less prone to overfitting\".\n\nFor the same reason, I am not sure if the models are stable enough. Mean and\nstandard deviation of multiple runs (different initializations of parameters)\nneed to be included. Statistical significance tests would also provide more\ninformation about the stability of the models and the reliability of results.\nWithout these tests, it is hard to say if the better results are because of the\nsuperiority of the proposed method or chance.\n\nI understand that the neural networks used for modeling the tasks use their\nregularization techniques. However, since the size of the dataset is too small,\nthe authors need to pay more attention to the regularization methods. The paper\ndoes not mention regularization at all and the supplementary material only\nmentions briefly about the regularization in LSTM-ER. This problem needs to be\naddressed properly in the paper.\n\nInstead of the current hyper-parameter optimization method (described in\nsupplementary materials) consider using Bayesian optimization methods.\n\nAlso move the information about pre-trained word embeddings and the error\nanalysis from the supplementary material to the paper. The extra one page\nshould be enough for this.\n\nPlease include some inter-annotator agreement scores. The paper describing the\ndataset has some relevant information. This information would provide some\ninsight about the performance of the systems and the available room for\nimprovement.\n\nPlease consider illustrating figure 1 with different colors to make the quality\nbetter for black and white prints.\n\nEdit:\n\nThanks for answering my questions. I have increased the recommendation score to\n4. Please do include the F1-score ranges in your paper and also report mean and\nvariance of different settings. I am still concerned about the model stability.\nFor example, the large variance of Kiperwasser setting needs to be analyzed\nproperly. Even the F1 changes in the range [0.56, 0.61] is relatively large.\nIncluding these score ranges in your paper helps replicating your work.\nThe work describes a joint neural approach to argumentation mining. There are\nseveral approaches explored including:\n 1) casting the problem as a dependency parsing problem (trying several\ndifferent parsers)\n 2) casting the problem as a sequence labeling problem\n3) multi task learning (based on sequence labeling model underneath)\n4) an out of the box neural model for labeling entities and relations (LSTM-ER)\n5) ILP based state-of-the art models\nAll the approaches are evaluated using F1 defined on concepts and relations. \nDependency based solutions do not work well, seq. labeling solutions are\neffective.\nThe out-of-the-box LSTM-ER model performs very well. Especially on paragraph\nlevel.\nThe Seq. labeling and LSTM-ER models both outperform the ILP approach.\nA very comprehensive supplement was given, with all the technicalities of\ntraining\nthe models, optimizing hyper-parameters etc.\nIt was also shown that sequence labeling models can be greatly improved by the\nmultitask\napproach (with the claim task helping more than the relation task).\nThe aper  is a very thorough investigation of neural based approaches to\nend-to-end argumentation mining.\n\n- Major remarks  \n  - my one concern is with the data set, i'm wondering if it's a problem that\nessays in the train set and in the test set might\n   be on the same topics, consequently writers might use the same or similar\narguments in both essays, leading to information\n   leakage from the train to the test set. In turn, this might give overly\noptimistic performance estimates. Though, i think the same\n   issues are present for the ILP models, so your model does not have an unfair\nadvantage. Still, this may be something to discuss.\n\n  - my other concern is that one of your best models LSTM-ER is acutally just a\nan out-of-the box application of a model from related\n    work. However, given the relative success of sequence based models and all\nthe experiments and useful lessons learned, I think this \n    work deserves to be published.\n\n- Minor remarks and questions:\n222 - 226 - i guess you are arguing that it's possible to reconstruct the full\ngraph once you get a tree as output? Still, this part is not quite clear.\n443-444 The ordering in this section is seq. tagging -> dependency based -> MTL\nusing seq. tagging, it would be much easier to follow if the order of the first\ntwo were\n                  reversed (by the time I got here i'd forgotten what STag_T\nstood for)\n455 - What does it mean that it de-couples them but jointly models them (isn't\ncoupling them required to jointly model them?)\n         - i checked Miwa and Bansal and I couldn't find it\n477 - 479 -  It's confusing when you say your system de-couples relation info\nfrom entity info, my best guess is that you mean it\n                        learns some tasks as \"the edges of the tree\" and some\nother tasks as \"the labels on those edges\", thus decoupling them. \n                        In any case,  I recommend you make this part clearer\n\nAre the F1 scores in the paragraph and essay settings comparable? In particular\nfor the relation tasks. I'm wondering if paragraph based \nmodels might miss some cross paragraph relations by default, because they will\nnever consider them.",
        "partial_review": "- Strengths:\n\nThe paper is well-written and easy to understand. The methods and results are\ninteresting.\n\n- Weaknesses:\n\nThe evaluation and the obtained results might be problematic (see my comments\nbelow).\n\n- General Discussion:\n\nThis paper proposes a system for end-to-end argumentation mining using neural\nnetworks. The authors model the problem using two approaches: (1) sequence\nlabeling (2) dependency parsing. The paper also includes the results of\nexperimenting with a multitask learning setting for the sequence labeling\napproach. The paper clearly explains the motivation behind the proposed model.\nExisting methods are based on ILP, manual feature engineering and manual design\nof ILP constraints. However, the proposed model avoids such manual effort.\nMoreover, the model jointly learns the subtasks in argumentation mining and\ntherefore, avoids the error back propagation problem in pipeline methods.\nExcept a few missing details (mentioned below), the methods are explained\nclearly.\n\nThe experiments are substantial, the comparisons are performed properly, and\nthe results are interesting. My main concern about this paper is the small size\nof the dataset and the large capacity of the used (Bi)LSTM-based recurrent\nneural networks (BLC and BLCC). The dataset includes only around 320 essays for\ntraining and 80 essays for testing. The size of the development set, however,\nis not mentioned in the paper (and also the supplementary materials). This is\nworrying because very few number of essays are left for training, which is a\ncrucial problem. The total number of tags in the training data is probably only\na few thousand. Compare it to the standard sequence labeling tasks, where\nhundreds of thousands (sometimes millions) of tags are available. For this\nreason, I am not sure if the model parameters are trained properly. The paper\nalso does not analyze the overfitting problem. It would be interesting to see\nthe training and development \"loss\" values during training (after each\nparameter update or after each epoch). The authors have also provided some\ninformation that can be seen as the evidence for overfitting: Line 622 \"Our\nexplanation is that taggers are simpler local models, and thus need less\ntraining data and are less prone to overfitting\".\n\nFor the same reason, I am not sure if the models are stable enough. Mean and\nstandard deviation of multiple runs (different initializations of parameters)\nneed to be included. Statistical significance tests would also provide more\ninformation about the stability of the models and the reliability of results.\nWithout these tests, it is hard to say if the better results are because of the\nsuperiority of the proposed method or chance.\n\nI understand that the neural networks used for modeling the tasks use their\nregularization techniques. However, since the size of the dataset is too small,\nthe authors need to pay more attention to the regularization methods. The paper\ndoes not mention regularization at all and the supplementary material only\nmentions briefly about the regularization in LSTM-ER. This problem needs to be\naddressed properly in the paper.\n\nInstead of the current hyper-parameter optimization method (described in\nsupplementary materials) consider using Bayesian optimization methods.\n\nAlso move the information about pre-trained word embeddings and the error\nanalysis from the supplementary material to the paper. The extra one page\nshould be enough for this.\n\nPlease include some inter-annotator agreement scores. The paper describing the\ndataset has some relevant information. This information would provide some\ninsight about the performance of the systems and the available room for\nimprovement.\n\nPlease consider illustrating figure 1 with different colors to make t",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Neural End-to-End Learning for Computational Argumentation Mining \n\nHere is the abstract of the paper: We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning `natural' subtasks, in a multi-task learning setup, improves performance. \n\nHere is the partial review: - Strengths:\n\nThe paper is well-written and easy to understand. The methods and results are\ninteresting.\n\n- Weaknesses:\n\nThe evaluation and the obtained results might be problematic (see my comments\nbelow).\n\n- General Discussion:\n\nThis paper proposes a system for end-to-end argumentation mining using neural\nnetworks. The authors model the problem using two approaches: (1) sequence\nlabeling (2) dependency parsing. The paper also includes the results of\nexperimenting with a multitask learning setting for the sequence labeling\napproach. The paper clearly explains the motivation behind the proposed model.\nExisting methods are based on ILP, manual feature engineering and manual design\nof ILP constraints. However, the proposed model avoids such manual effort.\nMoreover, the model jointly learns the subtasks in argumentation mining and\ntherefore, avoids the error back propagation problem in pipeline methods.\nExcept a few missing details (mentioned below), the methods are explained\nclearly.\n\nThe experiments are substantial, the comparisons are performed properly, and\nthe results are interesting. My main concern about this paper is the small size\nof the dataset and the large capacity of the used (Bi)LSTM-based recurrent\nneural networks (BLC and BLCC). The dataset includes only around 320 essays for\ntraining and 80 essays for testing. The size of the development set, however,\nis not mentioned in the paper (and also the supplementary materials). This is\nworrying because very few number of essays are left for training, which is a\ncrucial problem. The total number of tags in the training data is probably only\na few thousand. Compare it to the standard sequence labeling tasks, where\nhundreds of thousands (sometimes millions) of tags are available. For this\nreason, I am not sure if the model parameters are trained properly. The paper\nalso does not analyze the overfitting problem. It would be interesting to see\nthe training and development \"loss\" values during training (after each\nparameter update or after each epoch). The authors have also provided some\ninformation that can be seen as the evidence for overfitting: Line 622 \"Our\nexplanation is that taggers are simpler local models, and thus need less\ntraining data and are less prone to overfitting\".\n\nFor the same reason, I am not sure if the models are stable enough. Mean and\nstandard deviation of multiple runs (different initializations of parameters)\nneed to be included. Statistical significance tests would also provide more\ninformation about the stability of the models and the reliability of results.\nWithout these tests, it is hard to say if the better results are because of the\nsuperiority of the proposed method or chance.\n\nI understand that the neural networks used for modeling the tasks use their\nregularization techniques. However, since the size of the dataset is too small,\nthe authors need to pay more attention to the regularization methods. The paper\ndoes not mention regularization at all and the supplementary material only\nmentions briefly about the regularization in LSTM-ER. This problem needs to be\naddressed properly in the paper.\n\nInstead of the current hyper-parameter optimization method (described in\nsupplementary materials) consider using Bayesian optimization methods.\n\nAlso move the information about pre-trained word embeddings and the error\nanalysis from the supplementary material to the paper. The extra one page\nshould be enough for this.\n\nPlease include some inter-annotator agreement scores. The paper describing the\ndataset has some relevant information. This information would provide some\ninsight about the performance of the systems and the available room for\nimprovement.\n\nPlease consider illustrating figure 1 with different colors to make t \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n  \"complete_review\": \"- Strengths:\\n\\nThe paper is well-written and easy to understand. The methods and results are interesting.\\n\\n- Weaknesses:\\n\\nThe evaluation and the obtained results might be problematic (see my comments\\nbelow).\\n\\n- General Discussion:\\n\\nThis paper proposes a system for end-to-end argumentation mining using neural\\nnetworks. The authors model the problem using two approaches: (1) sequence\\nlabeling (2) dependency parsing. The paper also includes the results of\\nexperimenting with a multitask learning setting for the sequence labeling\\napproach. The paper clearly explains the motivation behind the proposed model.\\nExisting methods are based on ILP, manual feature engineering and manual design\\nof ILP constraints. However, the proposed model avoids such manual effort.\\nMoreover, the model jointly learns the subtasks in argumentation mining and\\ntherefore, avoids the error back propagation problem in pipeline methods.\\nExcept a few missing details (mentioned below), the methods are explained\\nclearly.\\n\\nThe experiments are substantial, the comparisons are performed properly, and\\nthe results are interesting. My main concern about this paper is the small size\\nof the dataset and the large capacity of the used (Bi)LSTM-based recurrent\\nneural networks (BLC and BLCC). The dataset includes only around 320 essays for\\ntraining and 80 essays for testing. The size of the development set, however,\\nis not mentioned in the paper (and also the supplementary materials). This is\\nworrying because very few number of essays are left for training, which is a\\ncrucial problem. The total number of tags in the training data is probably only\\na few thousand. Compare it to the standard sequence labeling tasks, where\\nhundreds of thousands (sometimes millions) of tags are available. For this\\nreason, I am not sure if the model parameters are trained properly. The paper\\ndoes not analyze the overfitting problem. It would be interesting to see\\nthe training and development \\\"loss\\\" values during training (after each\\nparameter update or after each epoch). The authors have also provided some\\ninformation that can be seen as the evidence for overfitting: Line 622 \\\"Our\\nexplanation is that taggers are simpler local models, and thus need less\\ntraining data and are less prone to overfitting\\\".\\n\\nFor the same reason, I am not sure if the models are stable enough. Mean and\\nstandard deviation of multiple runs (different initializations of parameters)\\nneed to be included. Statistical significance tests would also provide more\\ninformation about the stability of the models and the reliability of results.\\nWithout these tests, it is hard to say if the better results are because of the\\nsuperiority of the proposed method or chance.\\n\\nI understand that the neural networks used for modeling the tasks use their\\nregularization techniques. However, since the size of the dataset is too small,\\nthe authors need to pay more attention to the regularization methods. The paper\\ndoes not mention regularization at all and the supplementary material only\\nmentions briefly about the regularization in LSTM-ER. This problem needs to be\\naddressed properly in the paper.\\n\\nInstead of the current hyper-parameter optimization method (described in\\nsupplementary materials) consider using Bayesian optimization methods.\\n\\nAlso move the information about pre-trained word embeddings and the error\\nanalysis from the supplementary material to the paper. The extra one page\\nshould be enough for this.\\n\\nPlease include some inter-annotator agreement scores. The paper describing the\\ndataset has some relevant information. This information would provide some\\ninsight about the performance of the systems and the available room for\\nimprovement.\\n\\nPlease consider illustrating figure 1 with different colors to make t\"\n}",
        "human_end_index": 3764
    },
    {
        "id": "145",
        "source": "PeerRead/acl_2017/",
        "title": "Multimodal Word Distributions",
        "human_review": "Review: Multimodal Word Distributions\n\n- Strengths:  Overall a very strong paper.\n\n- Weaknesses: The comparison against similar approaches could be extended.\n\n- General Discussion:\n\nThe main focus of this paper is the introduction of a new model for learning\nmultimodal word distributions formed from Gaussian mixtures for multiple word\nmeanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which\nrepresented word as unimodal Gaussian distribution. By using a multimodal, the\ncurrent approach attain the problem of polysemy.\n\nOverall, a very strong paper, well structured and clear. The experimentation is\ncorrect and the qualitative analysis made in table 1 shows results as expected\nfrom the approach.  There\u2019s not much that can be faulted and all my comments\nbelow are meant to help the paper gain additional clarity. \n\nSome comments: \n\n_ It may be interesting to include a brief explanation of the differences\nbetween the approach from Tian et al. 2014 and the current one. Both split\nsingle word representation into multiple prototypes by using a mixture model. \n\n_ There are some missing citations that could me mentioned in related work as :\n\nEfficient Non-parametric Estimation of Multiple Embeddings per Word in Vector\nSpace Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014\nDo Multi-Sense Embeddings Improve Natural Language Understanding? Li and\nJurafsky, EMNLP 2015\nTopical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015\n\n_ Also, the inclusion of the result from those approaches in tables 3 and 4\ncould be interesting. \n\n_ A question to the authors: What do you attribute the loss of performance of\nw2gm against w2g in the analysis of SWCS?\n\nI have read the response.\nThis work uses Gaussian mixtures to represent words and demonstrates its\npotential in capturing multiple word meanings for polysemy. The training\nprocess is done based on a max-margin objective. The expected likelihood kernel\nis used as the similarity between two words' distributions. Experiment results\non word similarity and entailment tasks show the effectiveness of the proposed\nwork.\n\n- Strengths:\n\nThe problem is clearly motivated and defined. Gaussian mixtures are much more\nexpressive than deterministic vector representations. It can potentially\ncapture different word meanings by its modes, along with probability mass and\nuncertainty around those modes. This work represents an important contribution\nto word embedding. \n\nThis work propose a max-margin learning objective with closed-form similarity\nmeasurement for efficient training.\n\nThis paper is mostly well written. \n\n- Weaknesses:\n\nSee below for some questions. \n\n- General Discussion:\n\nIn the Gaussian mixture models, the number of gaussian components (k) is\nusually an important parameter. In the experiments of this paper, k is set to\n2. What is your criteria to select k? Does the increase of k hurt the\nperformance of this model? What does the learned distribution look like for a\nword that only has one popular meaning?\n\nI notice that you use the spherical case in all the experiments (the covariance\nmatrix reduces to a single number). Is this purely for computation efficiency?\nI wonder what's the performance of using a general diagonal covariance matrix.\nSince in this more general case, the gaussian mixture defines different degrees\nof uncertainty along different directions in the semantic space, which seems\nmore interesting.\n\nMinor comments:\nTable 4 is not referred to in the text.\nIn reference, Luong et al. lacks the publication year.\n\nI have read the response.",
        "partial_review": "Review: Multimodal Word Distributions\n\n- Strengths:  Overall a very strong paper.\n\n- Weaknesses: The comparison against similar approaches could be extended.\n\n- General Discussion:\n\nThe main focus of this paper is the introduction of a new model for learning\nmultimodal word distributions formed from Gaussian mixtures for multiple word\nmeanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which\nrepresented word as unimodal Gaussian distribution. By using a multimodal, the\ncurrent approach attain the problem of polysemy.\n\nOverall, a very strong paper, well structured and clear. The experimentation is\ncorrect and the qualitative analysis made in table 1 shows results as expected\nfrom the approach.  There\u2019s not much that can be faulted and all my comments\nbelow are meant to help the paper gain additional clarity. \n\nSome comments: \n\n_ It may be interesting to include a brief explanation of the differences\nbetween the approach from Tian et al. 2014 and the current one. Both split\nsingle word representation into multiple prototypes by using a mixture model. \n\n_ There are some missing citations that could me mentioned in related work as :\n\nEfficient Non-parametric Estimation of Multiple Embeddings per Word in Vector\nSpace Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014\nDo Multi-Sense Embeddings Improve Natural Language Understanding? Li and\nJurafsky, EMNLP 2015\nTopical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015\n\n_ Also, the inclusion of the result from those approaches in tables 3 and 4\ncould be interesting. \n\n_ A question to the authors: What do you attribute the loss of performance of\nw2gm against w2g in the analysis of SW",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Multimodal Word Distributions \n\nHere is the abstract of the paper: Word embeddings provide point representations of words containing useful semantic information.  We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information.  To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely  expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment. \n\nHere is the partial review: Review: Multimodal Word Distributions\n\n- Strengths:  Overall a very strong paper.\n\n- Weaknesses: The comparison against similar approaches could be extended.\n\n- General Discussion:\n\nThe main focus of this paper is the introduction of a new model for learning\nmultimodal word distributions formed from Gaussian mixtures for multiple word\nmeanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which\nrepresented word as unimodal Gaussian distribution. By using a multimodal, the\ncurrent approach attain the problem of polysemy.\n\nOverall, a very strong paper, well structured and clear. The experimentation is\ncorrect and the qualitative analysis made in table 1 shows results as expected\nfrom the approach.  There\u2019s not much that can be faulted and all my comments\nbelow are meant to help the paper gain additional clarity. \n\nSome comments: \n\n_ It may be interesting to include a brief explanation of the differences\nbetween the approach from Tian et al. 2014 and the current one. Both split\nsingle word representation into multiple prototypes by using a mixture model. \n\n_ There are some missing citations that could me mentioned in related work as :\n\nEfficient Non-parametric Estimation of Multiple Embeddings per Word in Vector\nSpace Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014\nDo Multi-Sense Embeddings Improve Natural Language Understanding? Li and\nJurafsky, EMNLP 2015\nTopical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015\n\n_ Also, the inclusion of the result from those approaches in tables 3 and 4\ncould be interesting. \n\n_ A question to the authors: What do you attribute the loss of performance of\nw2gm against w2g in the analysis of SW \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Review: Multimodal Word Distributions\\n\\n- Strengths: Overall a very strong paper.\\n\\n- Weaknesses: The comparison against similar approaches could be extended.\\n\\n- General Discussion:\\n\\nThe main focus of this paper is the introduction of a new model for learning multimodal word distributions formed from Gaussian mixtures for multiple word meanings. i. e. representing a word by a set of many Gaussian distributions. The approach extends the model introduced by Vilnis and McCallum (2014) which represented words as unimodal Gaussian distributions. By using a multimodal approach, the current model tackles the problem of polysemy.\\n\\nOverall, this is a very strong paper, well structured and clear. The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach. There\u2019s not much that can be faulted and all my comments below are meant to help the paper gain additional clarity.\\n\\nSome comments:\\n\\n- It may be interesting to include a brief explanation of the differences between the approach from Tian et al. (2014) and the current one. Both split the single word representation into multiple prototypes by using a mixture model.\\n\\n- There are some missing citations that could be mentioned in the related work section, such as:\\n  - Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space by Neelakantan, A., Shankar. J. Passos, A., McCallum (EMNLP 2014)\\n  - Do Multi-Sense Embeddings Improve Natural Language Understanding? by Li and Jurafsky (EMNLP 2015)\\n  - Topical Word Embeddings by Liu Y., Liu Z., Chua T., Sun M. (AAAI 2015)\\n\\n- Also, the inclusion of the results from those approaches in tables 3 and 4 could be interesting.\\n\\n- A question to the authors: What do you attribute the loss of performance of w2gm against w2g in the analysis of SW?\"\n}",
        "human_end_index": 1769
    },
    {
        "id": 150,
        "source": "PeerRead/acl_2017/",
        "title": "Deep Character-Level Neural Machine Translation By Learning Morphology",
        "human_review": "- Strengths:\n\nThe authors present a novel adaptation of encoder-decoder neural MT using an\napproach that starts and ends with characters, but in between works with\nrepresentations of morphemes and characters. \n\nThe authors release both their code as well as their final learned models for\nfr-en, cs-en, and en-cs. This is helpful in validating their work, as well as\nfor others looking to replicate and extends this work.\n\nThe system reported appears to produce translation results of reasonable\nquality even after the first training epoch, with continued progress in future\nepochs.\n\nThe system appears to learn reasonable morphological tokenizations, and appears\nable to handle previously unseen words (even nonce words) by implicitly backing\noff to morphemes.\n\n- Weaknesses:\n\nIn the paper, the authors do not explicitly state which WMT test and dev sets\ntheir results are reported on. This is problematic for readers wishing to\ncompare the reported results to existing work (for example, the results at\nmatrix.statmt.org). The only way this reviewer found to get this information\nwas to look in the README of the code supplement, which indicates that the test\nset was newstest2015 and the dev test was newstest2013. This should have been\nexplicitly described in the paper.\n\nThe instructions given in the software README are OK, but not great. The\ntraining and testing sections each could be enhanced with explicit examples of\nhow to run the respective commands. The software itself should respond to a\n--help flag, which it currently does not.\n\nThe paper describes a 6-level architecture, but the diagram in Figure 2 appears\nto show fewer than 6 layers. What's going on? The caption should be more\nexplicit, and if this figure is not showing all of the layers, then there\nshould be a figure somewhere (even if it's in an appendix) showing all of the\nlayers.\n\nThe results show comparison to other character-based neural systems, but do not\nshow state-of-the-art results for other types of MT system. WMT (and\nmatrix.statmt.org) has reported results for other systems on these datasets,\nand it appears that the state-of-the-art is much higher than any of the results\nreported in this paper. That should be acknowledged, and ideally should be\ndiscussed.\n\nThere are a handful of minor English disfluencies, misspellings, and minor\nLaTeX issues, such as reverse quotation marks. These should be corrected.\n\n- General Discussion:\n\nPaper is a nice contribution to the existing literature on character-based\nneural MT.\n- Strengths: In general, the paper is well structured and clear. It is possible\nto follow most of the explanation, the ideas presented are original and the\nresults obtained are quite interesting.\n\n- Weaknesses: I have some doubts about the interpretation of the results. In\naddition, I think that some of the claims regarding the capability of the\nmethod proposed to learn morphology are not propperly backed by scientific\nevidence.\n\n- General Discussion:\n\nThis paper explores a complex architecture for character-level neural machine\ntranslation (NMT). The proposed architecture extends a classical\nencoder-decoder architecture by adding a new deep word-encoding layer capable\nof encoding the character-level input into sub-word representations of the\nsource-language sentence. In the same way, a deep word-decoding layer is added\nto the output to transform the target-language sub-word representations into a\ncharacter sequence as the final output of the NMT system. The objective of such\narchitecture is to take advantage of the benefits of character-level NMT\n(reduction of the size of the vocabulary and flexibility to deal with unseen\nwords) and, at the same time, improving the performance of the whole system by\nusing an intermediate representation of sub-words to reduce the size of the\ninput sequence of characters. In addition, the authors claim that their deep\nword-encoding model is able to learn morphology better than other\nstate-of-the-art approaches.\n\nI have some concerns regarding the evaluation. The authors compare their\napproach to other state-of-the-art systems taking into account two parameters:\ntraining time and BLEU score. However, I do not clearly see the advantage of\nthe model proposed (DCNMT) in front of other approaches such as bpe2char. The\ndifference between both approaches as regards BLEU score is very small (0.04 in\nCs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming\nthe other one without statistical significance information: has statistical\nsignificance been evaluated? As regards the training time, it is worth\nmentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs\ntraining time is not provided (why not?) and for En-Fr bpe2char is not\nevaluated. I think that a more complete comparison with this system should be\ncarried out to prove the advantages of the model proposed.\n\nMy second concern is on the 5.2 Section, where authors start claiming that they\ninvestigated about the ability of their system to learn morphology. However,\nthe section only contains a examples and some comments on them. Even though\nthese examples are very well chosen and explained in a very didactic way, it is\nworth noting that no experiments or formal evaluation seem to have been\ncarried out to support the claims of the authors. I would definitely encourage\nauthors to extend this very interesting part of the paper that could even\nbecome a different paper itself. On the other hand, this Section does not seem\nto be a critical point of the paper, so for the current work I may suggest just\nto move this section to an appendix and soften some of the claims done\nregarding the capabilities of the system to learn morphology.\n\nOther comments, doubts and suggestions:\n\n - There are many acronyms that are used but are not defined (such as LSTM,\nHGRU, CNN or PCA) or which are defined after starting to use them (such as RNN\nor BPE). Even though some of these acronyms are well known in the field of deep\nlearning, I would encourage the authors to defined them to improve clearness.\n\n - The concept of energy is mentioned for the first time in Section 3.1. Even\nthough the explanation provided is enough at that point, it would be nice to\nrefresh the idea of energy in Section 5.2 (where it is used several times) and\nproviding some hints about how to interpret it: a high energy on a character\nwould be indicating that the current morpheme should be split at that point? In\naddition, the concept of peak (in Figure 5) is not described.\n\n - When the acronym BPE is defined, capital letters are used, but then, for the\nrest of mentions it is lower cased; is there a reason for this?\n\n - I am not sure if it is necessary to say that no monolingual corpus is used\nin Section 4.1.\n\n - It seems that there is something wrong with Figure 4a since the colours for\nthe energy values are not shown for every character.\n\n - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not\ntaken from the papers, since they are not reported. If the authors computed\nthese results by themselves (as it seems) they should mention it.\n\n - I would not say that French is morphologically poor, but rather that it is\nnot that rich as Slavic languages such as Czech.\n\n - Why a link is provided for WMT'15 training corpora but not for WMT'14?\n\n - Several references are incomplete\n\nTypos:\n\n  - \"..is the bilingual, parallel corpora provided...\" -> \"..are the bilingual,\nparallel corpora provided...\"\n\n  - \"Luong and Manning (2016) uses\" -> \"Luong and Manning (2016) use\"\n\n  - \"HGRU (It is\" -> \"HGRU (it is\"\n\n  - \"coveres\" -> \"covers\"\n\n  - \"both consists of two-layer RNN, each has 1024\" -> \"both consist of\ntwo-layer RNN, each have 1024\"\n\n  - \"the only difference between CNMT and DCNMT is CNMT\" -> \"the only\ndifference between CNMT and DCNMT is that CNMT\"",
        "partial_review": "- Strengths:\n\nThe authors present a novel adaptation of encoder-decoder neural MT using an\napproach that starts and ends with characters, but in between works with\nrepresentations of morphemes and characters. \n\nThe authors release both their code as well as their final learned models for\nfr-en, cs-en, and en-cs. This is helpful in validating their work, as well as\nfor others looking to replicate and extends this work.\n\nThe system reported appears to produce translation results of reasonable\nquality even after the first training epoch, with continued progress in future\nepochs.\n\nThe system appears to learn reasonable morphological tokenizations, and appears\nable to handle previously unseen words (even nonce words) by implicitly backing\noff to morphemes.\n\n- Weaknesses:\n\nIn the paper, the authors do not explicitly state which WMT test and dev sets\ntheir results are reported on. This is problematic for readers wishing to\ncompare the reported results to existing work (for example, the results at\nmatrix.statmt.org). The only way this reviewer found to get this information\nwas to look in the README of the code supplement, which indicates that the test\nset was newstest2015 and the dev test was newstest2013. This should have been\nexplicitly described in the paper.\n\nThe instructions given in the software README are OK, but not great. The\ntraining and testing sections each could be enhanced with explicit examples of\nhow to run the respective commands. The software itself should respond to a\n--help flag, which it currently does not.\n\nThe paper describes a 6-level architecture, but the diagram in Figure 2 appears\nto show fewer than 6 layers. What's going on? The caption should be more\nexplicit, and if this figure is not showing all of the layers, then there\nshould be a figure somewhere (even if it's in an appendix) showing all of the\nlayers.\n\nThe results show comparison to other character-based neural systems, but do not\nshow state-of-the-art results for other types of MT system. WMT (and\nmatrix.statmt.org) has reported results for other systems on these datasets,\nand it appears that the state-of-the-art is much higher than any of the results\nreported in this paper. That should be acknowledged, and ideally should be\ndiscussed.\n\nThere are a handful of minor English disfluencies, misspellings, and minor\nLaTeX issues, such as reverse quotation marks. These should be corrected.\n\n- General Discussion:\n\nPaper is a nice contribution to the existing literature on character-based\nneural MT.\n- Strengths: In general, the paper is well structured and clear. It is possible\nto follow most of the explanation, the ideas presented are original and the\nresults obtained are quite interesting.\n\n- Weaknesses: I have some doubts about the interpretation of the results. In\naddition, I think that some of the claims regarding the capability of the\nmethod proposed to learn morphology are not propperly backed by scientific\nevidence.\n\n- General Discussion:\n\nThis paper explores a complex architecture for character-level neural machine\ntranslation (NMT). The proposed architecture extends a classical\nencoder-decoder architecture by adding a new deep word-encoding layer capable\nof encoding the character-level input into sub-word representations of the\nsource-language sentence. In the same way, a deep word-decoding layer is added\nto the output to transform the target-language sub-word representations into a\ncharacter sequence as the final output of the NMT system. The objective of such\narchitecture is to take advantage of the benefits of character-level NMT\n(reduction of the size of the vocabulary and flexibility to deal with unseen\nwords) and, at the same time, improving the performance of the whole system by\nusing an intermediate representation of sub-words to reduce the size of the\ninput sequence of characters. In addition, the authors claim that their deep\nword-encoding model is able to learn morphology bette",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Deep Character-Level Neural Machine Translation By Learning Morphology \n\nHere is the abstract of the paper: Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models and conventional character-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Moreover, the final BLEU score of our model is comparable to the state-of-the-art systems. Further analyses show that our model is able to learn morphology. \n\nHere is the partial review: - Strengths:\n\nThe authors present a novel adaptation of encoder-decoder neural MT using an\napproach that starts and ends with characters, but in between works with\nrepresentations of morphemes and characters. \n\nThe authors release both their code as well as their final learned models for\nfr-en, cs-en, and en-cs. This is helpful in validating their work, as well as\nfor others looking to replicate and extends this work.\n\nThe system reported appears to produce translation results of reasonable\nquality even after the first training epoch, with continued progress in future\nepochs.\n\nThe system appears to learn reasonable morphological tokenizations, and appears\nable to handle previously unseen words (even nonce words) by implicitly backing\noff to morphemes.\n\n- Weaknesses:\n\nIn the paper, the authors do not explicitly state which WMT test and dev sets\ntheir results are reported on. This is problematic for readers wishing to\ncompare the reported results to existing work (for example, the results at\nmatrix.statmt.org). The only way this reviewer found to get this information\nwas to look in the README of the code supplement, which indicates that the test\nset was newstest2015 and the dev test was newstest2013. This should have been\nexplicitly described in the paper.\n\nThe instructions given in the software README are OK, but not great. The\ntraining and testing sections each could be enhanced with explicit examples of\nhow to run the respective commands. The software itself should respond to a\n--help flag, which it currently does not.\n\nThe paper describes a 6-level architecture, but the diagram in Figure 2 appears\nto show fewer than 6 layers. What's going on? The caption should be more\nexplicit, and if this figure is not showing all of the layers, then there\nshould be a figure somewhere (even if it's in an appendix) showing all of the\nlayers.\n\nThe results show comparison to other character-based neural systems, but do not\nshow state-of-the-art results for other types of MT system. WMT (and\nmatrix.statmt.org) has reported results for other systems on these datasets,\nand it appears that the state-of-the-art is much higher than any of the results\nreported in this paper. That should be acknowledged, and ideally should be\ndiscussed.\n\nThere are a handful of minor English disfluencies, misspellings, and minor\nLaTeX issues, such as reverse quotation marks. These should be corrected.\n\n- General Discussion:\n\nPaper is a nice contribution to the existing literature on character-based\nneural MT.\n- Strengths: In general, the paper is well structured and clear. It is possible\nto follow most of the explanation, the ideas presented are original and the\nresults obtained are quite interesting.\n\n- Weaknesses: I have some doubts about the interpretation of the results. In\naddition, I think that some of the claims regarding the capability of the\nmethod proposed to learn morphology are not propperly backed by scientific\nevidence.\n\n- General Discussion:\n\nThis paper explores a complex architecture for character-level neural machine\ntranslation (NMT). The proposed architecture extends a classical\nencoder-decoder architecture by adding a new deep word-encoding layer capable\nof encoding the character-level input into sub-word representations of the\nsource-language sentence. In the same way, a deep word-decoding layer is added\nto the output to transform the target-language sub-word representations into a\ncharacter sequence as the final output of the NMT system. The objective of such\narchitecture is to take advantage of the benefits of character-level NMT\n(reduction of the size of the vocabulary and flexibility to deal with unseen\nwords) and, at the same time, improving the performance of the whole system by\nusing an intermediate representation of sub-words to reduce the size of the\ninput sequence of characters. In addition, the authors claim that their deep\nword-encoding model is able to learn morphology bette \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper explores a complex architecture for character-level neural machine translation (NMT). The proposed architecture extends a classical encoder-decoder architecture by adding a new deep word-encoding layer capable of encoding the character-level input into sub-word representations of the source-language sentence. In the same way, a deep word-decoding layer is added to the output to transform the target-language sub-word representations into a character sequence as the final output of the NMT system. The objective of such architecture is to take advantage of the benefits of character-level NMT (reduction of the size of the vocabulary and flexibility to deal with unseen words) and, at the same time, improving the performance of the whole system by using an intermediate representation of sub-words to reduce the size of the input sequence of characters. In addition, the authors claim that their deep word-encoding model is able to learn morphology better than other existing systems. \n\nThe strengths of the paper include the well-structured and clear presentation of ideas, the originality of the proposed approach, and the interesting results obtained. The authors also release their code and learned models, which is helpful for validation and replication of the work.\n\nHowever, there are some weaknesses in the paper. The authors do not explicitly state which WMT test and dev sets their results are reported on, making it difficult for readers to compare their results to existing work. The instructions given in the software README could be improved with explicit examples of how to run the commands. The paper describes a 6-level architecture, but the diagram in Figure 2 appears to show fewer than 6 layers, which should be clarified. The results only compare to other character-based neural systems and do not discuss state-of-the-art results for other types of MT systems. There are also some minor English disfluencies, misspellings, and LaTeX issues that should be corrected.\n\nFurthermore, the interpretation of the results and some claims regarding the capability of the proposed method to learn morphology are not properly backed by scientific evidence.\n\nIn general, this paper is a valuable contribution to the literature on character-based NMT, but some improvements are needed in terms of clarity, reproducibility, and discussing the limitations of the proposed method.\"\n}",
        "human_end_index": 3939
    },
    {
        "id": "16",
        "source": "PeerRead/acl_2017/",
        "title": "Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms",
        "human_review": "- Strengths:\nThis paper tries to use the information from arguments, which is usually\nignored yet actually quite important, to improve the performance of event\ndetection. The framework is clear and simple. With the help of the supervised\nattention mechanism, an important method that has been used in many tasks such\nas machine translation, the performance of their system outperforms the\nbaseline significantly.\n\n- Weaknesses:\n The attention vector is simply the summation of two attention vectors of each\npart. Maybe the attention vector could be calculated in a more appropriate\napproach. For the supervised attention mechanism, two strategies are proposed.\nBoth of them are quite straightforward. Some more complicated strategies can\nwork better and can be tried.\n\n- General Discussion:\n Although there are some places that can be improved, this paper proposed a\nquite effective framework, and the performance is good. The experiment is\nsolid. It can be considered to be accepted.",
        "partial_review": "- Strengths:\nThis paper tries to use the information from arguments, which is usually\nignored yet actually quite important, to improve the performance of event\ndetection. The framework is clear and simple. With the help of the supervised\nattention mechanism, an important method that has been used in many tasks such\nas machine translation, the performance of their system outperforms the\nbaseline significantly.\n\n- Weaknesses:\n The attention vector is simply the summation of two attention vectors of each\np",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms \n\nHere is the abstract of the paper: This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset. \n\nHere is the partial review: - Strengths:\nThis paper tries to use the information from arguments, which is usually\nignored yet actually quite important, to improve the performance of event\ndetection. The framework is clear and simple. With the help of the supervised\nattention mechanism, an important method that has been used in many tasks such\nas machine translation, the performance of their system outperforms the\nbaseline significantly.\n\n- Weaknesses:\n The attention vector is simply the summation of two attention vectors of each\np \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"I found this paper to be quite interesting and well-written. The authors address the task of event detection and argue that arguments provide valuable clues for this task. I agree with their perspective as arguments are often ignored in existing approaches. The proposed model, which exploits argument information explicitly using supervised attention mechanisms, is clear and simple. The authors systematically investigate the model under different attention strategies and achieve the best F1 score on the ACE 2005 dataset. This demonstrates the effectiveness of their approach. However, one weakness I observed is that the attention vector is simply the summation of two attention vectors of each p. This might limit the model's ability to capture more complex relationships between arguments and events. Overall, I believe this paper makes a valuable contribution to the field of event detection by leveraging argument information and utilizing supervised attention mechanisms.\"\n}",
        "human_end_index": 508
    },
    {
        "id": 169,
        "source": "PeerRead/acl_2017/",
        "title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction",
        "human_review": "- Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either \u201cGood\u201d or \u201cAcceptable\u201d\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa\nThe paper presents a novel approach for evaluating grammatical error\ncorrection (GEC) systems. This approach makes it possible to assess\nthe performance of GEC systems by error type not only in terms of\nrecall but also in terms of precision, which was previously not\npossible in general since system output is usually not annotated with\nerror categories.\n\nStrengths:\n\n - The proposed evaluation is an important stepping stone for\n   analyzing GEC system behavior.\n - The paper includes evaluation for a variety of systems.\n - The approach has several advantages over previous work:\n   - it computes precision by error type\n   - it is independent of manual error annotation\n   - it can assess the performance on multi token errors\n - The automatically selected error tags for pre-computed error spans\n   are mostly approved of by human experts\n\nWeaknesses:\n\n - A key part \u2013 the rules to derive error types \u2013 are not described.\n - The classifier evaluation lacks a thorough error analysis and based\n   upon that it lacks directions of future work on how to improve the\n   classifier.\n - The evaluation was only performed for English and it is unclear how\n   difficult it would be to use the approach on another language.\n\nClassifier and Classifier Evaluation\n====================================\n\nIt is unclear on what basis the error categories were devised. Are\nthey based on previous work?\n\nAlthough the approach in general is independent of the alignment\nalgorithm, the rules are probably not, but the authors don't provide\ndetails on that.  The error categories are a major part of the paper\nand the reader should at least get a glimpse of how a rule to assign\nan error type looks like.\n\nUnfortunately, the paper does not apply the proposed evaluation on\nlanguages other than English.  It also does not elaborate on what\nchanges would be necessary to run the classifier on other languages. I\nassume that the rules used for determining edit boundaries as well as\nfor determining the error tags depend on the language/the\npre-processing pipeline to a certain extent and therefore need to be\nadapted. Also, the error categories might need to be changed.  The\nauthors do not provide any detail on the rules for assigning error\ncategories (how many are there overall/per error type? how complex are\nthey?) to estimate the effort necessary to use the approach on another\nlanguage.\n\nThe error spans computed in the pre-processing step seem to be\ninherently continuous (which is also the case with the M2 scorer), which\nis problematic since there are errors which can only be tagged\naccurately when the error span is discontinuous. In German, for\nexample, verbs with separable prefixes are separated from each other\nin the main clause: [1st constituent] [verb] [other constituents]\n[verb prefix]. Would the classifier be able to tag discontinuous edit\nspans?\n\nThe authors write that all human judges rated at least 95\\% of the\nautomatically assigned error tags as appropriate \"despite the degree\nof noise introduced by automatic edit extraction\" (295). I would be\nmore cautious with this judgment since the raters might also have been\nmore forgiving when the boundaries were noisy. In addition, they were\nnot asked to select a tag without knowing the system output but could\nin case of noisy boundaries be more biased towards the system\noutput. Additionally, there was no rating option between \"Bad (Not\nAppropriate)\" and \"Appropriate\", which might also have led raters to\nselect \"Appropriate\" over \"Bad\". To make the evaluation more sound,\nthe authors should also evaluate how the human judges rate the\nclassifier output if the boundaries were manually created,\ni.e. without the noise introduced by faulty boundaries.\n\nThe classifier evaluation lacks a thorough error analysis. It is only\nmentioned that \"Bad\" is usually traced back to a wrong POS\ntag. Questions I'd like to see addressed: When did raters select\n\"Bad\", when \"Appropriate\"? Does the rating by experts point at\npossibilities to improve the classifier?\n\nGold Reference vs. Auto Reference\n=================================\n\nIt is unclear on what data the significance test was performed\nexactly. Did you test on the F0.5 scores? If so, I don't think this is\na good idea since it is a derived measure with weak discriminative\npower (the performance in terms of recall an precision can be totally\ndifferent but have the same F0.5 score). Also, at the beginning of\nSection 4.1 the authors refer to the mismatch between automatic and\nreference in terms of alignment and classification but as far as I can\ntell, the comparison between gold and reference is only in terms of\nboundaries and not in terms of classification.\n\nError Type Evaluation\n=====================\n\nI do not think it is surprising that 5 teams (~line 473) failed to correct\nany unnecessary token error. For at least two of the systems there is\na straightforward explanation why they cannot handle superfluous\nwords. The most obvious is UFC: Their rule-base approach works on POS\ntags (Ng et al., 2014) and it is just not possible to determine\nsuperfluous words based on POS alone. Rozovskaya & Roth (2016) provide\nan explanation why AMU performs poorly on superfluous words.\n\nThe authors do not analyze or comment the results in Table 6 with\nrespect to whether the systems were designed to handle the error\ntype. For some error types, there is a straight-forward mapping\nbetween error type in the gold standard and in the auto reference, for\nexample for word order error. It remains unclear whether the systems\nfailed completely on specific error types or were just not designed to\ncorrect them (CUUI for example is reported with precision+recall=0.0,\nalthough it does not target word order errors). In the CUUI case (and\nthere are probably similar cases), this also points at an error in the\nclassification which is neither analyzed nor discussed.\n\nPlease report also raw values for TP, FP, TN, FN in the appendix for\nTable 6. This makes it easier to compare the systems using other\nmeasures. Also, it seems that for some error types and systems the\nresults in Table 6 are based only on a few instances. This would also\nbe made clear when reporting the raw values.\n\nYour write \"All but 2 teams (IITB and IPN) achieved the best score in\nat least 1 category, which suggests that different approaches to GEC\ncomplement different error types.\" (606) It would be nice to mention\nhere that this is in line with previous research.\n\nMulti-token error analysis is helpful for future work but the result\nneeds more interpretation: Some systems are probably inherently unable\nto correct such errors but none of the systems were trained on a\nparallel corpus of learner data and fluent (in the sense of Sakaguchi\net al, 2016) corrections.\n\nOther\n=====\n\n- The authors should have mentioned that for some of the GEC\n  approaches, it was not impossible before to provide error\n  annotations, e.g. systems with submodules for one error type each.\n  Admittedly, the system would need to be adapted to include the\n  submodule responsible for a change in the system output. Still, the\n  proposed approach enables to compare GEC systems for which producing\n  an error tagged output is not straightforward to other systems in a\n  unified way.\n- References: Some titles lack capitalizations. URL for Sakaguchi et\n  al. (2016) needs to be wrapped. Page information is missing for\n  Efron and Tibshirani (1993).\n\nAuthor response\n===============\n\nI agree that your approach is not \"fatally flawed\" and I think this review\nactually points out quite some positive aspects. The approach is good, but the\npaper is not ready.\n\nThe basis for the paper are the rules for classifying errors and the lack of\ndescription is a major factor.        This is not just a matter about additional\nexamples. If the rules are not seen as a one-off implementation, they need to\nbe described to be replicable or to adapt them.\n\nGeneralization to other languages should not be an afterthought.  It would be\nserious limitation if the approach only worked on one language by design.  Even\nif you don't perform an adaption for other languages, your approach should be\ntransparent enough for others to estimate how much work such an adaptation\nwould be and how well it could reasonably work.  Just stating that most\nresearch is targeted at ESL only reinforces the problem.\n\nYou write that the error types certain systems tackle would be \"usually obvious\nfrom the tables\".  I don't think it is as simple as that -- see the CUUI\nexample mentioned above as well as the unnecessary token errors.  There are\nfive systems that don't correct them (Table 5) and it should therefore be\nobvious that they did not try to tackle them. However, in the paper you write\nthat \"There\nis also no obvious explanation as to why these teams had difficulty with this\nerror type\".",
        "partial_review": "- Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either \u201cGood\u201d or \u201cAcceptable\u201d\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa\nThe paper presents a novel approach for evaluating grammatical error\ncorrection (GEC) systems. This approach makes it possible to assess\nthe performance of GEC systems by error type not only in terms of\nrecall but also in terms of precision, which was previously not\npossible in general since system output is usually not annotated with\nerror categories.\n\nStrengths:\n\n - The proposed evaluation is an important stepping stone for\n   analyzing GEC system behavior.\n - The paper includes evaluation for a variety of systems.\n - The approach has several advantages over previous work:\n   - it computes precision by error type\n   - it is independent of manual error annotation\n   - it can assess the performance on multi token errors\n - The automatically selected error tags for pre-computed error spans\n   are mostly approved of by human experts\n\nWeaknesses:\n\n - A key part \u2013 the rules to derive error types \u2013 are not described.\n - The classifier evaluation lacks a thorough error analysis and based\n   upon that it lacks directions of future work on how to improve the\n   classifier.\n - The evaluation was only performed for English and it is unclear how\n   difficult it would be to use the approach on another language.\n\nClassifier and Classifier Evaluation\n====================================\n\nIt is unclear on what basis the error categories were devised. Are\nthey based on previous work?\n\nAlthough the approach in general is independent of the alignment\nalgorithm, the rules are probably not, but the authors don't provide\ndetails on that.  The error categories are a major part of the paper\nand the reader should at least get a glimpse of how a rule to assign\nan error type looks like.\n\nUnfortunately, the paper does not apply the proposed evaluation on\nlanguages other than English.  It also does not elaborate on what\nchanges would be necessary to run the classifier on other languages. I\nassume that the rules used for determining edit boundaries as well as\nfor determining the error tags depend on the language/the\npre-processing pipeline to a certain extent and therefore need to be\nadapted. Also, the error categories might need to be changed.  The\nauthors do not provide any detail on the rules for assignin",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction \n\nHere is the abstract of the paper: Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as ``Good'' or ``Acceptable'' in at least 95\\% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time. \n\nHere is the partial review: - Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either \u201cGood\u201d or \u201cAcceptable\u201d\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa\nThe paper presents a novel approach for evaluating grammatical error\ncorrection (GEC) systems. This approach makes it possible to assess\nthe performance of GEC systems by error type not only in terms of\nrecall but also in terms of precision, which was previously not\npossible in general since system output is usually not annotated with\nerror categories.\n\nStrengths:\n\n - The proposed evaluation is an important stepping stone for\n   analyzing GEC system behavior.\n - The paper includes evaluation for a variety of systems.\n - The approach has several advantages over previous work:\n   - it computes precision by error type\n   - it is independent of manual error annotation\n   - it can assess the performance on multi token errors\n - The automatically selected error tags for pre-computed error spans\n   are mostly approved of by human experts\n\nWeaknesses:\n\n - A key part \u2013 the rules to derive error types \u2013 are not described.\n - The classifier evaluation lacks a thorough error analysis and based\n   upon that it lacks directions of future work on how to improve the\n   classifier.\n - The evaluation was only performed for English and it is unclear how\n   difficult it would be to use the approach on another language.\n\nClassifier and Classifier Evaluation\n====================================\n\nIt is unclear on what basis the error categories were devised. Are\nthey based on previous work?\n\nAlthough the approach in general is independent of the alignment\nalgorithm, the rules are probably not, but the authors don't provide\ndetails on that.  The error categories are a major part of the paper\nand the reader should at least get a glimpse of how a rule to assign\nan error type looks like.\n\nUnfortunately, the paper does not apply the proposed evaluation on\nlanguages other than English.  It also does not elaborate on what\nchanges would be necessary to run the classifier on other languages. I\nassume that the rules used for determining edit boundaries as well as\nfor determining the error tags depend on the language/the\npre-processing pipeline to a certain extent and therefore need to be\nadapted. Also, the error categories might need to be changed.  The\nauthors do not provide any detail on the rules for assignin \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents a novel approach for evaluating grammatical error correction (GEC) systems. This approach makes it possible to assess the performance of GEC systems by error type not only in terms of recall but also in terms of precision, which was previously not possible in general since system output is usually not annotated with error categories.\\n\\nStrengths:\\n\\n- The proposed evaluation is an important stepping stone for analyzing GEC system behavior.\\n- The paper includes evaluation for a variety of systems.\\n- The approach has several advantages over previous work:\\n   - it computes precision by error type\\n   - it is independent of manual error annotation\\n   - it can assess the performance on multi token errors\\n- The automatically selected error tags for pre-computed error spans are mostly approved by human experts\\n\\nWeaknesses:\\n\\n- A key part \\u2013 the rules to derive error types \\u2013 are not described.\\n- The classifier evaluation lacks a thorough error analysis and based upon that it lacks directions for future work on how to improve the classifier.\\n- The evaluation was only performed for English and it is unclear how difficult it would be to use the approach on another language.\\n\\nClassifier and Classifier Evaluation\\n====================================\\n\\nIt is unclear on what basis the error categories were devised. Are they based on previous work?\\n\\nAlthough the approach in general is independent of the alignment algorithm, the rules are probably not, but the authors don't provide details on that. The error categories are a major part of the paper and the reader should at least get a glimpse of how a rule to assign an error type looks like.\\n\\nUnfortunately, the paper does not apply the proposed evaluation on languages other than English. It also does not elaborate on what changes would be necessary to run the classifier on other languages. I assume that the rules used for determining edit boundaries as well as for determining the error tags depend on the language/the pre-processing pipeline to a certain extent and therefore need to be adapted. Also, the error categories might need to be changed. The authors do not provide any detail on the rules for assignin\\n\\nOutput in the following json format:\\n{\\n    \\\"complete_review\\\": \\\"\\\"\\n}\"\n}",
        "human_end_index": 6624
    },
    {
        "id": 178,
        "source": "PeerRead/acl_2017/",
        "title": "A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words",
        "human_review": "The paper describes an extension of word embedding methods to also provide\nrepresentations for phrases and concepts that correspond to words.  The method\nworks by fixing an identifier for groups of phrases, words and the concept that\nall denote this concept, replace the occurrences of the phrases and words by\nthis identifier in the training corpus, creating a \"tagged\" corpus, and then\nappending the tagged corpus to the original corpus for training.  The\nconcept/phrase/word sets are taken from an ontology.  Since the domain of\napplication is biomedical, the related corpora and ontologies are used.  The\nresearchers also report on the generation of a new test dataset for word\nsimilarity and relatedness for real-world entities, which is novel.\n\nIn general, the paper is nicely written.  The technique is pretty natural,\nthough not a very substantial contribution. The scope of the contribution is\nlimited, because of focused evaluation within the biomedical domain.\n\nMore discussion of the generated test resource could be useful.  The resource\ncould be the true interesting contribution of the paper.\n\nThere is one\nsmall technical problem, but that is probably just a matter of mathematical\nexpression than implementation.\n\nTechnical problem:\nEq. 8: The authors want to define the MAP calculation.                          This\nis a\ngood\nidea,\nthought I think that a natural cut-off could be defined, rather than ranking\nthe entire vocabulary.                          Equation 8 does not define a\nprobability;\nit is\nquite\neasy to show this, even if the size of the vocabulary is infinite.  So you need\nto change the explanation (take out talk of a probability).\n\nSmall corrections:\nline:\n556: most concepts has --> most concepts have\nSummary: This paper presents a model for embedding words, phrases and concepts\ninto vector spaces. To do so, it uses an ontology of concepts, each of which is\nmapped to phrases. These phrases are found in text corpora and treated as\natomic symbols. Using this, the paper uses what is essentially the skip-gram\nmethod to train embeddings for words, the now atomic phrases and also the\nconcepts associated with them. The proposed work is evaluated on the task of\nconcept similarity and relatedness using UMLS and Yago to act as the backing\nontologies.\n\nStrengths:\n\nThe key question addressed by the paper is that phrases that are not lexically\nsimilar can be semantically close and, furthermore, not all phrases are\ncompositional in nature. To this end, the paper proposes a plausible model to\ntrain phrase embeddings. The trained embeddings are shown to be competitive or\nbetter at identifying similarity between concepts.\n\nThe software released with the paper could be useful for biomedical NLP\nresearchers.\n\n- Weaknesses:\n\nThe primary weakness of the paper is that the model is not too novel. It is\nessentially a tweak to skip-gram. \n\nFurthermore, the full model presented by the paper doesn't seem to be the best\none in the results (in Table 4). On the two Mayo datasets, the Choi baseline is\nsubstantially better. A similar trend seems to dominate Table 6 too. On the\nlarger UMNSRS data, the proposed model is at best competitive with previous\nsimpler models (Chiu).\n\n- General Discussion:\n\nThe paper says that it is uses known phrases as distant supervision to train\nembeddings. However, it is not clear what the \"supervision\" here is. If I\nunderstand the paper correctly, every occurrence of a phrase associated with a\nconcept provides the context to train word embeddings. But this is not\nsupervision in the traditional sense (say for identifying the concept in the\ntext or other such predictive tasks). So the terminology is a bit confusing.\n\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of\nthe paper.\n\nThe use of \\beta to control for compositionality of phrases by words is quite\nsurprising. Essentially, this is equivalent to saying that there is a single\nglobal constant that decides \"how compositional\" any phrase should be. The\nsurprising part here is that the actual values of \\beta chosen by cross\nvalidation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which\nbasically argues against compositionality. \n\nThe experimental setup for table 4 needs some explanation. The paper says that\nthe data labels similarity/relatedness of concepts (or entities). However, if\nthe concepts-phrases mapping is really many-to-many, then how are the\nphrase/word vectors used to compute the similarities? It seems that we can only\nuse the concept vectors.\n\nIn table 5, the approximate phr method (which approximate concepts with the\naverage of the phrases in them) is best performing. So it is not clear why we\nneed the concept ontology. Instead, we could have just started with a seed set\nof phrases to get the same results.\nThe authors presents a method to jointly embed words, phrases and concepts,\nbased on plain text corpora and a manually-constructed ontology, in which\nconcepts are represented by one or more phrases. They apply their method in the\nmedical domain using the UMLS ontology, and in the general domain using the\nYAGO ontology. To evaluate their approach, the authors compare it to simpler\nbaselines and prior work, mostly on intrinsic similarity and relatedness\nbenchmarks. They use existing benchmarks in the medical domain, and use\nmechanical turkers to generate a new general-domain concept similarity and\nrelatedness dataset, which they also intend to release. They report results\nthat are comparable to prior work.\n\nStrengths:\n\n- The proposed joint embedding model is straightforward and makes reasonable\nsense to me. Its main value in my mind is in reaching a (configurable) middle\nground between treating phrases as atomic units on one hand to considering\ntheir\ncompositionallity on the other. The same approach is applied to concepts being\n\u2018composed\u2019 of several representative phrases.\n\n-  The paper describes a decent volume of work, including model development,\nan additional contribution in the form of a new evaluation dataset, and several\nevaluations and analyses performed.\n\nWeaknesses:\n\n- The evaluation reported in this paper includes only intrinsic tasks, mainly\non similarity/relatedness datasets. As the authors note, such evaluations are\nknown to have very limited power in predicting the utility of embeddings in\nextrinsic tasks. Accordingly, it has become recently much more common to\ninclude at least one or two extrinsic tasks as part of the evaluation of\nembedding models.\n\n- The similarity/relatedness evaluation datasets used in the paper are\npresented as datasets recording human judgements of similarity between\nconcepts. However, if I understand correctly, the actual judgements were made\nbased on presenting phrases to the human annotators, and therefore they should\nbe considered as phrase similarity datasets, and analyzed as such.\n\n- The medical concept evaluation dataset, \u2018mini MayoSRS\u2019 is extremely small\n(29 pairs), and its larger superset \u2018MayoSRS\u2019 is only a little larger (101\npairs) and was reported to have a relatively low human annotator agreement. The\nother medical concept evaluation dataset, \u2018UMNSRS\u2019, is more reasonable in\nsize, but is based only on concepts that can be represented as single words,\nand were represented as such to the human annotators. This should be mentioned\nin the paper and makes the relevance of this dataset questionable with respect\nto representations of phrases and general concepts. \n\n- As the authors themselves note, they (quite extensively) fine tune their\nhyperparameters on the very same datasets for which they report their results\nand compare them with prior work. This makes all the reported results and\nanalyses questionable.\n\n- The authors suggest that their method is superb to prior work, as it achieved\ncomparable results while prior work required much more manual annotation. I\ndon't think this argument is very strong because the authors also use large\nmanually-constructed ontologies, and also because the manually annotated\ndataset used in prior work comes from existing clinical records that did not\nrequire dedicated annotations.\n\n- In general, I was missing more useful insights into what is going on behind\nthe reported numbers. The authors try to treat the relation between a phrase\nand its component words on one hand, and a concept and its alternative phrases\non the other, as similar types of a compositional relation. However, they\nare different in nature and in my mind each deserves a dedicated analysis. For\nexample, around line 588, I would expect an NLP analysis specific to the\nrelation between phrases and their component words. Perhaps the reason for the\nreported behavior is dominant phrase headwords, etc. Another aspect that was\nabsent but could strengthen the work, is an investigation of the effect of the\nhyperparameters that control the tradeoff between the atomic and compositional\nviews of phrases and concepts.\n\nGeneral Discussion:\n\nDue to the above mentioned weaknesses, I recommend to reject this submission. I\nencourage the authors to consider improving their evaluation datasets and\nmethodology before re-submitting this paper.\n\nMinor comments:\n\n- Line 069: contexts -> concepts\n\n- Line 202: how are phrase overlaps handled?\n\n- Line 220: I believe the dimensions should be |W| x d. Also, the terminology\n\u2018negative sampling matrix\u2019 is confusing as the model uses these embeddings\nto represent contexts in positive instances as well.\n\n- Line 250: regarding \u2018the observed phrase just completed\u2019, it not clear to\nme how words are trained in the joint model. The text may imply that only the\nlast words of a phrase are considered as target words, but that doesn\u2019t make\nsense. \n\n- Notation in Equation 1 is confusing (using c instead of o)\n\n- Line 361: Pedersen et al 2007 is missing in the reference section.\n\n- Line 388: I find it odd to use such a fine-grained similarity scale (1-100) \nfor human annotations.\n\n- Line 430: The newly introduced term \u2018strings\u2019 here is confusing. I\nsuggest to keep using \u2018phrases\u2019 instead.\n\n- Line 496: Which task exactly was used for the hyper-parameter tuning?\nThat\u2019s important. I couldn\u2019t find that even in the appendix.\n\n- Table 3: It\u2019s hard to see trends here, for instance PM+CL behaves rather\ndifferently than either PM or CL alone. It would be interesting to see\ndevelopment set trends with respect to these hyper-parameters.\n\n- Line 535: missing reference to Table 5.",
        "partial_review": "The paper describes an extension of word embedding methods to also provide\nrepresentations for phrases and concepts that correspond to words.  The method\nworks by fixing an identifier for groups of phrases, words and the concept that\nall denote this concept, replace the occurrences of the phrases and words by\nthis identifier in the training corpus, creating a \"tagged\" corpus, and then\nappending the tagged corpus to the original corpus for training.  The\nconcept/phrase/word sets are taken from an ontology.  Since the domain of\napplication is biomedical, the related corpora and ontologies are used.  The\nresearchers also report on the generation of a new test dataset for word\nsimilarity and relatedness for real-world entities, which is novel.\n\nIn general, the paper is nicely written.  The technique is pretty natural,\nthough not a very substantial contribution. The scope of the contribution is\nlimited, because of focused evaluation within the biomedical domain.\n\nMore discussion of the generated test resource could be useful.  The resource\ncould be the true interesting contribution of the paper.\n\nThere is one\nsmall technical problem, but that is probably just a matter of mathematical\nexpression than implementation.\n\nTechnical problem:\nEq. 8: The authors want to define the MAP calculation.                          This\nis a\ngood\nidea,\nthought I think that a natural cut-off could be defined, rather than ranking\nthe entire vocabulary.                          Equation 8 does not define a\nprobability;\nit is\nquite\neasy to show this, even if the size of the vocabulary is infinite.  So you need\nto change the explanation (take out talk of a probability).\n\nSmall corrections:\nline:\n556: most concepts has --> most concepts have\nSummary: This paper presents a model for embedding words, phrases and concepts\ninto vector spaces. To do so, it uses an ontology of concepts, each of which is\nmapped to phrases. These phrases are found in text corpora and treated as\natomic symbols. Using this, the paper uses what is essentially the skip-gram\nmethod to train embeddings for words, the now atomic phrases and also the\nconcepts associated with them. The proposed work is evaluated on the task of\nconcept similarity and relatedness using UMLS and Yago to act as the backing\nontologies.\n\nStrengths:\n\nThe key question addressed by the paper is that phrases that are not lexically\nsimilar can be semantically close and, furthermore, not all phrases are\ncompositional in nature. To this end, the paper proposes a plausible model to\ntrain phrase embeddings. The trained embeddings are shown to be competitive or\nbetter at identifying similarity between concepts.\n\nThe software released with the paper could be useful for biomedical NLP\nresearchers.\n\n- Weaknesses:\n\nThe primary weakness of the paper is that the model is not too novel. It is\nessentially a tweak to skip-gram. \n\nFurthermore, the full model presented by the paper doesn't seem to be the best\none in the results (in Table 4). On the two Mayo datasets, the Choi baseline is\nsubstantially better. A similar trend seems to dominate Table 6 too. On the\nlarger UMNSRS data, the proposed model is at best competitive with previous\nsimpler models (Chiu).\n\n- General Discussion:\n\nThe paper says that it is uses known phrases as distant supervision to train\nembeddings. However, it is not clear what the \"supervision\" here is. If I\nunderstand the paper correctly, every occurrence of a phrase associated with a\nconcept provides the context to train word embeddings. But this is not\nsupervision in the traditional sense (say for identifying the concept in the\ntext or other such predictive tasks). So the terminology is a bit confusing.\n\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of\nthe paper.\n\nThe use of \\beta to control for compositionality of phrases by words is quite\nsurprising. Essentially, this is equivalent to saying that there is a single\nglobal constant that decides \"how compositional\" any phrase should be. The\nsurprising part here is that the actual values of \\beta chosen by cross\nvalidation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which\nbasically argues against compositionality. \n\nThe experimental setup for table 4 needs some explanation. The paper says that\nthe data labels similarity/relatedness of concepts (or entities). However, if\nthe concepts-phrases mapping is really many-to-many, then how are the\nphrase/word vectors used to compute the similarities? It seems that we can only\nuse the concept vectors.\n\nIn table 5, the approximate phr method (which approximate concepts with the\naverage of the phrases in them) is best performing. So it is not clear why we\nneed the concept ontology. Instead, we could have just started with a seed set\nof phrases to get the same results.\nThe authors presents a method to jointly embed words, phrases and concepts,\nbased on plain text corpora and a manually-constructed ontology, in which\nconcepts are represented by one or more phrases. They apply their method in the\nmedical domain using the UMLS ontology, and in the general domain using the\nYAGO ontology. To evaluate their appr",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words \n\nHere is the abstract of the paper: Recent work on embedding ontology concepts has relied on either expensive manual annotation or automated concept tagging methods that ignore the textual contexts around concepts.  We propose a novel method for jointly learning concept, phrase, and word embeddings from an unlabeled text corpus, by using the representative phrases for ontology concepts as distant supervision.  We learn embeddings for medical concepts in the Unified Medical Language System and general-domain concepts in YAGO, using various corpora.  Our embeddings show performance competitive with existing methods on concept similarity and relatedness tasks, while requiring no human corpus annotation and demonstrating more than 3x coverage in the vocabulary size. \n\nHere is the partial review: The paper describes an extension of word embedding methods to also provide\nrepresentations for phrases and concepts that correspond to words.  The method\nworks by fixing an identifier for groups of phrases, words and the concept that\nall denote this concept, replace the occurrences of the phrases and words by\nthis identifier in the training corpus, creating a \"tagged\" corpus, and then\nappending the tagged corpus to the original corpus for training.  The\nconcept/phrase/word sets are taken from an ontology.  Since the domain of\napplication is biomedical, the related corpora and ontologies are used.  The\nresearchers also report on the generation of a new test dataset for word\nsimilarity and relatedness for real-world entities, which is novel.\n\nIn general, the paper is nicely written.  The technique is pretty natural,\nthough not a very substantial contribution. The scope of the contribution is\nlimited, because of focused evaluation within the biomedical domain.\n\nMore discussion of the generated test resource could be useful.  The resource\ncould be the true interesting contribution of the paper.\n\nThere is one\nsmall technical problem, but that is probably just a matter of mathematical\nexpression than implementation.\n\nTechnical problem:\nEq. 8: The authors want to define the MAP calculation.                          This\nis a\ngood\nidea,\nthought I think that a natural cut-off could be defined, rather than ranking\nthe entire vocabulary.                          Equation 8 does not define a\nprobability;\nit is\nquite\neasy to show this, even if the size of the vocabulary is infinite.  So you need\nto change the explanation (take out talk of a probability).\n\nSmall corrections:\nline:\n556: most concepts has --> most concepts have\nSummary: This paper presents a model for embedding words, phrases and concepts\ninto vector spaces. To do so, it uses an ontology of concepts, each of which is\nmapped to phrases. These phrases are found in text corpora and treated as\natomic symbols. Using this, the paper uses what is essentially the skip-gram\nmethod to train embeddings for words, the now atomic phrases and also the\nconcepts associated with them. The proposed work is evaluated on the task of\nconcept similarity and relatedness using UMLS and Yago to act as the backing\nontologies.\n\nStrengths:\n\nThe key question addressed by the paper is that phrases that are not lexically\nsimilar can be semantically close and, furthermore, not all phrases are\ncompositional in nature. To this end, the paper proposes a plausible model to\ntrain phrase embeddings. The trained embeddings are shown to be competitive or\nbetter at identifying similarity between concepts.\n\nThe software released with the paper could be useful for biomedical NLP\nresearchers.\n\n- Weaknesses:\n\nThe primary weakness of the paper is that the model is not too novel. It is\nessentially a tweak to skip-gram. \n\nFurthermore, the full model presented by the paper doesn't seem to be the best\none in the results (in Table 4). On the two Mayo datasets, the Choi baseline is\nsubstantially better. A similar trend seems to dominate Table 6 too. On the\nlarger UMNSRS data, the proposed model is at best competitive with previous\nsimpler models (Chiu).\n\n- General Discussion:\n\nThe paper says that it is uses known phrases as distant supervision to train\nembeddings. However, it is not clear what the \"supervision\" here is. If I\nunderstand the paper correctly, every occurrence of a phrase associated with a\nconcept provides the context to train word embeddings. But this is not\nsupervision in the traditional sense (say for identifying the concept in the\ntext or other such predictive tasks). So the terminology is a bit confusing.\n\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of\nthe paper.\n\nThe use of \\beta to control for compositionality of phrases by words is quite\nsurprising. Essentially, this is equivalent to saying that there is a single\nglobal constant that decides \"how compositional\" any phrase should be. The\nsurprising part here is that the actual values of \\beta chosen by cross\nvalidation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which\nbasically argues against compositionality. \n\nThe experimental setup for table 4 needs some explanation. The paper says that\nthe data labels similarity/relatedness of concepts (or entities). However, if\nthe concepts-phrases mapping is really many-to-many, then how are the\nphrase/word vectors used to compute the similarities? It seems that we can only\nuse the concept vectors.\n\nIn table 5, the approximate phr method (which approximate concepts with the\naverage of the phrases in them) is best performing. So it is not clear why we\nneed the concept ontology. Instead, we could have just started with a seed set\nof phrases to get the same results.\nThe authors presents a method to jointly embed words, phrases and concepts,\nbased on plain text corpora and a manually-constructed ontology, in which\nconcepts are represented by one or more phrases. They apply their method in the\nmedical domain using the UMLS ontology, and in the general domain using the\nYAGO ontology. To evaluate their appr \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper describes an extension of word embedding methods to also provide representations for phrases and concepts. The proposed method uses an ontology of concepts and representative phrases to guide the training of the embeddings. The paper is well written and the technique is natural, though not a substantial contribution. The evaluation is limited to the biomedical domain, which limits the scope of the contribution. The generated test resource could be discussed in more detail as it may be the most interesting contribution of the paper. There is a small technical problem with Equation 8, where the authors define a MAP calculation that does not define a probability. The paper also contains some minor corrections and weaknesses, such as the model not being too novel, and the proposed model not performing as well as baselines on some datasets. The terminology regarding the use of known phrases as distant supervision could be clarified. The notation introduced but not used and the use of \\beta to control compositionality of phrases by words are surprising. The experimental setup for Table 4 needs further explanation to understand how the phrase/word vectors were used to compute similarities. Table 5 shows that the approximate phr method performs best, raising questions about the need for the concept ontology. Overall, the paper presents a method for jointly embedding words, phrases, and concepts, but has some weaknesses and room for improvement.\"\n}",
        "human_end_index": 5150
    },
    {
        "id": 18,
        "source": "PeerRead/acl_2017/",
        "title": "Attention-over-Attention Neural Networks for Reading Comprehension",
        "human_review": "- Strengths:\n\n-- A well-motivated approach, with a clear description and solid results.\n\n- Weaknesses:\n\n-- Nothing substantial other than the comments below. \n\n- General Discussion:\n\nThe paper describes a new method called attention-over-attention for reading\ncomprehension. First layers of the network compute a vector for each query word\nand document word, resulting in a |Q|xK matrix for the query and a |D|xK for\nthe document. Since the answer is a document word, an attention mechanism is\nused for assigning weights to each word, depending on their interaction with\nquery words. In this work, the authors deepen a traditional attention mechanism\nby computing a weight for each query word through a separate attention and then\nusing that to weight the main attention over document words. Evaluation is\nproperly conducted on benchmark datasets, and various insights are presented\nthrough an analysis of the results as well as a comparison to prior work. I\nthink this is a solid piece of work on an important problem, and the method is\nwell-motivated and clearly described, so that researchers can easily reproduce\nresults and apply the same techniques to other similar tasks.\n\n- Other remarks:\n\n-- p4, Equation 12: I am assuming i is iterating over training set and p(w) is\nreferring to P(w|D,Q) in the previous equation? Please clarify to avoid\nconfusion.\n\n-- I am wondering whether you explored/discussed initializing word embeddings\nwith existing vectors such as Google News or Glove? Is there a reason to\nbelieve the general-purpose word semantics would not be useful in this task?\n\n-- p6 L589-592: It is not clear what the authors are referring to when they say\n'letting the model explicitly learn weights between individual attentions'? Is\nthis referring to their own architecture, more specifically the GRU output\nindirectly affecting how much attention will be applied to each query and\ndocument word? Clarifying that would be useful. Also, I think the improvement\non validation is not 4.1, rather 4.0 (72.2-68.2).\n\n-- p7 Table 5: why do you think the weight for local LM is relatively higher\nfor the CN task while the benefit of adding it is less? Since you included the\ntable, I think it'll be nice to provide some insights to the reader.\n\n-- I would have liked to see the software released as part of this submission.\n\n-- Typo p2 L162 right column: \"is not that effective than expected\" --> \"is not\nas effective as expected\"?\n\n-- Typo p7 L689 right column: \"appear much frequent\" --> \"appears more\nfrequently\"?\n\n-- Typo p8 L719-721 left column: \"the model is hard to\" --> \"it is hard for the\nmodel to\"? & \"hard to made\" --> \"hard to make\"?",
        "partial_review": "- Strengths:\n\n-- A well-motivated approach, with a clear description and solid results.\n\n- Weaknesses:\n\n-- Nothing substantial other than the comments below. \n\n- General Discussion:\n\nThe paper describes a new method called attention-over-attention for reading\ncomprehension. First layers of the network compute a vector for each query word\nand document word, resulting in a |Q|xK matrix for the query and a |D|xK for\nthe document. Since the answer is a document word, an attention mechanism is\nused for assigning weights to each word, depending on their interaction with\nquery words. In this work, the authors deepen a traditional attention mechanism\nby computing a weight for each query word through a separate attention and then\nusing that to weight the main attention over document words. Evaluation is\nproperly conducted on benchmark datasets, and various insights are presented\nthrough an analysis of the results as well as a comparison to prior work. I\nthink this is a solid piece of work on an important problem, and the method is\nwell-motivated and clearly described, so that researchers can easily reproduce\nresults and apply the same techniques to other similar tasks.\n\n- Other remarks:\n\n-- p4, Equation 12: I am assuming i is iterating over training set and p(w) is\nreferring to P(w|D,Q) in the previo",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Attention-over-Attention Neural Networks for Reading Comprehension \n\nHere is the abstract of the paper: Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces ``attended attention'' for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test. \n\nHere is the partial review: - Strengths:\n\n-- A well-motivated approach, with a clear description and solid results.\n\n- Weaknesses:\n\n-- Nothing substantial other than the comments below. \n\n- General Discussion:\n\nThe paper describes a new method called attention-over-attention for reading\ncomprehension. First layers of the network compute a vector for each query word\nand document word, resulting in a |Q|xK matrix for the query and a |D|xK for\nthe document. Since the answer is a document word, an attention mechanism is\nused for assigning weights to each word, depending on their interaction with\nquery words. In this work, the authors deepen a traditional attention mechanism\nby computing a weight for each query word through a separate attention and then\nusing that to weight the main attention over document words. Evaluation is\nproperly conducted on benchmark datasets, and various insights are presented\nthrough an analysis of the results as well as a comparison to prior work. I\nthink this is a solid piece of work on an important problem, and the method is\nwell-motivated and clearly described, so that researchers can easily reproduce\nresults and apply the same techniques to other similar tasks.\n\n- Other remarks:\n\n-- p4, Equation 12: I am assuming i is iterating over training set and p(w) is\nreferring to P(w|D,Q) in the previo \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents a well-motivated and well-described approach called attention-over-attention for reading comprehension. The method is based on a deepening of the traditional attention mechanism through the computation of a weight for each query word using a separate attention mechanism and then using that weight to weight the main attention over document words. The evaluation is conducted on benchmark datasets and the results are properly analyzed and compared to prior work. The paper is a solid piece of work on an important problem, and the method can easily be reproduced and applied to other similar tasks. One minor comment is that on page 4, Equation 12, it is not clear what i is iterating over and what p(w) refers to.\"\n}",
        "human_end_index": 1312
    },
    {
        "id": 180,
        "source": "PeerRead/acl_2017/",
        "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task",
        "human_review": "This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper is clear and well-written and the\nexperiments are good. Every hypothesis is tested and compared to each other.\n\nHowever, I do have some concerns about the paper:\n\n1. The authors took the liberty to change the font size and the line spacing of\nthe abstract, enabling them to have a longer abstract and to fit the content\ninto the 8 pages requirement.\n\n2. I don't think this paper fits the tagging, chunking, parsing area, as it is\nmore an information extraction problem.\n\n3. I have difficulties to see why some annotations such as sombody in Fig. 1\nare related to a product.\n\n4. The basic results are very basic indeed and - with all the tools available\nnowadays in NLP -, I am sure that it would have been possible to have more\nelaborate baselines without too much extra work.\n\n5. Domain adaptation experiments corroborate what we already know about\nuser-generated data where two forums on video games, e.g., may have different\ntypes of users (age, gender, etc.) leading to very different texts. So this\ndoes not give new highlights on this specific problem.",
        "partial_review": "This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper is clear and well-written and the\nexperiments are good. Every hypothesis is tested and compared to each other.\n\nHowever, I do have some concerns about the paper:\n\n1. The authors took the liberty to change the font size and the line spacing of\nthe abstract, enabling them to have a longer abstract and to fit the content\ninto the 8 pages requirement.\n\n2. I don't think this paper fits the tagging, chunking, parsing area, as it is\nmore an information extraction pr",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task \n\nHere is the abstract of the paper: One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate datasets consisting of data from four different forums. Each of these forums constitutes its own \"fine-grained domain\" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques.  We release a dataset of 93,924 posts from across 4 forums, with annotations for 1,938 posts. \n\nHere is the partial review: This paper presents a new dataset with annotations of products coming from\nonline cybercrime forums. The paper is clear and well-written and the\nexperiments are good. Every hypothesis is tested and compared to each other.\n\nHowever, I do have some concerns about the paper:\n\n1. The authors took the liberty to change the font size and the line spacing of\nthe abstract, enabling them to have a longer abstract and to fit the content\ninto the 8 pages requirement.\n\n2. I don't think this paper fits the tagging, chunking, parsing area, as it is\nmore an information extraction pr \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a new dataset with annotations of products coming from online cybercrime forums. The paper is clear and well-written and the experiments are good. Every hypothesis is tested and compared to each other. However, I do have some concerns about the paper:\\n\\n1. The authors took the liberty to change the font size and the line spacing of the abstract, enabling them to have a longer abstract and to fit the content into the 8 pages requirement.\\n\\n2. I don't think this paper fits the tagging, chunking, parsing area, as it is more an information extraction problem rather than traditional NLP tasks. The authors should clarify the motivation for choosing the specific domain and task they focused on.\\n\\nOverall, the paper is valuable for the NLP community as it introduces a new dataset and a fine-grained domain adaptation task. However, some improvements need to be made to address the concerns mentioned above.\"\n}",
        "human_end_index": 574
    },
    {
        "id": "182",
        "source": "PeerRead/acl_2017/",
        "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
        "human_review": "Dear Authors\n\nthanks for replying to our review comments, which clarifies some detail\nquestions. I appreciate your promise to publish the code, which will be very\nhelpful to other researchers. \n\nBased on this, i increased my overall score to 4. \n\nStrengths:\n- well-written\n- extensive experiments\n- good results\n\n- Weaknesses:\n- nothing ground-breaking, application of existing technologies\n- code not available\n- results are as could be expected\n\n- General Discussion:\n- why didn't you use established audio features such as MFCCs?\n\n- Minor Details:\n- L155 and other places: a LSTM -> an LSTM\n- L160, L216 and other Places: why are there hyphens (-) after the text?\n- L205: explanation of convolution is not clear\n- Table1 should appear earlier, on page 2 already cited\n- L263: is 3D-CNN a standard approach in video processing? alternatives?\n- L375, 378: the ^ should probably positioned above the y\n- L380: \"to check overfitting\" -> did you mean \"to avoid\"?\n- L403, 408..: put names in \" \" or write them italic, to make it easier to\nrecognize them\n- L420: a SVM -> an SVM\n- L448: Output ... are -> wrong numerus, either \"Outputs\", or use \"is\" \n- L489: superflous whitespace after \"layer\"\n- L516, 519: \"concatenation\" should not be in a new line\n- L567: why don't you know the exact number of persons?\n- L626: remove comma after Since\n- L651: doesnt -> does not \n- L777: insert \"hand, the\" after other\n- References: need some cleanup: L823 superflous whitespace, L831 Munich, L860\nwhat is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines,\nL956 indent Linguistics properly",
        "partial_review": "Dear Authors\n\nthanks for replying to our review comments, which clarifies some detail\nquestions. I appreciate your promise to publish the code, which will be very\nhelpful to other researchers. \n\nBased on this, i increased my overall score to 4. \n\nStrengths:\n- well-written\n- extensive experiments\n- good results\n\n- Weaknesses:\n- nothing ground-breaking, application of existing technologies\n- code not available\n- results are as could be expected\n\n- General Discussion:\n- why didn't you use established audio features such as MFCCs?\n\n- Minor Details:\n- L155 and other places: a LSTM -> an LSTM\n- L160, L216 and other Places: why are there hyphens (-) after the text?\n- L205: explanation of convolution is not clear\n- Table1 should appear earlier, on page 2 already cited\n- L263: is 3D-CNN a standard approach in video p",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Context-Dependent Sentiment Analysis in User-Generated Videos \n\nHere is the abstract of the paper: Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability. \n\nHere is the partial review: Dear Authors\n\nthanks for replying to our review comments, which clarifies some detail\nquestions. I appreciate your promise to publish the code, which will be very\nhelpful to other researchers. \n\nBased on this, i increased my overall score to 4. \n\nStrengths:\n- well-written\n- extensive experiments\n- good results\n\n- Weaknesses:\n- nothing ground-breaking, application of existing technologies\n- code not available\n- results are as could be expected\n\n- General Discussion:\n- why didn't you use established audio features such as MFCCs?\n\n- Minor Details:\n- L155 and other places: a LSTM -> an LSTM\n- L160, L216 and other Places: why are there hyphens (-) after the text?\n- L205: explanation of convolution is not clear\n- Table1 should appear earlier, on page 2 already cited\n- L263: is 3D-CNN a standard approach in video p \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Dear Authors,\\n\\nThank you for addressing our previous review comments and providing clarification on certain details. Your willingness to publish the code is greatly appreciated and will undoubtedly benefit other researchers in the field.\\n\\nTaking into account these responses and your efforts to improve the paper, I have increased my overall score to 4.\\n\\nStrengths:\\n- The paper is well-written and presents its research concisely.\\n- The experiments conducted are extensive, providing a solid basis for the claims made.\\n- The results obtained demonstrate a noticeable performance improvement over the current state of the art, indicating the efficacy of the proposed LSTM-based model.\\n\\nWeaknesses:\\n- The application of existing technologies without notable innovation could be considered a limitation of the paper. It would be valuable to explore further opportunities for advancement in this area.\\n- The code not being available poses a hindrance to replication and further research. Publishing the code as promised would greatly enhance the reproducibility and impact of the study.\\n- Although the results obtained are promising, they are not unexpected considering the methodological choices made. Exploring additional techniques or alternative approaches could add further robustness and novelty to the research.\\n\\nGeneral Discussion:\\n- One aspect that could be addressed is the omission of established audio features such as MFCCs. Providing an explanation for this choice would strengthen the research and allow for a more comprehensive evaluation of multimodal sentiment analysis in videos.\\n\\nMinor Details:\\n- In several instances (e.g., L155, L160, L216), a LSTM should be written as an LSTM.\\n- Clarification on the presence of hyphens (-) after text in certain locations (e.g., L160, L216) would be helpful to understand their significance in the context of the paper.\\n- The explanation of convolution in L205 could benefit from further elaboration to ensure clarity for readers.\\n- Table1 should be included earlier, preferably on page 2 since it is referenced prior to its appearance.\\n- Elaboration on whether 3D-CNN is considered a standard approach in video processing would enhance the discussion in L263.\\n\\nThank you for your attention to these details. I look forward to further improvements and the eventual publication of your work.\"\n}",
        "human_end_index": 819
    },
    {
        "id": 19,
        "source": "PeerRead/acl_2017/",
        "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution",
        "human_review": "- Strengths:\nThis paper introduced a novel method to improve zero pronoun resolution\nperformance.. The main contributions of this papers are: 1) proposed a simple\nmethod to automatically generate a large training set for zero pronoun\nresolution task; 2) adapted a two step learning process to transfer knowledge\nfrom large data set to the specific domain data; 3) differentiate unknown words\nusing different tags. In general, the paper is well written. Experiments are\nthoroughly designed. \n\n- Weaknesses:\n\nBut I have a few questions regarding finding the antecedent of a zero pronoun:\n1. How will an antecedent be identified, when the prediction is a pronoun? The\nauthors proposed a method by matching the head of noun phrases. It\u2019s not\nclear how to handle the situation when the head word is not a pronoun.\n2. What if the prediction is a noun that could not be found in the previous\ncontents?\n3. The system achieves great results on standard data set. I\u2019m curious is it\npossible to evaluate the system in two steps? The first step is to evaluate the\nperformance of the model prediction, i.e. to recover the dropped zero pronoun\ninto a word; the second step is to evaluate how well the systems works on\nfinding an antecedent.\n\nI\u2019m also curious why the authors decided to use attention-based neural\nnetwork. A few sentences to provide the reasons would be helpful for other\nresearchers.\n\nA minor comment:\nIn figure 2, should it be s1, s2 \u2026 instead of d1, d2 \u2026.? \n\n- General Discussion:\nOverall it is a great paper with innovative ideas and solid experiment setup.\n- Strengths:\n\nThe approach is novel and the results are very promising, beating\nstate-of-the-art.\n\n- Weaknesses:\n\n The linguistic motivation behind the paper is troublesome (see below). I feel\nthat the paper would benefit a lot from a more thoughtful interpretation of the\nresults.\n\n- General Discussion:\n\nThis paper presents an approach for Zero Pronoun Resolution in Chinese. The\nauthors advocate a novel procedure for generating large amount of relevant data\nfrom unlabeled documents. These data are then integrated smartly in an NN-based\narchitecture at a pre-training step. The results improve on state-of-the-art.\n\nI have mixed feelings about this study. On the one hand, the approach seems\nsound and shows promising results, beating very recent systems (e.g., Chen&Ng\n2016). On the other hand, the way the main contribution is framed is very\ndisturbing from the linguistic point of view. In particular, (zero) pronoun\nresolution is, linguistically speaking, a context modeling task, requiring\naccurate interpretation of discourse/salience, semantic and syntactic clues. It\nstarts from the assumption that (zero) pronouns are used in specific contexts,\nwhere full NPs shouldn't normally be possible. From this perspective,\ngenerating ZP data via replacing nominal with zeroes (\"blank\") doesn't sound\nvery convincing. And indeed, as the authors themselves show, the pre-training\nmodule alone doesn't achieve a reasonable performance. To sum it up, i don't\nthink that these generated pseudo-data can be called AZP data. It seems more\nlikely that they encode some form of selectional preferences (?). It would be\nnice if the authors could invest some effort in better understanding what\nexactly the pre-training module learns -- and then reformulate the\ncorresponding sections. \n\nThe paper can benefit from a proofreading by a native speaker of English -- for\nexample, the sentence on lines 064-068 is not grammatical.\n\n-- other points --\n\nlines 78-79: are there any restrictions on the nouns and especially pronouns?\nfor example, do you use this strategy for very common pronouns (as English\n\"it\")? if so, how do you guarantee that the two occurrences of the same token \nare indeed coreferent?\n\nline 91: the term antecedent is typically used to denote a preceding mention\ncoreferent with the anaphor, which is not what you mean here\n\nline 144: OntoNotes (typo)\n\nlines 487-489: it has been shown that evaluation on gold-annotated data does\nnot provide reliable estimation of performance. and, indeed, all the recent\nstudies of coreference evaluate on system mentions. for example, the studies of\nChen&Ng you are citing, provide different types of evaluation, including those\non system mentions. please consider rerunning your experiments to get a more\nrealistic evaluation setup\n\nline 506: i don't understand what the dagger over the system's name means. is\nyour improvement statistically significant on all the domains? including bn and\ntc??\n\nline 565: learn (typo)\n\nsection 3.3: in this section you use the abbreviation AZP instead of ZP without\nintroducing it, please unify the terminology\n\nreferences -- please double-check for capitalization",
        "partial_review": "- Strengths:\nThis paper introduced a novel method to improve zero pronoun resolution\nperformance.. The main contributions of this papers are: 1) proposed a simple\nmethod to automatically generate a large training set for zero pronoun\nresolution task; 2) adapted a two step learning process to transfer knowledge\nfrom large data set to the specific domain data; 3) differentiate unknown words\nusing different tags. In general, the paper is well written. Experiments are\nthoroughly designed. \n\n- Weaknesses:\n\nBut I have a few questions regarding finding the antecedent of a zero pronoun:\n1. How will an antecedent be identified, when the prediction is a pronoun? The\nauthors proposed a method by matching the head of noun phrases. It\u2019s not\nclear how to handle the situation when the head word is not a pronoun.\n2. What if the prediction is a noun that could not be found in the previous\ncontents?\n3. The system achieves great results on standard data set. I\u2019m curious is it\npossible to evaluate the system in two steps? The first step is to evaluate the\nperformance of the model prediction, i.e. to recover the dropped zero pronoun\ninto a word; the second step is to evaluate how well the systems works on\nfinding an antecedent.\n\nI\u2019m also curious why the authors decided to use attention-based neural\nnetwork. A few sentences to provide the reasons would be helpful for other\nresearchers.\n\nA minor comment:\nIn figure 2, should it be s1, s2 \u2026 instead of d1, d2 \u2026.? \n\n- General Discussion:\nOverall it is a great paper with innovative ideas and solid experiment setup.\n- Strengths:\n\nThe approach is novel and the results are very promising, beating\nstate-of-the-art.\n\n- Weaknesses:\n\n The linguistic motivation behind the paper is troublesome (see below). I feel\nthat the paper would benefit a lot from a more thoughtful interpretation of the\nresults.\n\n- General Discussion:\n\nThis paper presents an approach for Zero Pronoun Resolution in Chinese. The\nauthors advocate a novel procedure for generating large amount of relevant data\nfrom unlabeled documents. These data are then integrated smartly in an NN-based\narchitecture at a pre-training step. The results improve on state-of-the-art.\n\nI have mixed feelings about this study. On the one hand, the approach seems\nsound and shows promising results, beating very ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution \n\nHere is the abstract of the paper: Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task.  Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1\\% F-score on OntoNotes 5.0 data. \n\nHere is the partial review: - Strengths:\nThis paper introduced a novel method to improve zero pronoun resolution\nperformance.. The main contributions of this papers are: 1) proposed a simple\nmethod to automatically generate a large training set for zero pronoun\nresolution task; 2) adapted a two step learning process to transfer knowledge\nfrom large data set to the specific domain data; 3) differentiate unknown words\nusing different tags. In general, the paper is well written. Experiments are\nthoroughly designed. \n\n- Weaknesses:\n\nBut I have a few questions regarding finding the antecedent of a zero pronoun:\n1. How will an antecedent be identified, when the prediction is a pronoun? The\nauthors proposed a method by matching the head of noun phrases. It\u2019s not\nclear how to handle the situation when the head word is not a pronoun.\n2. What if the prediction is a noun that could not be found in the previous\ncontents?\n3. The system achieves great results on standard data set. I\u2019m curious is it\npossible to evaluate the system in two steps? The first step is to evaluate the\nperformance of the model prediction, i.e. to recover the dropped zero pronoun\ninto a word; the second step is to evaluate how well the systems works on\nfinding an antecedent.\n\nI\u2019m also curious why the authors decided to use attention-based neural\nnetwork. A few sentences to provide the reasons would be helpful for other\nresearchers.\n\nA minor comment:\nIn figure 2, should it be s1, s2 \u2026 instead of d1, d2 \u2026.? \n\n- General Discussion:\nOverall it is a great paper with innovative ideas and solid experiment setup.\n- Strengths:\n\nThe approach is novel and the results are very promising, beating\nstate-of-the-art.\n\n- Weaknesses:\n\n The linguistic motivation behind the paper is troublesome (see below). I feel\nthat the paper would benefit a lot from a more thoughtful interpretation of the\nresults.\n\n- General Discussion:\n\nThis paper presents an approach for Zero Pronoun Resolution in Chinese. The\nauthors advocate a novel procedure for generating large amount of relevant data\nfrom unlabeled documents. These data are then integrated smartly in an NN-based\narchitecture at a pre-training step. The results improve on state-of-the-art.\n\nI have mixed feelings about this study. On the one hand, the approach seems\nsound and shows promising results, beating very  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents a novel approach for zero pronoun resolution and proposes a method to automatically generate large-scale pseudo training data. The experiments are well-designed and the results show significant improvement over the state-of-the-art systems. However, there are some concerns regarding the identification of antecedents and handling unknown words. Additionally, the authors did not provide a clear motivation for using an attention-based neural network. Overall, the paper is well-written and the approach is innovative, but the interpretation of the results and the clarification of certain aspects could be improved.\"\n}",
        "human_end_index": 2309
    },
    {
        "id": 193,
        "source": "PeerRead/acl_2017/",
        "title": "A Transition-Based Directed Acyclic Graph Parser for UCCA",
        "human_review": "This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a quite successful transition-based parser for inference into\nthat representation. I liked this paper a lot. I believe there is a lot of\nvalue simply in the introduction of UCCA (not new, but I believe relatively new\nto this community), which has the potential to spark new thinking about\nsemantic representations of text. I also think the model was well thought out.\nWhile the model itself was fairly derivative of existing transition-based\nschemes, the extensions the authors introduced to make the model applicable in\nthis domain were reasonable and well-explained, at what I believe to be an\nappropriate level of detail.\n\nThe empirical evaluation was pretty convincing -- the results were good, as\ncompared to several credible baselines, and the authors demonstrated this\nperformance in multiple domains. My biggest complaint about this paper is the\nlack of multilingual evaluation, especially given\nthat the formalism being experimented with is exactly one that is supposed to\nbe fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in\nfact, I think the \"20k leagues\" corpus used in this paper is one such), so it\nwould be good to see results in a language other than English.\n\nOne minor point: in section 6, the authors refer to their model as\n\"grammarless\", which strikes me as not quite correct. It's true that the UCCA\nrepresentation isn't derived from linguistic notions of syntax, but it still\ndefines a way to construct a compositional abstract symbolic representation of\ntext, which to me, is precisely a grammar. (This is clearly a quibble, and I\ndon't know why it irked me enough that I feel compelled to address it, but it\ndid.)\n\nEdited to add: Thanks to the authors for their response.\nThis paper presents the first parser to UCCA, a recently proposed meaning\nrepresentation. The parser is transition based, and uses a new transition set\ndesigned to recover challenging discontinuous structures with reentrancies.\nExperiments demonstrate that the parser works well, and that it is not easy to\nbuild these representation on top of existing parsing approaches. \n\nThis is a well written and interesting paper on an important problem. The\ntransition system is well motivated and seems to work well for the problem. The\nauthors also did a very thorough experimental evaluation, including both\nvarying the classifier for the base parser (neural, linear model, etc.) and\nalso comparing to the best output you could get from other existing, but less\nexpressive, parsing formulations. This paper sets a strong standard to UCCA\nparsing, and should also be interesting to researchers working with other\nexpressive meaning representations or complex transition systems. \n\nMy only open question is the extent to which this new parser subsumes all of\nthe other transition based parsers for AMR, SDP, etc. Could the UCCA transition\nscheme be used in these cases (which heuristic alignments if necessary), and\nwould it just learn to not use the extra transitions for non-terminals, etc.\nWould it reduce to an existing algorithm, or perhaps work better? Answering\nthis question isn\u2019t crucial, the paper is very strong as is, but it would add\nto the overall understanding and could point to interesting areas for future\nwork.\n\n----\n\nI read the author response and agree with everything they say.\n(the authors response answer most of the clarification questions of my review)\n\n=========================\n- Summary:\n=========================\n\nThe paper describes a transition-based system for UCCA graphs, featuring\nnon-terminal nodes,  reentrancy and discontinuities. The transition set is a\nmix of already proposed transitions\n(The key aspects are the swap transition to cope with discontinuities, and\ntransitions not popping the stack to allow multiple parents for a node.).\nThe best performance is obtained using as transition classifier a MLP with\nfeatures based on bidirectional LSTMs.\n\nThe authors compare the obtained performance with other state-of-the art\nparsers, using conversion schemes (to bilexical graphs, and to tree\napproximations): the parsers are trained on converted data, used to predict\ngraphs (or trees), and the predicted structures are converted ack to UCCA and\nconfronted with gold UCCA representations.\n\n=========================\n- Strengths:\n=========================\n\nThe paper presents quite solid work, with state-of-the art transition-based\ntechniques, and machine learning for parsing techniques.\n\nIt is very well written, formal and experimental aspects are described in a\nvery precise way, and the authors demonstrate a very good knowledge of the\nrelated work, both for parsing techniques and for shallow semantic\nrepresentations.\n\n=========================\n- Weaknesses:\n=========================\n\nMaybe the weakness of the paper is that the originality lies mainly in the\ntargeted representations (UCCA), not really in the proposed parser.\n\n=========================\n- More detailed comments and clarification questions:\n=========================\n\nIntroduction\n\nLines 46-49: Note that \"discontinuous nodes\" could be linked to\nnon-projectivity in the dependency framework. So maybe rather state that the\ndifference is with phrase-structure syntax not dependency syntax.\n\nSection 2:\n\nIn the UCCA scheme's description, the alternative \"a node (or unit) corresponds\nto a terminal or to several sub-units\" is not very clear. Do you mean something\nelse than a node is either a terminal or a non terminal? Can't a non terminal\nnode have one child only (and thus neither be a terminal nor have several\nsub-units) ?\n\nNote that \"movement, action or state\" is not totally appropriate, since there\nare processes which are neither movements nor actions (e.g. agentless\ntransformations).\n(the UCCA guidelines use these three terms, but later state the state/process\ndichotomy, with processes being an \"action, movement or some other relation\nthat evolves in time\").\n\nlines 178-181: Note that the contrast between \"John and Mary's trip\" and \"John\nand Mary's children\" is not very felicitous. The relational noun \"children\"\nevokes an underlying relation between two participants (the children and\nJohn+Mary), that has to be accounted for in UCCA too.\n\nSection 4:\n\nConcerning the conversion procedures:\n- While it is very complete to provide the precise description of the\nconversion procedure in the supplementary material, it would ease reading to\ndescribe it informally in the paper (as a variant of the\nconstituent-to-dependency conversion procedure \u00e0 la Manning, 95). Also, it\nwould be interesting to motivate the priority order used to define the head of\nan edge.\n\n- How l(u) is chosen in case of several children with same label should be made\nexplicit (leftmost ?).\n\n- In the converted graphs in figure 4, some edges seem inverted (e.g. the\ndirection between \"John\" and \"moved\" and between \"John\" and \"gave\" should be\nthe same).\n\n- Further, I am confused as to why the upper bound for remote edges in\nbilexical approximations is so low. The current description of the conversions\ndo not allow to get an quick idea of which kind of remote edges cannot be\nhandled.\n\nConcerning the comparison to other parsers:\nIt does not seem completely fair to tune the proposed parser, but to use\ndefault settings for the other parsers.\n\nSection 5\n\nLine 595: please better motivate the claim \"using better input encoding\"\n\nSection 6\n\nI am not convinced by the alledged superiority of representations with\nnon-terminal nodes. Although it can be considered more elegant not to choose a\nhead for some constructions, it can be noted that formally co-head labels can\nbe used in bilexical dependencies to recover the same information.",
        "partial_review": "This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a quite successful transition-based parser for inference into\nthat representation. I liked this paper a lot. I believe there is a lot of\nvalue simply in the introduction of UCCA (not new, but I believe relatively new\nto this community), which has the potential to spark new thinking about\nsemantic representations of text. I also think the model was well thought out.\nWhile the model itself was fairly derivative of existing transition-based\nschemes, the extensions the authors introduced to make the model applicable in\nthis domain were reasonable and well-explained, at what I believe to be an\nappropriate level of detail.\n\nThe empirical evaluation was pretty convincing -- the results were good, as\ncompared to several credible baselines, and the authors demonstrated this\nperformance in multiple domains. My biggest complaint about this paper is the\nlack of multilingual evaluation, especially given\nthat the formalism being experimented with is exactly one that is supposed to\nbe fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in\nfact, I think the \"20k leagues\" corpus used in this paper is one such), so it\nwould be good to see results in a language other than English.\n\nOne minor point: in section 6, the authors refer to their model as\n\"grammarless\", which strikes me as not quite correct. It's true that the UCCA\nrepresentation isn't derived from linguistic notions of syntax, but it still\ndefines a way to construct a compositional abstract symbolic representation of\ntext, which to me, is precisely a grammar. (This is clearly a quibble, and I\ndon't know why it irked me enough that I feel compelled to address it, but it\ndid.)\n\nEdited to add: Thanks to the authors for their response.\nThis paper presents the first parser to UCCA, a recently proposed meaning\nrepresentation. The parser is transition based, and uses a new transition set\ndesigned to recover challenging discontinuous structures with reentrancies.\nExperiments demonstrate that the parser works well, and that it is not easy to\nbuild these representation on top of existing parsing approaches. \n\nThis is a well written and interesting paper on an important problem. The\ntransition system is well motivated and seems to work well for the problem. The\nauthors also did a very thorough experimental evaluation, including both\nvarying the classifier for the base parser (neural, linear model, etc.) and\nalso comparing to the best output you could get from other existing, but less\nexpressive, parsing formulations. This paper sets a strong standard to UCCA\nparsing, and should also be interesting to researchers working with other\nexpressive meaning representations or complex transition systems. \n\nMy only open question is the extent to which this new parser subsumes all of\nthe other transition based parsers for AMR, SDP, etc. Could the UCCA transition\nscheme be used in these cases (which heuristic alignments if necessary), and\nwould it just learn to not use the extra transitions for non-terminals, etc.\nWould it reduce to an existing algorithm, or perhaps work better? Answering\nthis question isn\u2019t crucial, the paper is very strong as is, but it would add\nto the overall understanding and could point to interesting areas for future\nwork.\n\n----\n\nI read the author response and agree with everything they say.\n(the authors response answer most of the clarification questions of my review)\n\n=========================\n- Summary:\n=========================\n\nThe paper describes a transition-based system for UCCA graphs, featuring\nnon-terminal nodes,  reentrancy and discontinuities. The transition set is a\nmix of already proposed transitions\n(The key aspects are the swap transition to cope with discontinuities, and\ntransitions not popping the stack to al",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A Transition-Based Directed Acyclic Graph Parser for UCCA \n\nHere is the abstract of the paper: We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures. \n\nHere is the partial review: This paper introduces UCCA as a target representation for semantic parsing and\nalso describes a quite successful transition-based parser for inference into\nthat representation. I liked this paper a lot. I believe there is a lot of\nvalue simply in the introduction of UCCA (not new, but I believe relatively new\nto this community), which has the potential to spark new thinking about\nsemantic representations of text. I also think the model was well thought out.\nWhile the model itself was fairly derivative of existing transition-based\nschemes, the extensions the authors introduced to make the model applicable in\nthis domain were reasonable and well-explained, at what I believe to be an\nappropriate level of detail.\n\nThe empirical evaluation was pretty convincing -- the results were good, as\ncompared to several credible baselines, and the authors demonstrated this\nperformance in multiple domains. My biggest complaint about this paper is the\nlack of multilingual evaluation, especially given\nthat the formalism being experimented with is exactly one that is supposed to\nbe fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in\nfact, I think the \"20k leagues\" corpus used in this paper is one such), so it\nwould be good to see results in a language other than English.\n\nOne minor point: in section 6, the authors refer to their model as\n\"grammarless\", which strikes me as not quite correct. It's true that the UCCA\nrepresentation isn't derived from linguistic notions of syntax, but it still\ndefines a way to construct a compositional abstract symbolic representation of\ntext, which to me, is precisely a grammar. (This is clearly a quibble, and I\ndon't know why it irked me enough that I feel compelled to address it, but it\ndid.)\n\nEdited to add: Thanks to the authors for their response.\nThis paper presents the first parser to UCCA, a recently proposed meaning\nrepresentation. The parser is transition based, and uses a new transition set\ndesigned to recover challenging discontinuous structures with reentrancies.\nExperiments demonstrate that the parser works well, and that it is not easy to\nbuild these representation on top of existing parsing approaches. \n\nThis is a well written and interesting paper on an important problem. The\ntransition system is well motivated and seems to work well for the problem. The\nauthors also did a very thorough experimental evaluation, including both\nvarying the classifier for the base parser (neural, linear model, etc.) and\nalso comparing to the best output you could get from other existing, but less\nexpressive, parsing formulations. This paper sets a strong standard to UCCA\nparsing, and should also be interesting to researchers working with other\nexpressive meaning representations or complex transition systems. \n\nMy only open question is the extent to which this new parser subsumes all of\nthe other transition based parsers for AMR, SDP, etc. Could the UCCA transition\nscheme be used in these cases (which heuristic alignments if necessary), and\nwould it just learn to not use the extra transitions for non-terminals, etc.\nWould it reduce to an existing algorithm, or perhaps work better? Answering\nthis question isn\u2019t crucial, the paper is very strong as is, but it would add\nto the overall understanding and could point to interesting areas for future\nwork.\n\n----\n\nI read the author response and agree with everything they say.\n(the authors response answer most of the clarification questions of my review)\n\n=========================\n- Summary:\n=========================\n\nThe paper describes a transition-based system for UCCA graphs, featuring\nnon-terminal nodes,  reentrancy and discontinuities. The transition set is a\nmix of already proposed transitions\n(The key aspects are the swap transition to cope with discontinuities, and\ntransitions not popping the stack to al \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "- Detailed Review:\n\nThis paper introduces a novel parser for UCCA graphs, a framework for semantic representation that poses challenges for existing parsing techniques due to its reentrancy, discontinuous structures, and complex semantic units. The paper provides a clear and well-motivated introduction to UCCA as a target representation for semantic parsing, highlighting its potential to spark new thinking about semantic representations of text. The model itself is well thought out, with reasonable extensions to existing transition-based schemes to make it applicable to UCCA parsing.\n\nThe empirical evaluation is convincing, with good results compared to credible baselines and demonstrated performance in multiple domains. However, the lack of multilingual evaluation is a notable limitation, especially considering the universality of the UCCA formalism. It would be beneficial to see results in languages other than English to assess the parser's performance across different linguistic contexts.\n\nOne minor quibble is the authors' characterization of their model as \"grammarless\" in Section 6. While the UCCA representation may not be derived from linguistic syntax, it does define a way to construct a compositional abstract symbolic representation of text, which can be considered a grammar. Overall, this is a minor point and does not significantly impact the quality of the paper.\n\nThe paper presents a strong contribution to the field of UCCA parsing, with a well-motivated transition system that effectively recovers challenging discontinuous structures with reentrancies. The thorough experimental evaluation, including comparisons with other parsing formulations and classifiers, establishes a strong standard for UCCA parsing. The paper is not only relevant to researchers working with UCCA but also valuable to those working with other expressive meaning representations or complex transition systems.\n\nOne potential question that arises is whether the UCCA transition scheme could be applied to other transition-based parsers for AMR or SDP. It would be interesting to investigate if the UCCA transition set, with heuristic alignments if necessary, could be used in these cases and if it would lead to improved performance or reduction to existing algorithms. While this question is not crucial to the paper's main contributions, it could provide insights and open avenues for future research.\n\nIn conclusion, this paper is well written, addresses an important problem, and presents a strong contribution to UCCA parsing. The authors' response adequately addresses the points raised in the review and provides further clarification.",
        "human_end_index": 3851
    },
    {
        "id": 201,
        "source": "PeerRead/acl_2017/",
        "title": "Investigating Different Context Types and Representations for Learning Word Embeddings",
        "human_review": "- Strengths:\n\nThis paper presents a 2 x 2 x 3 x 10 array of accuracy results based on\nsystematically changing the parameters of embeddings models:\n\n(context type, position sensitive, embedding model, task), accuracy\n\n- context type \u2208 {Linear, Syntactic}\n- position sensitive \u2208 {True, False}\n- embedding model \u2208 {Skip Gram, BOW, GLOVE}\n- task \u2208 {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific.\ntasks}\n\nThe aim of these experiments was to investigate the variation in\nperformance as these parameters are changed. The goal of the study itself\nis interesting for the ACL community and similar papers have appeared\nbefore as workshop papers and have been well cited, such as Nayak et al.'s\npaper mentioned below.\n\n- Weaknesses:\nSince this paper essentially presents the effect of systematically changing the\n\ncontext types and position sensitivity, I will focus on the execution of the\ninvestigation and the analysis of the results, which I am afraid is not \nsatisfactory.\n\nA) The lack of hyper-parameter tuning is worrisome. E.g.\n   - 395 Unless otherwise notes, the number of word embedding dimension is set\nto 500.\n   - 232 It still enlarges the context vocabulary about 5 times in practice.\n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n\n  This is worrisome because lack of hyperparameter tuning makes it difficult to\nmake statements like method A is better than method B. E.g. bound methods may\nperform better with a lower dimensionality than unbound models, since their\neffective context vocabulary size is larger.\n\nB) The paper sometimes presents strange explanations for its results. E.g.\n   - 115 \"Experimental results suggest that although it's hard to find any \nuniversal insight, the characteristics of different contexts on different\nmodels are concluded according to specific tasks.\"\n\n   What does this sentence even mean? \n\n   - 580 Sequence labeling tasks tend to classify words with the same syntax \nto the same category. The ignorance of syntax for word embeddings which  are\nlearned by bound representation becomes beneficial. \n\n   These two sentences are contradictory, if a sequence labeling task\n   classified words with \"same syntax\" to same category then syntx becomes\n   a ver valuable feature. Bound representation's ignorance of syntax\n   should cause a drop in performance just like other tasks which does not\n   happen.\n\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a\n   systematic study of the word embeddings, and similarly the paper \n   \"Evaluating Word Embeddings Using a Representative Suite of Practical\n   Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at \n   ACL 2016. should have been cited. I understand that the focus of Nayak\n   et al's paper is not exactly the same as this paper, however they\n   provide recommendations about hyperparameter tuning and experiment\n   design and even provide a web interface for automatically running\n   tagging experiments using neural networks instead of the \"simple linear\n   classifiers\" used in the current paper.\n\nD) The paper uses a neural BOW words classifier for the text classification\ntasks\n   but a simple linear classifier for the sequence labeling tasks. What is\n   the justification for this choice of classifiers? Why not use a simple\n   neural classifier for the tagging tasks as well? I raise this point,\n   since the tagging task seems to be the only task where bound\n   representations are consistently beating the unbound representations,\n   which makes this task the odd one out. \n\n- General Discussion:\nFinally, I will make one speculative suggestion to the authors regarding\nthe analysis of the data. As I said earlier, this paper's main contribution is\nan\nanalysis of the following table.\n(context type, position sensitive, embedding model, task, accuracy)\nSo essentially there are 120 accuracy values that we want to explain in\nterms of the aspects of the model. It may be beneficial to perform\nfactor analysis or some other pattern mining technique on this 120 sample data.\n- Strengths: \nEvaluating bag of words and \"bound\" contexts from either dependencies or\nsentence ordering is important, and will be a useful reference to the\ncommunity. The experiments were relatively thorough (though some choices could\nuse further justification), and the authors used downstream tasks instead of\njust intrinsic evaluations.\n\n- Weaknesses: \nThe authors change the objective function of GBOW from p(c|\\sum w_i) to\np(w|\\sum c_i). This is somewhat justified as dependency-based context with a\nbound representation only has one word available for predicting the context,\nbut it's unclear exactly why that is the case and deserves more discussion.\nPresumably the\nnon-dependency context with a bound representation would also suffer from this\ndrawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors\ndon't compare any results against the original objective, which is a definite\nweakness. In addition, the authors change GSG to match GBOW, again without\ncomparing to the original objective. Adding results from word vectors trained\nusing the original GBOW and GSG objective functions would justify these changes\n(assuming the results don't show large changes).\nThe hyperparameter settings should be discussed further. This played a large\nrole in Levy et al. (2015), so you should consider trying different\nhyperparameter values. These depend pretty heavily on the task, so simply\ntaking good values from another task may not work well.\n\nIn addition, the authors are unclear on exactly what model is trained in\nsection 3.4. They say only that it is a \"simple linear classifier\". In section\n3.5, they use logistic regression with the average of the word vectors as\ninput, but call it  a Neural Bag-of-Words model. Technically previous work also\nused this name, but I find it misleading, since it's just logistic regression\n(and hence a linear model, which is not something I would call \"Neural\"). It is\nimportant to know if the model trained in section 3.4 is the same as the model\ntrained in 3.5, so we know if the different conclusions are the result of the\ntask or the model changing. \n\n- General Discussion: This paper evaluates context taken from dependency parses\nvs context taken from word position in a given sentence, and bag-of-words vs\ntokens with relative position indicators. This paper is useful to the\ncommunity, as they show when and where researchers should use word vectors\ntrained using these different decisions. \n\n- Emphasis to improve:\nThe main takeaway from this paper that future researchers will use is given at\nthe end of 3.4 and 3.5, but really should be summarized at the start of the\npaper. Specifically, the authors should put in the abstract that for POS,\nchunking, and NER, bound representations outperform bag-of-words\nrepresentations, and that dependency contexts work better than linear contexts\nin most cases. In addition, for a simple text classification model, bound\nrepresentations perform worse than bag-of-words representations, and there\nseemed to be no major difference between the different models or context types.\n\n- Small points of improvement: \nShould call \"unbounded\" context \"bag of words\". This may lead to some confusion\nas one of the techniques you use is Generalized Bag-Of-Words, but this can be\nclarified easily.\n043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\". \n069: citations should have a comma instead of semicolon separating them.\n074: \"DEPS\" should be capitalized consistently throughout the paper (usually it\nappears as \"Deps\"). Also should be introduced as something like dependency\nparse tree context (Deps).\n085: typo: \"How different contexts affect model's performances...\" Should have\nthe word \"do\".\n- Strengths:\n\nThis paper systematically investigated how context types (linear vs\ndependency-based) and representations (bound word vs unbound word) affect word\nembedding learning. They experimented with three models (Generalized\nBag-Of-Words, Generalized Skip-Gram and Glove) in multiple different tasks\n(word similarity, word analogy, sequence labeling and text classification).\nOverall, \n1)            It is well-written and structured.\n2)            The experiments are very thoroughly evaluated. The analysis could\nhelp\nresearchers to choose different word embeddings or might even motivate new\nmodels. \n3)            The attached software can also benefit the community. \n\n- Weaknesses:\n\n The novelty is limited. \n\n- General Discussion:\n\nFor the dependency-based context types, how does the dependency parsing affect\nthe overall performance? Is it fair to compare those two different context\ntypes since the dependency-based one has to rely on the predicted dependency\nparsing results (in this case CoreNLP) while the linear one does not?",
        "partial_review": "- Strengths:\n\nThis paper presents a 2 x 2 x 3 x 10 array of accuracy results based on\nsystematically changing the parameters of embeddings models:\n\n(context type, position sensitive, embedding model, task), accuracy\n\n- context type \u2208 {Linear, Syntactic}\n- position sensitive \u2208 {True, False}\n- embedding model \u2208 {Skip Gram, BOW, GLOVE}\n- task \u2208 {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific.\ntasks}\n\nThe aim of these experiments was to investigate the variation in\nperformance as these parameters are changed. The goal of the study itself\nis interesting for the ACL community and similar papers have appeared\nbefore as workshop papers and have been well cited, such as Nayak et al.'s\npaper mentioned below.\n\n- Weaknesses:\nSince this paper essentially presents the effect of systematically changing the\n\ncontext types and position sensitivity, I will focus on the execution of the\ninvestigation and the analysis of the results, which I am afraid is not \nsatisfactory.\n\nA) The lack of hyper-parameter tuning is worrisome. E.g.\n   - 395 Unless otherwise notes, the number of word embedding dimension is set\nto 500.\n   - 232 It still enlarges the context vocabulary about 5 times in practice.\n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n\n  This is worrisome because lack of hyperparameter tuning makes it difficult to\nmake statements like method A is better than method B. E.g. bound methods may\nperform better with a lower dimensionality than unbound models, since their\neffective context vocabulary size is larger.\n\nB) The paper sometimes presents strange explanations for its results. E.g.\n   - 115 \"Experimental results suggest that although it's hard to find any \nuniversal insight, the characteristics of different contexts on different\nmodels are concluded according to specific tasks.\"\n\n   What does this sentence even mean? \n\n   - 580 Sequence labeling tasks tend to classify words with the same syntax \nto the same category. The ignorance of syntax for word embeddings which  are\nlearned by bound representation becomes beneficial. \n\n   These two sentences are contradictory, if a sequence labeling task\n   classified words with \"same syntax\" to same category then syntx becomes\n   a ver valuable feature. Bound representation's ignorance of syntax\n   should cause a drop in performance just like other tasks which does not\n   happen.\n\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a\n   systematic study of the word embeddings, and similarly the paper \n   \"Evaluating Word Embeddings Using a Representative Suite of Practical\n   Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at \n   ACL 2016. should have been cited. I understand that the focus of Nayak\n   et al's paper is not exactly the same as this paper, however they\n   provide recommendations about hyperparameter tuning and experiment\n   design and even provide a web interface for automatically running\n   tagging experiments using neural networks instead of the \"simple linear\n   classifiers\" used in the current paper.\n\nD) The paper uses a neural BOW words classifier for the text classification\ntasks\n   but a simple linear classifier for the sequence labeling tasks. What is\n   the justification for this choice of classifiers? Why not use a simple\n   neural classifier for the tagging tasks as well? I raise this point,\n   since the tagging task seems to be the only task where bound\n   representations are consistently beating the unbound representations,\n   which makes this task the odd one out. \n\n- General Discussion:\nFinally, I will make one speculative suggestion to the authors regarding\nthe analysis of the data. As I said earlier, this paper's main contribution is\nan\nanalysis of the following table.\n(context type, position sensitive, embedding model, task, accuracy)\nSo essentially there are 120 accuracy values that we want to explain in\nterms of the aspects of the model. It may be beneficial to perform\nfactor analysis or some other pattern mining technique on this 120 sample data.\n- Strengths: \nEvaluating bag of words and \"bound\" contexts from either dependencies or\nsentence ordering is important, and will be a useful reference to the\ncommunity. The experiments were relatively thorough (though some choices could\nuse further justification), and the authors used downstream tasks instead of\njust intrinsic evaluations.\n\n- Weaknesses: \nThe authors cha",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Investigating Different Context Types and Representations for Learning Word Embeddings \n\nHere is the abstract of the paper: The number of word embedding models is growing every year. Most of them learn word embeddings based on the co-occurrence information of words and their contexts. However, it's still an open question what is the best definition of context. We provide the first systematical investigation of different context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness under 6 tasks, which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community. \n\nHere is the partial review: - Strengths:\n\nThis paper presents a 2 x 2 x 3 x 10 array of accuracy results based on\nsystematically changing the parameters of embeddings models:\n\n(context type, position sensitive, embedding model, task), accuracy\n\n- context type \u2208 {Linear, Syntactic}\n- position sensitive \u2208 {True, False}\n- embedding model \u2208 {Skip Gram, BOW, GLOVE}\n- task \u2208 {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific.\ntasks}\n\nThe aim of these experiments was to investigate the variation in\nperformance as these parameters are changed. The goal of the study itself\nis interesting for the ACL community and similar papers have appeared\nbefore as workshop papers and have been well cited, such as Nayak et al.'s\npaper mentioned below.\n\n- Weaknesses:\nSince this paper essentially presents the effect of systematically changing the\n\ncontext types and position sensitivity, I will focus on the execution of the\ninvestigation and the analysis of the results, which I am afraid is not \nsatisfactory.\n\nA) The lack of hyper-parameter tuning is worrisome. E.g.\n   - 395 Unless otherwise notes, the number of word embedding dimension is set\nto 500.\n   - 232 It still enlarges the context vocabulary about 5 times in practice.\n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n\n  This is worrisome because lack of hyperparameter tuning makes it difficult to\nmake statements like method A is better than method B. E.g. bound methods may\nperform better with a lower dimensionality than unbound models, since their\neffective context vocabulary size is larger.\n\nB) The paper sometimes presents strange explanations for its results. E.g.\n   - 115 \"Experimental results suggest that although it's hard to find any \nuniversal insight, the characteristics of different contexts on different\nmodels are concluded according to specific tasks.\"\n\n   What does this sentence even mean? \n\n   - 580 Sequence labeling tasks tend to classify words with the same syntax \nto the same category. The ignorance of syntax for word embeddings which  are\nlearned by bound representation becomes beneficial. \n\n   These two sentences are contradictory, if a sequence labeling task\n   classified words with \"same syntax\" to same category then syntx becomes\n   a ver valuable feature. Bound representation's ignorance of syntax\n   should cause a drop in performance just like other tasks which does not\n   happen.\n\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a\n   systematic study of the word embeddings, and similarly the paper \n   \"Evaluating Word Embeddings Using a Representative Suite of Practical\n   Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at \n   ACL 2016. should have been cited. I understand that the focus of Nayak\n   et al's paper is not exactly the same as this paper, however they\n   provide recommendations about hyperparameter tuning and experiment\n   design and even provide a web interface for automatically running\n   tagging experiments using neural networks instead of the \"simple linear\n   classifiers\" used in the current paper.\n\nD) The paper uses a neural BOW words classifier for the text classification\ntasks\n   but a simple linear classifier for the sequence labeling tasks. What is\n   the justification for this choice of classifiers? Why not use a simple\n   neural classifier for the tagging tasks as well? I raise this point,\n   since the tagging task seems to be the only task where bound\n   representations are consistently beating the unbound representations,\n   which makes this task the odd one out. \n\n- General Discussion:\nFinally, I will make one speculative suggestion to the authors regarding\nthe analysis of the data. As I said earlier, this paper's main contribution is\nan\nanalysis of the following table.\n(context type, position sensitive, embedding model, task, accuracy)\nSo essentially there are 120 accuracy values that we want to explain in\nterms of the aspects of the model. It may be beneficial to perform\nfactor analysis or some other pattern mining technique on this 120 sample data.\n- Strengths: \nEvaluating bag of words and \"bound\" contexts from either dependencies or\nsentence ordering is important, and will be a useful reference to the\ncommunity. The experiments were relatively thorough (though some choices could\nuse further justification), and the authors used downstream tasks instead of\njust intrinsic evaluations.\n\n- Weaknesses: \nThe authors cha \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a systematic investigation of different context types and context representations for learning word embeddings. The array of accuracy results based on systematically changing the parameters of embedding models provides valuable insights for the ACL community. However, there are several weaknesses that need to be addressed. Firstly, the lack of hyperparameter tuning raises concerns about the validity of the comparisons between different methods. Additionally, some explanations provided for the results are confusing and contradictory, requiring further clarification. Moreover, the paper could benefit from citing related work such as Nayak et al.'s paper that provides recommendations on hyperparameter tuning and experiment design. The choice of classifiers for the different tasks also lacks justification and should be addressed. Finally, a suggestion is made to perform factor analysis or another pattern mining technique to further analyze the data. Despite these weaknesses, the paper's strengths lie in evaluating different context types and providing comprehensive experiments on downstream tasks. Overall, this paper contributes to the community and serves as a useful reference, but several improvements are necessary.\"\n}",
        "human_end_index": 4440
    },
    {
        "id": "21",
        "source": "PeerRead/acl_2017/",
        "title": "Transductive Non-linear Learning for Chinese Hypernym Prediction",
        "human_review": "The paper is clearly written, and the claims are well-supported.  The Related\nWork in particular is very thorough, and clearly establishes where the proposed\nwork fits in the field.\n\nI had two main questions about the method: (1) phrases are mentioned in section\n3.1, but only word representations are discussed.  How are phrase\nrepresentations derived?\n(2) There is no explicit connection between M^+ and M^- in the model, but they\nare indirectly connected through the tanh scoring function.  How do the learned\nmatrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what\nwould be the benefits/drawbacks of linking the two together directly, by\nenforcing some measure of dissimilarity?\n\nAdditionally, statistical significance of the observed improvements would be\nvaluable.\n\nTypographical comments:\n- Line 220: \"word/phase pair\" should be \"word/phrase pair\"\n- Line 245: I propose an alternate wording: instead of \"entities are translated\nto,\" say \"entities are mapped to\".  At first, I read that as a translation\noperation in the vector space, which I think isn't exactly what's being\ndescribed.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement\nin F-measure\"\n- Line 636: extraneous commas in citation\n- Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing)\n- Line 727: extraneous period and comma in citation\n- Strengths:\n\n1. Interesting research problem\n2. The method in this paper looks quite formal.\n3. The authors have released their dataset with the submission.\n4. The design of experiments is good.\n\n- Weaknesses:\n\n1. The advantage and disadvantage of the transductive learning has not yet\ndiscussed.\n\n- General Discussion:\n\nIn this paper, the authors introduce a transductive learning approach for\nChinese hypernym prediction, which is quite interesting problem. The authors\nestablish mappings from entities to hypernyms in the embedding space directly,\nwhich sounds also quite novel. This paper is well written and easy to follow.\nThe first part of their method, preprocessing using embeddings, is widely used\nmethod for the initial stage. But it's still a normal way to preprocess the\ninput data. The transductive model is an optimization framework for non-linear\nmapping utilizing both labeled and unlabeled data. The attached supplementary\nnotes about the method makes it more clear. The experimental results have shown\nthe effectiveness of the proposed method in this paper. The authors also\nreleased dataset, which contributes to similar research for other researchers\nin future.",
        "partial_review": "The paper is clearly written, and the claims are well-supported.  The Related\nWork in particular is very thorough, and clearly establishes where the proposed\nwork fits in the field.\n\nI had two main questions about the method: (1) phrases are mentioned in section\n3.1, but only word representations are discussed.  How are phrase\nrepresentations derived?\n(2) There is no explicit connection between M^+ and M^- in the model, but they\nare indirectly connected through the tanh scoring function.  How do the learned\nmatrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what\nwould be the benefits/drawbacks of linking the two together directly, by\nenforcing some measure of dissimilarity?\n\nAdditionally, statistical significance of the observed improvements would be\nvaluable.\n\nTypographical comments:\n- Line 220: \"word/phase pair\" should be \"word/phrase pair\"\n- Line 245: I propose an alternate wording: instead of \"entities are translated\nto,\" say \"entities are mapped to\".  At first, I read that as a translation\noperation in the vector space, which I think isn't exactly what's being\ndescribed.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement\nin F-measure\"\n- Line 636: extraneous commas in citation\n- Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing)\n- Line 727: extraneous period and comma in citatio",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Transductive Non-linear Learning for Chinese Hypernym Prediction \n\nHere is the abstract of the paper: Finding the correct hypernyms for entities\u00a0is essential for taxonomy learning, fine-grained entity categorization, query understanding, etc. Due to the flexibility\u00a0of the Chinese language, it is challenging\u00a0to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in\u00a0the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding\u00a0arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction. \n\nHere is the partial review: The paper is clearly written, and the claims are well-supported.  The Related\nWork in particular is very thorough, and clearly establishes where the proposed\nwork fits in the field.\n\nI had two main questions about the method: (1) phrases are mentioned in section\n3.1, but only word representations are discussed.  How are phrase\nrepresentations derived?\n(2) There is no explicit connection between M^+ and M^- in the model, but they\nare indirectly connected through the tanh scoring function.  How do the learned\nmatrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what\nwould be the benefits/drawbacks of linking the two together directly, by\nenforcing some measure of dissimilarity?\n\nAdditionally, statistical significance of the observed improvements would be\nvaluable.\n\nTypographical comments:\n- Line 220: \"word/phase pair\" should be \"word/phrase pair\"\n- Line 245: I propose an alternate wording: instead of \"entities are translated\nto,\" say \"entities are mapped to\".  At first, I read that as a translation\noperation in the vector space, which I think isn't exactly what's being\ndescribed.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement\nin F-measure\"\n- Line 636: extraneous commas in citation\n- Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing)\n- Line 727: extraneous period and comma in citatio \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper provides a clear and well-supported presentation of the proposed work. The Related Work section is especially thorough and effectively situates the proposed method within the field. However, there are a couple of areas that require clarification. Firstly, in section 3.1, phrases are mentioned but only word representations are discussed. It would be helpful to explain how phrase representations are derived. Additionally, although there is no explicit connection between M^+ and M^- in the model, they are indirectly connected through the tanh scoring function. It would be beneficial to elaborate on how the learned matrices compare to one another, such as whether M^- is similar to -1*M^+. Furthermore, discussing the potential advantages and disadvantages of directly linking the two together, by enforcing some measure of dissimilarity, would enrich the paper. Lastly, it would be valuable to include a discussion on the statistical significance of the observed improvements. \n\nA few minor typographical comments:\n- Line 220: \"word/phase pair\" should be \"word/phrase pair\"\n- Line 245: Instead of saying \"entities are translated to,\" consider using \"entities are mapped to\" to avoid potential confusion with a vector space translation operation.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement in F-measure\"\n- Line 636: Remove extraneous commas in citation\n- Line 646: Consider revising \"The most case\" to \"The most likely case\" (assuming this is the intended meaning)\n- Line 727: Remove extraneous period and comma in citation\"\n}",
        "human_end_index": 1377
    },
    {
        "id": 214,
        "source": "PeerRead/acl_2017/",
        "title": "Exploring Macro Discourse Structure with Macro-micro Unified Primary-secondary Relationship",
        "human_review": "This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from Chinese\ntreebank 8.0. They then built a model to recognize the primary-secondary\nrelations and 5 discourse relations (joint, elaboration, sequence, background,\ncause-result) in this corpus.\n\nThe paper is poorly written and I have difficulties to follow it. I strongly\nsuggest that the authors should find a native English speaker to carefully\nproofread the paper. Regarding the content, I have several concerns: \n\n1 The logic of the paper is not clear and justifiable: \n1) what are \"logical semantics\" and \"pragmatic function\"(line 115-116)? I'd\nprefer the authors to define them properly.\n\n2) macro discourse structure: there are some conflicts of the definition\nbetween macro structure and micro structure. Figure 4 demonstrates the\ncombination of macro discourse structure and micro discourse structure. There,\nthe micro discourse structure is presented *within paragraphs*. However, in the\nspecific example of micro discourse structure shown in Figure 6, the\nmicro-level discourse structure is *beyond the paragraph boundary* and captures\nthe discourse relations across paragraphs. This kind of micro-level discourse\nstructure is indeed similar to the macro structure proposed by the authors in\nFigure 5, and it's also genre independent. So, why can't we just use the\nstructure in Figure 6? What's the advantage of macro discourse structure\nproposed in Figure 5? For me, it's genre dependent and doesn't provide richer\ninformation compared to Figure 6.\n\nBy the way, why sentence 6 and sentence 15 are missing in Figure 5? Is it\nbecause they are subtitles? But sentence 12 which is a subtitle is present\nthere.\n\n2 Corpus construction (section 4) is not informative enough: without a\ndetailed example, it's hard to know the meaning of \"discourse topic, lead,\nabstract, paragraph topics (line 627-629)\". And you were saying you \"explore\nthe relationships between micro-structure and macro-structure\", but I can't\nfind the correspondent part.\n\nTable 4 is about agreement study The authors claimed \"Its very difficult to\nachieve high consistence because the judgments of relation and structure are\nvery subjective. Our measurement data is only taken on the layer of leaf\nnodes.\"--------> First, what are the leaf nodes? In the macro-level, they are\nparagraphs; in the micro-level, they are EDUs. Should we report the agreement\nstudy for macro-level and micro-level separately? Second, it seems for me that\nthe authors only take a subset of data to measure the agreement. This doesn't\nreflect the overall quality of the whole corpus, i.e., high agreement on the\nleaf nodes annotation doesn't ensure that we will get high agreement on the\nnon-leaf nodes annotation.\n\nSome other unclear parts in section 4:\n\nTable 4: \"discourse structure, discourse relation\" are not clear, what is\ndiscourse structure and what is discourse relation? \nTable 5: \"amount of macro discourse relations\", still not clear to me, you mean\nthe discourse relations between paragraphs? But in Figure 6, these relations\ncan exist both between sentences and between paragraphs.\n\n3 Experiments: since the main purpose of the paper is to provide richer\ndiscourse structure (both on macro and micro level), I would expect to see some\ninitial results in this direction. The current experiment is not very\nconvincing: a) no strong baselines; b) features are not clearly described and\nmotivated; c) I don't understand why only a sub set of discourse relations from\nTable 6 is chosen to perform the experiment of discourse relation recognition.\n\nIn general, I think the paper needs major improvement and currently it is not\nready for acceptance.\nThis paper presents a unified annotation that combines macrostructures and RST\nstructure in Chinese news articles. Essentially, RST structure is adopted for\neach paragraph and macrostructure is adopted on top of the paragraphs. \nWhile the view that nuclearity should not depend on the relation label itself\nbut also on the context is appealing, I find the paper having major issues in\nthe annotation and the experiments, detailed below:\n\n- The notion of \u201cprimary-secondary\u201d relationship is advocated much in the\npaper, but later in the paper that it became clear this is essentially the\nnotion of nuclearity, extended to macrostructure and making it\ncontext-dependent instead of relation-dependent. Even then, the status\nnuclear-nuclear, nuclear-satellite, satellite-nuclear are \u201credefined\u201d as\nnew concepts.\n\n- Descriptions of established theories in discourse are often incorrect. For\nexample, there is rich existing work on pragmatic functions of text but it is\nclaimed to be something little studied. There are errors in the related work\nsection, e.g., treating RST and the Chinese Dependency Discourse Treebank as\ndifferent as coherence and cohesion; the computational approach subsection\nlacking any reference to work after 2013; the performance table of nuclearity\nclassification confusing prior work for sentence-level and document-level\nparsing.\n\n- For the annotation, I find the macro structure annotation description\nconfusing; furthermore, statistics for the macro labels are not\nlisted/reported. The agreement calculation is also problematic; the paper\nstated that \"Our measurement data is only taken on the layer of leaf nodes\". I\ndon't think this can verify the validity of the annotation. There are multiple\nmentions in the annotation procedure that says \u201cprelim experiments show this\nis a good approach\u201d, but how? Finally it is unclear how the kappa values are\ncalculated since this is a structured task; is this the same calculation as RST\ndiscourse treebank?\n\n- It is said in the paper that nuclearity status closely associates with the\nrelation label itself. So what is the baseline performance that just uses the\nrelation label? Note that some features are not explained at all (e.g., what\nare \u201chierarchical characteristics\u201d?)\n\n- The main contribution of the paper is the combination of macro and micro\nstructure. However, in the experiments only relations at the micro level are\nevaluated; even so, only among 5 handpicked ones. I don't see how this\nevaluation can be used to verify the macro side hence supporting the paper.\n\n- The paper contains numerous grammatical errors. Also, there is no text\ndisplayed in Figure 7 to illustrate the example.\n- Strengths:\nThe macro discourse structure is a useful complement to micro structures like\nRST. The release of the dataset would be helpful to a range of NLP\napplications.\n\n- Weaknesses:\n1. Providing more comparisons with the existed CDTB will be better.\n2. The \u201cprimary-secondary\u201d relationship is mentioned a lot in this paper,\nhowever, its difference with the nuclearity is unclear and not precisely\ndefined.\n3. The experiment method is not clearly described in the paper.\n\n- General Discussion:",
        "partial_review": "This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from Chinese\ntreebank 8.0. They then built a model to recognize the primary-secondary\nrelations and 5 discourse relations (joint, elaboration, sequence, background,\ncause-result) in this corpus.\n\nThe paper is poorly written and I have difficulties to follow it. I strongly\nsuggest that the authors should find a native English speaker to carefully\nproofread the paper. Regarding the content, I have several concerns: \n\n1 The logic of the paper is not clear and justifiable: \n1) what are \"logical semantics\" and \"pragmatic function\"(line 115-116)? I'd\nprefer the authors to define them properly.\n\n2) macro discourse structure: there are some conflicts of the definition\nbetween macro structure and micro structure. Figure 4 demonstrates the\ncombination of macro discourse structure and micro discourse structure. There,\nthe micro discourse structure is presented *within paragraphs*. However, in the\nspecific example of micro discourse structure shown in Figure 6, the\nmicro-level discourse structure is *beyond the paragraph boundary* and captures\nthe discourse relations across paragraphs. This kind of micro-level discourse\nstructure is indeed similar to the macro structure proposed by the authors in\nFigure 5, and it's also genre independent. So, why can't we just use the\nstructure in Figure 6? What's the advantage of macro discourse structure\nproposed in Figure 5? For me, it's genre dependent and doesn't provide richer\ninformation compared to Figure 6.\n\nBy the way, why sentence 6 and sentence 15 are missing in Figure 5? Is it\nbecause they are subtitles? But sentence 12 which is a subtitle is present\nthere.\n\n2 Corpus construction (section 4) is not informative enough: without a\ndetailed example, it's hard to know the meaning of \"discourse topic, lead,\nabstract, paragraph topics (line 627-629)\". And you were saying you \"explore\nthe relationships between micro-structure and macro-structure\", but I can't\nfind the correspondent part.\n\nTable 4 is about agreement study The authors claimed \"Its very difficult to\nachieve high consistence because the judgments of relation and structure are\nvery subjective. Our measurement data is only taken on the layer of leaf\nnodes.\"--------> First, what are the leaf nodes? In the macro-level, they are\nparagraphs; in the micro-level, they are EDUs. Should we report the agreement\nstudy for macro-level and micro-level separately? Second, it seems for me that\nthe authors only take a subset of data to measure the agreement. This doesn't\nreflect the overall quality of the whole corpus, i.e., high agreement on the\nleaf nodes annotation doesn't ensure that we will get high agreement on the\nnon-leaf nodes annotation.\n\nSome other unclear parts in section 4:\n\nTable 4: \"discourse structure, discourse relation\" are not clear, what is\ndiscourse structure and what is discourse relation? \nTable 5: \"amount of macro discourse relations\", still not clear to me, you mean\nthe discourse relations between paragraphs? But in Figure 6, these relations\ncan exist both between sentences and between paragraphs.\n\n3 Experiments: since the main purpose of the paper is to provide richer\ndiscourse structure (both on macro and micro level), I would expect to see so",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Exploring Macro Discourse Structure with Macro-micro Unified Primary-secondary Relationship \n\nHere is the abstract of the paper: Discourse structure analysis is helpful for the machine to identify different types of discourse writing styles, and lays the foundation for the study of discourse automatic generation. In this paper, after studying the difference and the relationship between micro and macro discourse structures, we explore the macro discourse structure, and put forward a Macro Chinese Discourse Treebank (MCDTB) on the top of existing Chinese Discourse Treebank (CDTB), by unifying micro and macro discourse structures. Especially, at the micro level, we put forward the primary-secondary relationship from the logical semantic perspective, while at the macro level, we put forward the primary-secondary relationship from the pragmatic function perspective. Preliminary experiments show that our macro-micro unified schema is appropriate for the discourse structure analysis. \n\nHere is the partial review: This paper proposed a macro discourse structure scheme. The authors carried out\na pilot study annotating a corpus consisting of 97 news articles from Chinese\ntreebank 8.0. They then built a model to recognize the primary-secondary\nrelations and 5 discourse relations (joint, elaboration, sequence, background,\ncause-result) in this corpus.\n\nThe paper is poorly written and I have difficulties to follow it. I strongly\nsuggest that the authors should find a native English speaker to carefully\nproofread the paper. Regarding the content, I have several concerns: \n\n1 The logic of the paper is not clear and justifiable: \n1) what are \"logical semantics\" and \"pragmatic function\"(line 115-116)? I'd\nprefer the authors to define them properly.\n\n2) macro discourse structure: there are some conflicts of the definition\nbetween macro structure and micro structure. Figure 4 demonstrates the\ncombination of macro discourse structure and micro discourse structure. There,\nthe micro discourse structure is presented *within paragraphs*. However, in the\nspecific example of micro discourse structure shown in Figure 6, the\nmicro-level discourse structure is *beyond the paragraph boundary* and captures\nthe discourse relations across paragraphs. This kind of micro-level discourse\nstructure is indeed similar to the macro structure proposed by the authors in\nFigure 5, and it's also genre independent. So, why can't we just use the\nstructure in Figure 6? What's the advantage of macro discourse structure\nproposed in Figure 5? For me, it's genre dependent and doesn't provide richer\ninformation compared to Figure 6.\n\nBy the way, why sentence 6 and sentence 15 are missing in Figure 5? Is it\nbecause they are subtitles? But sentence 12 which is a subtitle is present\nthere.\n\n2 Corpus construction (section 4) is not informative enough: without a\ndetailed example, it's hard to know the meaning of \"discourse topic, lead,\nabstract, paragraph topics (line 627-629)\". And you were saying you \"explore\nthe relationships between micro-structure and macro-structure\", but I can't\nfind the correspondent part.\n\nTable 4 is about agreement study The authors claimed \"Its very difficult to\nachieve high consistence because the judgments of relation and structure are\nvery subjective. Our measurement data is only taken on the layer of leaf\nnodes.\"--------> First, what are the leaf nodes? In the macro-level, they are\nparagraphs; in the micro-level, they are EDUs. Should we report the agreement\nstudy for macro-level and micro-level separately? Second, it seems for me that\nthe authors only take a subset of data to measure the agreement. This doesn't\nreflect the overall quality of the whole corpus, i.e., high agreement on the\nleaf nodes annotation doesn't ensure that we will get high agreement on the\nnon-leaf nodes annotation.\n\nSome other unclear parts in section 4:\n\nTable 4: \"discourse structure, discourse relation\" are not clear, what is\ndiscourse structure and what is discourse relation? \nTable 5: \"amount of macro discourse relations\", still not clear to me, you mean\nthe discourse relations between paragraphs? But in Figure 6, these relations\ncan exist both between sentences and between paragraphs.\n\n3 Experiments: since the main purpose of the paper is to provide richer\ndiscourse structure (both on macro and micro level), I would expect to see so \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"me examples or experiments that demonstrate the advantages or improvements of the proposed macro discourse structure. While the authors mentioned preliminary experiments, they did not provide any specific results or findings from these experiments. Additionally, the evaluation of the proposed macro-micro unified schema is mentioned, but no details or metrics are provided to assess its appropriateness or effectiveness. Overall, the paper lacks clarity in its definitions and explanations, and more concrete examples and evaluations are needed to support the proposed macro discourse structure.\"\n}",
        "human_end_index": 3345
    },
    {
        "id": 216,
        "source": "PeerRead/acl_2017/",
        "title": "Topical Coherence in LDA-based Models through Induced Segmentation",
        "human_review": "- Strengths:\n1. The idea of assigning variable-length document segments with dependent\ntopics is novel. This prior knowledge is worth incorporated in the LDA-based\nframework.\n2. Whereas we do not have full knowledge on recent LDA literature, we find the\npart of related work quite convincing.\n3. The method proposed for segment sampling with O(M) complexity is impressive.\nIt is crucial for efficient computation. \n\n- Weaknesses:\n1. Compared to Balikas COLING16's work, the paper has a weaker visualization\n(Fig 5), which makes us doubt about the actual segmenting and assigning results\nof document. It could be more convincing to give a longer exemplar and make\ncolor assignment consistent with topics listed in Figure 4.\n2. Since the model is more flexible than that of Balikas COLING16, it may be\nunderfitting, could you please explain this more?\n\n- General Discussion:\nThe paper is well written and structured. The intuition introduced in the\nAbstract and again exemplified in the Introduction is quite convincing. The\nexperiments are of a full range, solid, and achieves better quantitative\nresults against previous works. If the visualization part is stronger, or\nexplained why less powerful visualization, it will be more confident. Another\nconcern is about computation efficiency, since the seminal LDA work proposed to\nuse Variational Inference which is faster during training compared to MCMC, we\nwish to see the author\u2019s future development.\n### Strengths:\n- Well-written, well-organized\n- Incorporate topical segmentation to copula LDA to enable the joint learning\nof segmentation and latent models\n- Experimental setting is well-designed and show the superiority of the\nproposed method from several different indicators and datasets\n\n### Weaknesses:\n- No comparison with \"novel\" segmentation methods\n\n### General Discussion:\nThis paper presents segLDAcop, a joint latent model for topics and segments.\nThis model is based on the copula LDA and incorporates the topical segmentation\nto the copula LDA. The authors conduct comprehensive experiments by using\nseveral different datasets and evaluation metrics to show the superiority of\ntheir model.\n\nThis paper is well-written and well-organized. The proposed model is a\nreasonable extension of the copula LDA to enable the joint inference of\nsegmentations and topics. Experimental setting is carefully designed and the\nsuperiority of the proposed model is fairly validated.\nOne concern is that the authors only use the simple NP segmentation and single\nword segmentation as segments of the previous method. As noted in the paper,\nthere are many work to smartly generate segments before running LDA though it\nis largely affected by the bias of statistical or linguistic tools used. The\ncomparison with more novel (state-of-the-art) segments would be preferable to\nprecisely show the validity of the proposed method.\n\n### Minor comment\n- In line 105, \"latent radom topics\" -> \"latent random topics\"",
        "partial_review": "- Strengths:\n1. The idea of assigning variable-length document segments with dependent\ntopics is novel. This prior knowledge is worth incorporated in the LDA-based\nframework.\n2. Whereas we do not have full knowledge on recent LDA literature, we find the\npart of related work quite convincing.\n3. The method proposed for segment sampling with O(M) complexity is impressive.\nIt is crucial for efficient computation. \n\n- Weaknesses:\n1. Compared to Balikas COLING16's work, the paper has a weaker visualization\n(Fig 5), which makes us doubt about the actual segmenting and assigning results\nof document. It could be more convincing to give a longer exemplar and make\ncolor assignment consistent with topics listed in Figure 4.\n2. Since the model is more flexible than that of Balikas COLING16, it may be\nunderfitting, could you please explain this more?\n\n- General Discussion:\nThe paper is well written and structured. The intuition introduced in the\nAbstract and again exemplified in the Introduction is quite convincing. The\nexperiments are of a full range, solid, and achieves better quantitative\nresults against previous works. If the visualization part is stronger, or\nexplained why less powerful visualization, it will be more confident. Another\nconcern is about computation efficiency, since the seminal LDA work proposed to\nuse Variational Inference which is faster during training compared to MCMC, we\nwish to see the author\u2019s",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Topical Coherence in LDA-based Models through Induced Segmentation \n\nHere is the abstract of the paper: This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification. \n\nHere is the partial review: - Strengths:\n1. The idea of assigning variable-length document segments with dependent\ntopics is novel. This prior knowledge is worth incorporated in the LDA-based\nframework.\n2. Whereas we do not have full knowledge on recent LDA literature, we find the\npart of related work quite convincing.\n3. The method proposed for segment sampling with O(M) complexity is impressive.\nIt is crucial for efficient computation. \n\n- Weaknesses:\n1. Compared to Balikas COLING16's work, the paper has a weaker visualization\n(Fig 5), which makes us doubt about the actual segmenting and assigning results\nof document. It could be more convincing to give a longer exemplar and make\ncolor assignment consistent with topics listed in Figure 4.\n2. Since the model is more flexible than that of Balikas COLING16, it may be\nunderfitting, could you please explain this more?\n\n- General Discussion:\nThe paper is well written and structured. The intuition introduced in the\nAbstract and again exemplified in the Introduction is quite convincing. The\nexperiments are of a full range, solid, and achieves better quantitative\nresults against previous works. If the visualization part is stronger, or\nexplained why less powerful visualization, it will be more confident. Another\nconcern is about computation efficiency, since the seminal LDA work proposed to\nuse Variational Inference which is faster during training compared to MCMC, we\nwish to see the author\u2019s \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The reviewers found the idea of assigning variable-length document segments with dependent topics to be novel and worth incorporating in the LDA-based framework. They also found the related work to be convincing and appreciated the method proposed for segment sampling with O(M) complexity. However, they pointed out weaknesses in the visualization of the results and requested a longer exemplar with consistent color assignment. They also expressed concerns about the model potentially underfitting compared to a previous work and requested further explanation. Overall, they found the paper to be well-written and structured, with solid experiments and better quantitative results compared to previous works. They suggested that stronger visualization and explanation for the weaker visualization would increase confidence in the results. They also expressed interest in the computation efficiency of the model, especially compared to Variational Inference. \"\n}",
        "human_end_index": 1431
    },
    {
        "id": 220,
        "source": "PeerRead/acl_2017/",
        "title": "Vancouver Welcomes You! Minimalist Location Metonymy Resolution",
        "human_review": "- Strengths: Great paper: Very well-written, interesting results, creative\nmethod, good and enlightening comparisons with earlier approaches. In addition,\nthe corpus, which is very carefully annotated, will prove to be a valuable\nresource for other researchers. I appreciated the qualitative discussion in\nsection 5. Too many ML papers just give present a results table without much\nfurther ado, but the discussion in this paper really provides insights for the\nreader. \n\n- Weaknesses: In section 4.1, the sentence \"The rest of the model\u2019s input is\nset to zeroes...\" is quite enigmatic until you look at Figure 2. Some extra\nsentence here explaining what is going on would be helpful. Furthermore, in\nFigure 2, in the input layers to the LSTMs it says \"5*Embeddings(50D)\" also for\nthe networks taking dependency labels as input. Surely this is wrong? (Or if it\nis correct, please explain what you mean). \n\n- General Discussion: Concerning the comment in 4.2 \"LSTMs are excellent at\nmodelling language sequences ... which is why we use this type of model.\". This\ncomment seems strange to me. This is not a sequential problem in that sense.\nFor each datapoint, you feed the network all 5 words in an example in one go,\nand the next example has nothing to do with the preceding one. The LSTM\narchitecture could still be superior, of course, but not for the reason you\nstate. Or have I misunderstood something? I'd be interested to hear the\nauthors' comments on this point.",
        "partial_review": "- Strengths: Great paper: Very well-written, interesting results, creative\nmethod, good and enlightening comparisons with earlier approaches. In addition,\nthe corpus, which is very carefully annotated, will prove to be a valuable\nresource for other researchers. I appreciated the qualitative discussion in\nsection 5. Too many ML papers just give present a results table without much\nfurther ado, but the discussion in this paper really provides insights for the\nreader. \n\n- Weaknesses: In section 4.1, the sentence \"The rest of the model\u2019s input is\nset to zeroes...\" is quite enigmatic until you look at Figure 2. Some extra\nsentence here explaining what is going on would be h",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Vancouver Welcomes You! Minimalist Location Metonymy Resolution \n\nHere is the abstract of the paper: Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines. \n\nHere is the partial review: - Strengths: Great paper: Very well-written, interesting results, creative\nmethod, good and enlightening comparisons with earlier approaches. In addition,\nthe corpus, which is very carefully annotated, will prove to be a valuable\nresource for other researchers. I appreciated the qualitative discussion in\nsection 5. Too many ML papers just give present a results table without much\nfurther ado, but the discussion in this paper really provides insights for the\nreader. \n\n- Weaknesses: In section 4.1, the sentence \"The rest of the model\u2019s input is\nset to zeroes...\" is quite enigmatic until you look at Figure 2. Some extra\nsentence here explaining what is going on would be h \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"elpful for clarity and understanding. It would also be helpful to provide more details about the methodology in section 3. While the paper mentions a 'minimalist neural approach' and a 'novel predicate window method', it would be beneficial to explain these concepts in more depth to aid readers who may not be familiar with them. Additionally, although the paper mentions that the results are competitive, it would be useful to provide more specific information on how the proposed approach compares to existing methods in terms of accuracy, precision, and recall. Finally, it would be beneficial to include limitations or potential areas for future research at the end of the paper to provide direction for further investigation in this field.\" \n}",
        "human_end_index": 677
    },
    {
        "id": 222,
        "source": "PeerRead/acl_2017/",
        "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
        "human_review": "- Strengths:\n\nWhen introducing the task, the authors use illustrative examples as well as the\ncontributions of this paper. \nRelated Works section covers the state of the art, at the same time pointing\nsimilarities and differences between related Works and the proposed method.\nThe presentation of the method is very clear, since the authors separate the\ntagging scheme and the end-to-end model.\nAnother strong point of this work is the baselines used to compare the proposed\nmethods with several classical triplet extraction methods.\nAt last, the presentation of examples from dataset used to illustrate the\nadvantages and disadvantages of the methods was very important. These outputs\ncomplement the explanation of tagging and evaluation of triplets. \n\n- Weaknesses:\n\nOne of the main contributions of this paper is a new tagging scheme described\nin Section 3.1, however there are already other schemes for NER and RE being\nused, such as IO, BIO and BILOU. \nDid the authors perform any experiment using other tagging scheme for this\nmethod?\nRegarding the dataset, in line 14, page 5, the authors cite the number of\nrelations (24), but they do not mention the number or the type of named\nentities.\nIn Section 4.1, the evaluation criteria of triplets are presented. These\ncriteria were based on previous work? As I see it, the stage of entity\nidentification is not complete if you consider only the head of the entity.\nRegarding example S3, shown in Table 3, the output of the LSTM-LSTM-Bias was\nconsidered correct? The text states that the relation role is wrong, although\nit is not clear if the relation role is considered in the evaluation. \n\n- General Discussion:\n\nThis paper proposes a novel tagging scheme and investigates the end-to-end\nmodels to jointly extract entities and relations. \nThe article is organized in a clear way and it is well written, which makes it\neasy to understand the proposed method.",
        "partial_review": "- Strengths:\n\nWhen introducing the task, the authors use illustrative examples as well as the\ncontributions of this paper. \nRelated Works section covers the state of the art, at the same time pointing\nsimilarities and differences between related Works and the proposed method.\nThe presentation of the method is very clear, since the authors separate the\ntagging scheme and the end-to-end model.\nAnother strong point of this work is the baselines used to compare the proposed\nmethods with several classical triplet extraction methods.\nAt last, the presentation of examples from dataset used to illustrate the\nadvantages and disadvantages of the methods was very important. These outputs\ncomplement the explanation of tagging and evaluation of triplets. \n\n- Weaknesses:\n\nOne of the main contributions of this paper is a new tagging scheme described\nin Section 3.1, however there are already other schemes for NER and RE being\nused, such as IO, BIO a",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme \n\nHere is the abstract of the paper: Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What\u2019s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset. \n\nHere is the partial review: - Strengths:\n\nWhen introducing the task, the authors use illustrative examples as well as the\ncontributions of this paper. \nRelated Works section covers the state of the art, at the same time pointing\nsimilarities and differences between related Works and the proposed method.\nThe presentation of the method is very clear, since the authors separate the\ntagging scheme and the end-to-end model.\nAnother strong point of this work is the baselines used to compare the proposed\nmethods with several classical triplet extraction methods.\nAt last, the presentation of examples from dataset used to illustrate the\nadvantages and disadvantages of the methods was very important. These outputs\ncomplement the explanation of tagging and evaluation of triplets. \n\n- Weaknesses:\n\nOne of the main contributions of this paper is a new tagging scheme described\nin Section 3.1, however there are already other schemes for NER and RE being\nused, such as IO, BIO a \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"IOB and IOBES are common tagging schemes for named entity recognition (NER) tasks, and there are also other tagging schemes used for relation extraction (RE) tasks, such as IOB2 and IOBES2. It would be beneficial for the authors to provide a more detailed comparison between their proposed tagging scheme and these existing schemes. This would allow readers to better understand the advantages and disadvantages of the novel tagging scheme and its potential impact on the joint extraction task. Additionally, although the experimental results show that the tagging based methods outperform most of the existing pipelined and joint learning methods, it would be helpful for the authors to provide more insight into why this is the case. Are there specific characteristics of the tagging based methods or the dataset used that contribute to their superior performance? Overall, the paper is well-written and the methodology is presented clearly. However, addressing these weaknesses would strengthen the paper and provide a more comprehensive evaluation of the proposed approach.\"\n}",
        "human_end_index": 947
    },
    {
        "id": "226",
        "source": "PeerRead/acl_2017/",
        "title": "Polish evaluation dataset for compositional distributional semantics models",
        "human_review": "- Strengths:\n\n1) This paper proposed a semi-automated framework (human generation -> auto\nexpansion -> human post-editing) to construct a compositional\nsemantic similarity evaluation data set.\n\n2) The proposed framework is used to create a Polish compositional semantic\nsimilarity evaluation data set which is useful for future work in developing\nPolish compositional semantic models.\n\n- Weaknesses:\n\n1) The proposed framework has only been tested on one language. It is not clear\nwhether the framework is portable to other languages. For example, the proposed\nframework relies on a dependency parser which may not be available in some\nlanguages or in poor performance in some other languages.\n\n2) The number of sentence pairs edited by leader judges is not reported so the\ncorrectness and efficiency of the automatic expansion framework can not be\nevaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs\nneed further post-editing is worrying. \n\n3) There are quite a number of grammatical mistakes. Here are some examples but\nnot the complete and exhaustive list:\n\nline 210, 212, 213: \"on a displayed image/picture\" -> \"in a displayed\nimage/picture\"\n\nline 428: \"Similarly as in\" -> \"Similar to\"\n\nA proofread pass on the paper is needed.\n\n- General Discussion:",
        "partial_review": "- Strengths:\n\n1) This paper proposed a semi-automated framework (human generation -> auto\nexpansion -> human post-editing) to construct a compositional\nsemantic similarity evaluation data set.\n\n2) The proposed framework is used to create a Polish compositional semantic\nsimilarity evaluation data set which is useful for future work in developing\nPolish compositional semantic models.\n\n- Weaknesses:\n\n1) The proposed framework has only been tested on one language. It is not clear\nwhether the framework is portable to other languages. For example, the proposed\nframework relies on a dependency parser which may not be available in some\nlanguages or in poor performance in some other lang",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Polish evaluation dataset for compositional distributional semantics models \n\nHere is the abstract of the paper: The paper presents a procedure of building an evaluation dataset. for the validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish. \n\nHere is the partial review: - Strengths:\n\n1) This paper proposed a semi-automated framework (human generation -> auto\nexpansion -> human post-editing) to construct a compositional\nsemantic similarity evaluation data set.\n\n2) The proposed framework is used to create a Polish compositional semantic\nsimilarity evaluation data set which is useful for future work in developing\nPolish compositional semantic models.\n\n- Weaknesses:\n\n1) The proposed framework has only been tested on one language. It is not clear\nwhether the framework is portable to other languages. For example, the proposed\nframework relies on a dependency parser which may not be available in some\nlanguages or in poor performance in some other lang \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a novel procedure for constructing an evaluation dataset for compositional distributional semantics models in languages other than English. The proposed semi-automated framework, consisting of human generation, auto expansion, and human post-editing, effectively addresses the challenge of gathering necessary extraneous resources and applying language-specific transformation rules. The resulting Polish evaluation dataset, comprising 10K sentence pairs annotated for semantic relatedness and entailment, is a valuable resource for evaluating compositional distributional semantics models of Polish. However, the main weakness of this paper lies in the fact that the proposed framework has only been tested on one language, and it remains unclear whether it can be applied to other languages. The reliance on a dependency parser might limit the applicability of the framework in languages without such a resource or in cases where the parser performs poorly. To strengthen the paper, it would be beneficial to discuss potential challenges and adaptations when applying the framework to other languages and highlight any limitations or considerations in the dataset construction process. Overall, this paper constitutes an important contribution but would benefit from further exploration and validation across multiple languages.\"\n}",
        "human_end_index": 687
    },
    {
        "id": 237,
        "source": "PeerRead/acl_2017/",
        "title": "A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space",
        "human_review": "# Summary\n\nThis paper presents an empirical study to identify a latent dimension of\nsentiment in word embeddings.\n\n# Strengths\n\n S1) Tackles a challenging problem of unsupervised sentiment analysis.\n\n S2) Figure 2, in particular, is a nice visualisation.\n\n# Weaknesses\n\n W1) The experiments, in particular, are very thin. I would recommend also\nmeasuring F1 performance and expanding the number of techniques compared.\n\n W2) The methodology description needs more organisation and elaboration. The\nideas tested are itemised, but insufficiently justified. \n\n W3) The results are quite weak in terms of the reported accuracy and depth of\nanalysis. Perhaps this work needs more development, particularly with\nvalidating the central assumption that the Distributional Hypothesis implies\nthat opposite words, although semantically similar, are separated well in the\nvector space?\n- Strengths\nThis paper deals with the issue of finding word polarity orientation in an\nunsupervised manner, using word embeddings.\n\n- Weaknesses\nThe paper presents an interesting and useful idea, however, at this moment, it\nis not applied to any test case. The ideas on which it is based are explained\nin an \"intuitive\" manner and not thoroughly justified. \n\n- General Discussion\nThis is definitely interesting work. The paper would benefit from more\nexperiments being carried out, comparison with other methods (for example, the\nuse of the Normalized Google Distance by authors such as (Balahur and Montoyo,\n2008) - http://ieeexplore.ieee.org/abstract/document/4906796/) and the\napplication of the knowledge obtained to a real sentiment analysis scenario. At\nthis point, the work, although promising, is in its initial phase.",
        "partial_review": "# Summary\n\nThis paper presents an empirical study to identify a latent dimension of\nsentiment in word embeddings.\n\n# Strengths\n\n S1) Tackles a challenging problem of unsupervised sentiment analysis.\n\n S2) Figure 2, in particular, is a nice visualisation.\n\n# Weaknesses\n\n W1) The experiments, in particular, are very thin. I would recommend also\nmeasuring F1 performance and expanding the number of techniques compared.\n\n W2) The methodology description needs more organisation and elaboration. The\nideas tested are itemised, but insufficiently justified. \n\n W3) The results are quite weak in terms of the reported accuracy and depth of\nanalysis. Perhaps this work needs more development, particularly with\nvalidating the central assumption that the Distributional Hypothesis implies\nthat opposite words, although semantically similar, are separated well in ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space \n\nHere is the abstract of the paper: This study implements a vector space model approach to measure the sentiment orientations of words. Two representative vectors for positive/negative polarity are constructed using high-dimensional vec-tor space in both an unsupervised and a semi-supervised manner. A sentiment ori-entation value per word is determined by taking the difference between the cosine distances against the two reference vec-tors. These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002). As a result of our experi-ment, we demonstrate that this novel ap-proach significantly outperforms the pre-vious unsupervised approach and is more practical and data efficient as well. \n\nHere is the partial review: # Summary\n\nThis paper presents an empirical study to identify a latent dimension of\nsentiment in word embeddings.\n\n# Strengths\n\n S1) Tackles a challenging problem of unsupervised sentiment analysis.\n\n S2) Figure 2, in particular, is a nice visualisation.\n\n# Weaknesses\n\n W1) The experiments, in particular, are very thin. I would recommend also\nmeasuring F1 performance and expanding the number of techniques compared.\n\n W2) The methodology description needs more organisation and elaboration. The\nideas tested are itemised, but insufficiently justified. \n\n W3) The results are quite weak in terms of the reported accuracy and depth of\nanalysis. Perhaps this work needs more development, particularly with\nvalidating the central assumption that the Distributional Hypothesis implies\nthat opposite words, although semantically similar, are separated well in  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"I have reviewed the paper titled 'A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space'.\\n\\nSummary:\\nThis paper presents an empirical study to identify a latent dimension of sentiment in word embeddings.\\n\\nStrengths:\\n1) Tackles a challenging problem of unsupervised sentiment analysis.\\n2) Figure 2, in particular, is a nice visualization.\\n\\nWeaknesses:\\n1) The experiments, in particular, are very thin. I would recommend also measuring F1 performance and expanding the number of techniques compared.\\n2) The methodology description needs more organization and elaboration. The ideas tested are itemized, but insufficiently justified.\\n3) The results are quite weak in terms of the reported accuracy and depth of analysis. Perhaps this work needs more development, particularly with validating the central assumption that the Distributional Hypothesis implies that opposite words, although semantically similar, are separated well in the multi-dimensional vector space.\\n\\nOverall, while the paper addresses a challenging problem and presents a novel approach, there are significant weaknesses that need to be addressed. The experimental setup needs to be expanded to include a more comprehensive evaluation, including F1 performance. Additionally, the methodology section should be improved to provide better justification for the chosen approach. The results should be strengthened by conducting a more in-depth analysis and validating the central assumption. With these improvements, the paper has the potential to make a valuable contribution to the field of sentiment analysis.\" \n}",
        "human_end_index": 857
    },
    {
        "id": 239,
        "source": "PeerRead/acl_2017/",
        "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks",
        "human_review": "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and simple supervised tasks. The main motivation is that word\nembeddings are generally used in a transfer learning setting, where evaluation\nis done based on how faster is to train a target model. The approach uses a set\nof simple tasks evaluated in a supervised fashion, including common benchmarks\nsuch as word similarity and word analogy. Experiments on a broad set of\nembeddings show that ranks tend to be task-specific and change according to the\namount of training data used.\n\nStrengths\n\n- The transfer learning / data efficiency motivation is an interesting one, as\nit directly relates to the idea of using embeddings as a simple\n\"semi-supervised\" approach.\n\nWeaknesses\n\n- A good evaluation approach would be one that propagates to end tasks.\nSpecifically, if the approach gives some rank R for a set of embeddings, I\nwould like it to follow the same rank for an end task like text classification,\nparsing or machine translation. However, the approach is not assessed in this\nway so it is difficult to trust the technique is actually more useful than what\nis traditionally done.\n- The discussion about injective embeddings seems completely out-of-topic and\ndoes not seem to add to the paper's understanding.\n- The experimental section is very confusing. Section 3.7 points out that the\nanalysis results in answers to questions as \"is it worth fitting syntax\nspecific embeddings even when supervised datset is large?\" but I fail to\nunderstand where in the evaluation the conclusion was made.\n- Still in Section 3.7, the manuscript says \"This hints, that purely\nunsupervised large scale pretraining might not be suitable for NLP\napplications\". This is a very bold assumption and I again fail to understand\nhow this can be concluded from the proposed evaluation approach.\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no\ncontrol over which corpora they were trained on. This limits the validity of\nthe evaluation shown in the paper.\n- The manuscript needs proofreading, especially in terms of citing figures in\nthe right places (why Figure 1, which is on page 3, is only cited in page 6?).\n\nGeneral Discussion\n\nI think the paper starts with a very interesting motivation but it does not\nproperly evaluate if their approach is good or not. As mentioned above, for any\nintrinsic evaluation approach I expect to see some study if the conclusions\npropagate to end tasks and this is not done in the paper. The lack of clarity\nand proofreading in the manuscript also hinders the understanding. In the\nfuture, I think the paper would vastly benefit from some extrinsic studies and\na more controlled experimental setting (using the same corpora to train all\nembeddings, for instance). But in the current state I do not think it is a good\naddition to the conference.\n- Strengths:\n\nThis paper proposed an interesting and important metric for evaluating the\nquality of word embeddings, which is the \"data efficiency\" when it is used in\nother supervised tasks.\n\nAnother interesting point in the paper is that the authors separated out three\nquestions: 1) whether supervised task offers more insights to evaluate\nembedding quality; 2) How stable is the ranking vs labeled data set size; 3)\nThe benefit to linear vs non-linear models.\n\nOverall, the authors presented comprehensive experiments to answer those\nquestions, and the results see quite interesting to know for the research\ncommunity.\n\n- Weaknesses:\n\nThe overall result is not very useful for ML practioners in this field, because\nit merely confirms what has been known or suspected, i.e. it depends on the\ntask at hand, the labeled data set size, the type of the model, etc. So, the\nresult in this paper is not very actionable. The reviewer noted that this\ncomprehensive analysis deepens the understanding of this topic.\n\n- General Discussion:\n\nThe paper's presentation can be improved. Specifically: \n\n1) The order of the figures/tables in the paper should match the order they are\nmentioned in the papers. Right now their order seems quite random.\n\n2) Several typos (L250, 579, etc). Please use a spell checker.\n\n3) Equation 1 is not very useful, and its exposition looks strange. It can be\nremoved, and leave just the text explanations.\n\n4) L164 mentions the \"Appendix\", but it is not available in the paper.\n\n5) Missing citation for the public skip-gram data set in L425.\n\n6) The claim in L591-593 is too strong. It must be explained more clearly, i.e.\nwhen it is useful and when it is not.\n\n7) The observation in L642-645 is very interesting and important. It will be\ngood to follow up on this and provide concrete evidence or example from some\nembedding. Some visualization may help too.\n\n8) In L672 should provide examples of such \"specialized word embeddings\" and\nhow they are different than the general purpose embedding.\n\n9) Figuer 3 is too small to read.",
        "partial_review": "This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and simple supervised tasks. The main motivation is that word\nembeddings are generally used in a transfer learning setting, where evaluation\nis done based on how faster is to train a target model. The approach uses a set\nof simple tasks evaluated in a supervised fashion, including common benchmarks\nsuch as word similarity and word analogy. Experiments on a broad set of\nembeddings show that ranks tend to be task-specific and change according to the\namount of training data used.\n\nStrengths\n\n- The transfer learning / data efficiency motivation is an interesting one, as\nit directly relates to the idea of using embeddings as a simple\n\"semi-supervised\" approach.\n\nWeaknesses\n\n- A good evaluation approach would be one that propagates to end tasks.\nSpecifically, if the approach gives some rank R for a set of embeddings, I\nwould like it to follow the same rank for an end task like text classification,\nparsing or machine translation. However, the approach is not assessed in this\nway so it is difficult to trust the technique is actually more useful than what\nis traditionally done.\n- The discussion about injective embeddings seems completely out-of-topic and\ndoes not seem to add to the paper's understanding.\n- The experimental section is very confusing. Section 3.7 points out that the\nanalysis results in answers to questions as \"is it worth fitting syntax\nspecific embeddings even when supervised datset is large?\" but I fail to\nunderstand where in the evaluation the conclusion was made.\n- Still in Section 3.7, the manuscript says \"This hints, that purely\nunsupervised large scale pretraining might not be suitable for NLP\napplications\". This is a very bold assumption and I again fail to understand\nhow this can be concluded from the proposed evaluation approach.\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no\ncontrol over which corpora they were trained on. This limits the validity of\nthe evaluation shown in the paper.\n- The manuscript needs proofreading, especially in terms of citing figures in\nthe right places (why Figure 1, which is on page 3, is only cited in page 6?).\n\nGeneral Discussion\n\nI think the paper starts with a very interesting motivation but it does not\nproperly evaluate if their approach is good or not. As mentioned above, for any\nintrinsic evaluation approach I expect to see some study if the conclusions\npropagate to end tasks and this is n",
        "task3_prompt": "\nComplete a partially written peer review about the paper: How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks \n\nHere is the abstract of the paper: Recent practice in word embeddings points towards importance of learning specialized representations.  Additionally, in many applications purely unsupervised training is useful only under limited dataset size. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what \\emph{useful} information is \\emph{easily} accessible. Specifically, we argue that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data should be varied and scores of a \\emph{supervised} model for each subset should be reported (as commonly done in transfer learning).   In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and  brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non--linearly encoded in the embedding space, which questions the cosine--based, unsupervised, evaluation methods.  All results and analysis scripts are available online. \n\nHere is the partial review: This paper proposes a framework for evaluation of word embeddings based on data\nefficiency and simple supervised tasks. The main motivation is that word\nembeddings are generally used in a transfer learning setting, where evaluation\nis done based on how faster is to train a target model. The approach uses a set\nof simple tasks evaluated in a supervised fashion, including common benchmarks\nsuch as word similarity and word analogy. Experiments on a broad set of\nembeddings show that ranks tend to be task-specific and change according to the\namount of training data used.\n\nStrengths\n\n- The transfer learning / data efficiency motivation is an interesting one, as\nit directly relates to the idea of using embeddings as a simple\n\"semi-supervised\" approach.\n\nWeaknesses\n\n- A good evaluation approach would be one that propagates to end tasks.\nSpecifically, if the approach gives some rank R for a set of embeddings, I\nwould like it to follow the same rank for an end task like text classification,\nparsing or machine translation. However, the approach is not assessed in this\nway so it is difficult to trust the technique is actually more useful than what\nis traditionally done.\n- The discussion about injective embeddings seems completely out-of-topic and\ndoes not seem to add to the paper's understanding.\n- The experimental section is very confusing. Section 3.7 points out that the\nanalysis results in answers to questions as \"is it worth fitting syntax\nspecific embeddings even when supervised datset is large?\" but I fail to\nunderstand where in the evaluation the conclusion was made.\n- Still in Section 3.7, the manuscript says \"This hints, that purely\nunsupervised large scale pretraining might not be suitable for NLP\napplications\". This is a very bold assumption and I again fail to understand\nhow this can be concluded from the proposed evaluation approach.\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no\ncontrol over which corpora they were trained on. This limits the validity of\nthe evaluation shown in the paper.\n- The manuscript needs proofreading, especially in terms of citing figures in\nthe right places (why Figure 1, which is on page 3, is only cited in page 6?).\n\nGeneral Discussion\n\nI think the paper starts with a very interesting motivation but it does not\nproperly evaluate if their approach is good or not. As mentioned above, for any\nintrinsic evaluation approach I expect to see some study if the conclusions\npropagate to end tasks and this is n \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper proposes a framework for evaluation of word embeddings based on data efficiency and simple supervised tasks. The main motivation is that word embeddings are generally used in a transfer learning setting, where evaluation is done based on how faster is to train a target model. The approach uses a set of simple tasks evaluated in a supervised fashion, including common benchmarks such as word similarity and word analogy. Experiments on a broad set of embeddings show that ranks tend to be task-specific and change according to the amount of training data used.\\n\\nStrengths\\n\\n- The transfer learning / data efficiency motivation is an interesting one, as it directly relates to the idea of using embeddings as a simple \\\"semi-supervised\\\" approach.\\n\\nWeaknesses\\n\\n- A good evaluation approach would be one that propagates to end tasks. Specifically, if the approach gives some rank R for a set of embeddings, I would like it to follow the same rank for an end task like text classification, parsing or machine translation. However, the approach is not assessed in this way so it is difficult to trust the technique is actually more useful than what is traditionally done.\\n- The discussion about injective embeddings seems completely out-of-topic and does not seem to add to the paper's understanding.\\n- The experimental section is very confusing. Section 3.7 points out that the analysis results in answers to questions as \\\"is it worth fitting syntax specific embeddings even when supervised datset is large?\\\" but I fail to understand where in the evaluation the conclusion was made.\\n- Still in Section 3.7, the manuscript says \\\"This hints, that purely unsupervised large scale pretraining might not be suitable for NLP applications\\\". This is a very bold assumption and I again fail to understand how this can be concluded from the proposed evaluation approach.\\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no control over which corpora they were trained on. This limits the validity of the evaluation shown in the paper.\\n- The manuscript needs proofreading, especially in terms of citing figures in the right places (why Figure 1, which is on page 3, is only cited in page 6?).\\n\\nGeneral Discussion\\n\\nI think the paper starts with a very interesting motivation but it does not properly evaluate if their approach is good or not. As mentioned above, for any intrinsic evaluation approach I expect to see some study if the conclusions propagate to end tasks and this is n\"\n}",
        "human_end_index": 2504
    },
    {
        "id": "251",
        "source": "PeerRead/acl_2017/",
        "title": "Skip-Gram - Zipf + Uniform = Vector Additivity",
        "human_review": "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of skip-gram, and\ngenerally found it inspiring to read. I very much appreciate the effort to\nunderstand the assumptions of the model, and the way it affects (or is affected\nby) the composition operations that it is used to perform. In that respect, I\nthink it is a very worthwhile read for the community.\n\nMy main criticism is however that the paper is linguistically rather naive. The\nauthors' use of 'compositionality' (as an operation that takes a set of words\nand returns another with the same meaning) is extremely strange. Two words can\nof course be composed and produce a vector that is a) far away from both; b)\ndoes not correspond to any other concept in the space; c) still has meaning\n(productivity wouldn't exist otherwise!) Compositionality in linguistic terms\nsimply refers to the process of combining linguistic constituents to produce\nhigher-level constructs. It does not assume any further constraint, apart from\nsome vague (and debatable) notion of semantic transparency. The paper's\nimplication (l254) that composition takes place over sets is also wrong:\nordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a\nwell-known shortcoming of additive composition. \n\nAnother important aspect is that there are pragmatic factors that make humans\nprefer certain phrases to single words in particular contexts (and the\nopposite), naturally changing the underlying distribution of words in a large\ncorpus. For instance, talking of a 'male royalty' rather than a 'king' or\n'prince' usually has implications with regard to the intent of the speaker\n(here, perhaps highlighting a gender difference). This means that the equation\nin l258 (or for that matter the KL-divergence modification) does not hold, not\nbecause of noise in the data, but because of fundamental linguistic processes.\nThis point may be addressed by the section on SDR, but I am not completely sure\n(see my comments below).\n\nIn a nutshell, I think the way that the authors present composition is flawed,\nbut the paper convinces me that this is indeed what happens in skip-gram, and I\nthink this is an interesting contribution. \n\nThe part about Sufficient Dimensionality Reduction seems a little disconnected\nfrom the previous argument as it stands. I'm afraid I wasn't able to fully\nfollow the argument, and I would be grateful for some clarification in the\nauthors' response. If I understand it well, the argument is that skip-gram\nproduces a model where a word's neighbours follow some exponential\nparametrisation of a categorical distribution, but it is unclear whether this\nactually reflects the distribution of the corpus (as opposed to what happens\nin, say, a pure count-based model). The fact that skip-gram performs well\ndespite not reflecting the data is that it implements some form of SDR, which\ndoes not need to make any assumption about the underlying form of the data. But\nthen, is it fair to say that the resulting representations are optimised for\ntasks where geometrical regularities are important, regardless of the actual\npattern of the data? I.e. there some kind of denoising going on?\n\nMinor comments:\n\n- The abstract is unusually long and could, I think, be shortened.\n\n- para starting l71: I think it would be misconstrued to see circularity here.\nFirth observed that co-occurrence effects were correlated with similarity\njudgements, but those judgements are the very cognitive processes that we are\ntrying to model with statistical methods. Co-occurrence effects and vector\nspace word representations are in some sense 'the same thing', modelling an\nunderlying linguistic process we do not have direct observations for. So\npair-wise similarity is not there to break any circularity, it is there because\nit better models the kind of judgements humans known to make.\n\n- l296: I think 'paraphrase' would be a better word than 'synonym' here, given\nthat we are comparing a set of words with a unique lexical item.\n\n- para starting l322: this is interesting, and actually, a lot of the zipfian\ndistribution (the long tail) is fairly uniform.\n\n- l336: it is probably worth pointing out that the analogy relation does not\nhold so well in practice and requires to 'ignore' the first returned neighbour\nof the analogy computation (which is usually one of the observed terms).\n\n- para starting l343: I don't find it so intuitive to say that 'man' would be a\nsynonym/paraphrase of anything involving 'woman'. The subtraction involved in\nthe analogy computation is precisely not a straightforward composition\noperation, as it involves an implicit negation. \n\n- A last, tiny general comment. It is usual to write p(w|c) to mean the\nprobability of a word given a context, but in the paper 'w' is actually the\ncontext and 'c' the target word. It makes reading a little bit harder...\nPerhaps change the notation?\n\nLiterature:\n\nThe claim that Arora (2016) is the only work to try and understand vector\ncomposition is a bit strong. For instance, see the work by Paperno & Baroni on\nexplaining the success of addition as a composition method over PMI-weighted\nvectors:\n\nD. Paperno and M. Baroni. 2016. When the whole is less than the sum of its\nparts: How composition affects PMI values in distributional semantic vectors.\nComputational Linguistics 42(2): 345-350.\n\n***\nI thank the authors for their response and hope to see this paper accepted.",
        "partial_review": "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of skip-gram, and\ngenerally found it inspiring to read. I very much appreciate the effort to\nunderstand the assumptions of the model, and the way it affects (or is affected\nby) the composition operations that it is used to perform. In that respect, I\nthink it is a very worthwhile read for the community.\n\nMy main criticism is however that the paper is linguistically rather naive. The\nauthors' use of 'compositionality' (as an operation that takes a set of words\nand returns another with the same meaning) is extremely strange. Two words can\nof course be composed and produce a vector that is a) far away from both; b)\ndoes not correspond to any other concept in the space; c) still has meaning\n(productivity wouldn't exist otherwise!) Compositionality in linguistic terms\nsimply refers to the process of combining linguistic constituents to produce\nhigher-level constructs. It does not assume any further constraint, apart from\nsome vague (and debatable) notion of semantic transparency. The paper's\nimplication (l254) that composition takes place over sets is also wrong:\nordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a\nwell-known shortcoming of additive composition. \n\nAnother important aspect is that there are pragmatic factors that make humans\nprefer certain phrases to single words in particular contexts (and the\nopposite), naturally changing the underlying distribution of words in a large\ncorpus. For instance, talking of a 'male royalty' rather than a 'king' or\n'prince' usually has implications with regard to the intent of the speaker\n(here, perhaps highlighting a gender difference). This means that the equation\nin l258 (or for that matter the KL-divergence modification) does not hold, not\nbecause of noise in the data, but because of fundamental linguistic processes.\nThis point may be addressed by the section on SDR, but I am not completely sure\n(see my comments below).\n\nIn a nutshell, I think the way that the authors present composition is flawed,\nbut the paper convinces me that this is indeed what happens in skip-gram, and I\nthink this is an interesting contribution. \n\nThe part about Sufficient Dimensionality Reduction seems a little disconnected\nfrom the previous argument as it stands. I'm afraid I wasn't able to fully\nfollow the argument, and I would be grateful for some clarification",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Skip-Gram - Zipf + Uniform = Vector Additivity \n\nHere is the abstract of the paper: In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected \"side-effect\" of such models is that their vectors often exhibit compositionality, i.e., \\emph{adding} two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., \"man\" + \"royal\" = \"king\".  This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator.   Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models. \n\nHere is the partial review: This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of skip-gram, and\ngenerally found it inspiring to read. I very much appreciate the effort to\nunderstand the assumptions of the model, and the way it affects (or is affected\nby) the composition operations that it is used to perform. In that respect, I\nthink it is a very worthwhile read for the community.\n\nMy main criticism is however that the paper is linguistically rather naive. The\nauthors' use of 'compositionality' (as an operation that takes a set of words\nand returns another with the same meaning) is extremely strange. Two words can\nof course be composed and produce a vector that is a) far away from both; b)\ndoes not correspond to any other concept in the space; c) still has meaning\n(productivity wouldn't exist otherwise!) Compositionality in linguistic terms\nsimply refers to the process of combining linguistic constituents to produce\nhigher-level constructs. It does not assume any further constraint, apart from\nsome vague (and debatable) notion of semantic transparency. The paper's\nimplication (l254) that composition takes place over sets is also wrong:\nordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a\nwell-known shortcoming of additive composition. \n\nAnother important aspect is that there are pragmatic factors that make humans\nprefer certain phrases to single words in particular contexts (and the\nopposite), naturally changing the underlying distribution of words in a large\ncorpus. For instance, talking of a 'male royalty' rather than a 'king' or\n'prince' usually has implications with regard to the intent of the speaker\n(here, perhaps highlighting a gender difference). This means that the equation\nin l258 (or for that matter the KL-divergence modification) does not hold, not\nbecause of noise in the data, but because of fundamental linguistic processes.\nThis point may be addressed by the section on SDR, but I am not completely sure\n(see my comments below).\n\nIn a nutshell, I think the way that the authors present composition is flawed,\nbut the paper convinces me that this is indeed what happens in skip-gram, and I\nthink this is an interesting contribution. \n\nThe part about Sufficient Dimensionality Reduction seems a little disconnected\nfrom the previous argument as it stands. I'm afraid I wasn't able to fully\nfollow the argument, and I would be grateful for some clarification \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper delves into the mathematical properties of the skip-gram model, explaining the reason for its success on the analogy task and for the general superiority of additive composition models. It also establishes a link between skip-gram and Sufficient Dimensionality Reduction. \\n\\nI liked the focus of this paper on explaining the properties of skip-gram, and generally found it inspiring to read. I very much appreciate the effort to understand the assumptions of the model, and the way it affects (or is affected by) the composition operations that it is used to perform. In that respect, I think it is a very worthwhile read for the community.\\n\\nMy main criticism is however that the paper is linguistically rather naive. The authors' use of 'compositionality' (as an operation that takes a set of words and returns another with the same meaning) is extremely strange. Two words can of course be composed and produce a vector that is a) far away from both; b) does not correspond to any other concept in the space; c) still has meaning (productivity wouldn't exist otherwise!) Compositionality in linguistic terms simply refers to the process of combining linguistic constituents to produce higher-level constructs. It does not assume any further constraint, apart from some vague (and debatable) notion of semantic transparency. The paper's implication (l254) that composition takes place over sets is also wrong: ordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a well-known shortcoming of additive composition. \\n\\nAnother important aspect is that there are pragmatic factors that make humans prefer certain phrases to single words in particular contexts (and the opposite), naturally changing the underlying distribution of words in a large corpus. For instance, talking of a 'male royalty' rather than a 'king' or 'prince' usually has implications with regard to the intent of the speaker (here, perhaps highlighting a gender difference). This means that the equation in l258 (or for that matter the KL-divergence modification) does not hold, not because of noise in the data, but because of fundamental linguistic processes. This point may be addressed by the section on SDR, but I am not completely sure (see my comments below).\\n\\nIn a nutshell, I think the way that the authors present composition is flawed, but the paper convinces me that this is indeed what happens in skip-gram, and I think this is an interesting contribution. \\n\\nThe part about Sufficient Dimensionality Reduction seems a little disconnected from the previous argument as it stands. I'm afraid I wasn't able to fully follow the argument, and I would be grateful for some clarification\"\n}",
        "human_end_index": 2689
    },
    {
        "id": "256",
        "source": "PeerRead/acl_2017/",
        "title": "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders",
        "human_review": "Review, ACL 2017, paper 256:\n\nThis paper extends the line of work which models generation in dialogue as a\nsequence to sequence generation problem, where the past N-1 utterances (the\n\u2018dialogue context\u2019) are encoded into a context vector (plus potential\nother, hand-crafted features), which is then decoded into a response: the Nth\nturn in the dialogue. As it stands, such models tend to suffer from lack of\ndiversity, specificity and local coherence in the kinds of response they tend\nto produce when trained over large dialogue datasets containing many topics\n(e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce\ndiverse responses using the decoder, e.g. through word-by-word beam search\n(which has been shown not to work very well, even lose crucial information\nabout grammar and valid sequences), or via a different objective function (such\nas in Li et. al.\u2019s work) the authors introduce a latent variable, z, over\nwhich a probability distribution is induced as part of the network. At\nprediction time, after encoding utterances 1 to k, a context z is sampled, and\nthe decoder is greedily used to generate a response from this. The evaluation\nshows small improvements in BLEU scores over a vanilla seq2seq model that does\nnot involve learning a probability distribution over contexts and sampling from\nthis.\n\nThe paper is certainly impressive from a technical point of view, i.e. in the\napplication of deep learning methods, specifically conditioned variational auto\nencoders, to the problem of response generation, and its attendant difficulties\nin training such models. Their use of Information-Retrieval techniques to get\nmore than one reference response is also interesting. \n\nI have some conceptual comments on the introduction and the motivations behind\nthe work, some on the model architecture, and the evaluation which I write\nbelow in turn:\n\nComments on the introduction and motivations\u2026. \n\nThe authors seem not fully aware of the long history of this field, and its\nvarious facets, whether from a theoretical perspective, or from an applied one.\n\n1. \u201c[the dialogue manager] typically takes a new utterance and the dialogue\ncontext as input, and generates discourse level decisions.\u201d \n\n        This is not accurate. Traditionally at least, the job of the dialogue\nmanager is to select actions (dialogue acts) in a particular dialogue context.\nThe                    action chosen is then passed to a separate generation\nmodule\nfor\nrealisation. Dialogue management is usually done in the context of task-based\nsystems which are goal driven. The dialogue manager is to choose actions which\nare optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few\nsteps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer\nand colleagues, and various publications from Steve Young, Milica Gasic and\ncolleagues for an overview of the large literature on Reinforcement Learning\nand MDP models for task-based dialogue systems.\n\n2. The authors need to make a clear distinction between task-based,\ngoal-oriented dialogue, and chatbots/social bots, the latter being usually no\nmore than a language model, albeit a sophisticated one (though see Wen et. al.\n2016). What is required from these two types of system is usually distinct.\nWhereas the former is required to complete a task, the latter is, perhaps only\nrequired to keep the user engaged. Indeed the data-driven methods that have\nbeen used to build such systems are usually very different. \n3. The authors refer to \u2018open-domain\u2019 conversation. I would suggest that\nthere is no such thing as open-domain conversation - conversation is always in\nthe context of some activity and for doing/achieving something specific in the\nworld. And it is this overarching goal, the overarching activity, this\noverarching genre, which determines the outward shape of dialogues and\ndetermines what sorts of dialogue structure are coherent. Coherence itself is\nactivity/context-specific. Indeed a human is not capable of open-domain\ndialogue: if they are faced with a conversational topic or genre that they have\nnever participated in, they would embarrass themselves with utterances that\nwould look incoherent and out of place to others already familiar with it.\n(think of a random person on the street trying to follow the conversations at\nsome coffee break at ACL). This is the fundamental problem I see with systems\nthat attempt to use data from an EXTREMELY DIVERSE, open-ended set of\nconversational genres (e.g. movie subtitles) in order to train one model,\nmushing everything together so that what emerges at the other end is just very\ngood grammatical structure. Or very generic responses. \n\nComments on the model architecture:\n\nRather than generate from a single encoded context, the authors induce a\ndistribution over possible contexts, sample from this, and generate greedily\nwith the decoder. It seems to me that this general model is counter intuitive,\nand goes against evidence from the Linguistic/Psycholinguistic literature on\ndialogue: this literature shows that people tend to resolve potential problems\nin understanding and acceptance very locally - i.e. make sure they agree on\nwhat the context of the conversation is - and only then move on with the rest\nof the conversation, so that at any given point, there is little uncertainty\nabout the current context of the conversation. The massive diversity one sees\nresults from the diversity in what the conversation is actually trying to\nachieve (see above), diversity in topics and contexts etc, so that in a given,\nfixed context, there is a multitude of possible next actions, all coherent, but\nleading the conversation down a different path.\n\nIt therefore seems strange to me at least to shift the burden of explaining\ndiversity and coherence in follow-up actions to that of the\nlinguistic/verbal/surface contexts in which they are uttered, though of course,\nuncertainty here can also arise as a result of mismatches in vocabulary,\ngrammars, concepts, people\u2019s backgrounds etc. But this probably wouldn\u2019t\nexplain much of the variation in follow-up response. \n\nIn fact, at least as far as task-based Dialogue systems are concerned, the\nchallenge is to capture synonymy of contexts, i.e. dialogues that are distinct\non the surface, but lead to the same or similar context, either in virtue of\ninteractional and syntactic equivalence relations, or synonymy relations that\nmight hold in a particular domain between words or sequences of words (e.g.\n\u201cwhat is your destination?\u201d = \u201cwhere would you like to go?\u201d in a flight\nbooking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon,\n2016 - the latter use a grammar to cluster semantically similar dialogues.\n\nComments on the evaluation:\n\nThe authors seek to show that their model can generate more coherent, and more\ndiverse responses. The evaluation method, though very interesting, seems to\naddress coherence but not diversity, despite what they say in section 5.2:\n\nThe precision and recall metrics measure distance between ground truth\nutterances and the ones the model generates, but not that between the generated\nutterances themselves (unless I\u2019m misunderstanding the evaluation method).\nSee e.g. Li et al. who measure diversity by counting the number distinct\nn-grams in the generated responses.\n\nFurthermore, I\u2019m not sure that the increase in BLEU scores are meaningful:\nthey are very small. In the qualitative assessment of the generated responses,\none certainly sees more diversity, and more contentful utterances in the\nexamples provided. But I can\u2019t see how frequent such cases in fact are.\n\nAlso, it would have made for a stronger, more meaningful paper if the authors\nhad compared their results with other work, (e.g. Li et. al) that use very\ndifferent methods to promote diversity (e.g. by using a different objective\nfunction). The authors in fact do not mention this, or characterise it\nproperly, despite actually referring to Li et. al. 2015.\nThis paper presents a neural sequence-to-sequence model for encoding dialog\ncontexts followed by decoding system responses in open-domain conversations.\nThe authors introduced conditional variational autoencoder (CVAE) which is a\ndeep neural network-based generative model to learn the latent variables for\ndescribing responses conditioning dialog contexts and dialog acts.\nThe proposed models achieved better performances than the baseline based on RNN\nencoder-decoder without latent variables in both quantitative and qualitative\nevaluations.\n\nThis paper is well written with clear descriptions, theoretically sound ideas,\nreasonable comparisons, and also detailed analysis.\nI have just a few minor comments as follows:\n\n- Would it be possible to provide statistical significance of the results from\nthe proposed models compared to the baseline in quantitative evaluation? The\ndifferences don't seem that much for some metrics.\n\n- Considering the importance of dialog act in kgCVAE model, the DA tagging\nperformances should affect the quality of the final results. Would it be there\nany possibility to achieve further improvement by using better DA tagger?\nRecently, deep learning models have achieved better performances than SVM also\nin DA tagging.\n\n- What do you think about doing human evaluation as a part of qualitative\nanalysis? It could be costly, but worth a try to analyze the results in more\npragmatic perspective.\n\n- As a future direction, it could be also interesting if kgCVAE model is\napplied to more task-oriented human-machine conversations which usually have\nmuch richer linguistic features available than open conversation.\n\n- In Table 1, 'BLUE-1 recall' needs to be corrected to 'BLEU-1 recall'.",
        "partial_review": "Review, ACL 2017, paper 256:\n\nThis paper extends the line of work which models generation in dialogue as a\nsequence to sequence generation problem, where the past N-1 utterances (the\n\u2018dialogue context\u2019) are encoded into a context vector (plus potential\nother, hand-crafted features), which is then decoded into a response: the Nth\nturn in the dialogue. As it stands, such models tend to suffer from lack of\ndiversity, specificity and local coherence in the kinds of response they tend\nto produce when trained over large dialogue datasets containing many topics\n(e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce\ndiverse responses using the decoder, e.g. through word-by-word beam search\n(which has been shown not to work very well, even lose crucial information\nabout grammar and valid sequences), or via a different objective function (such\nas in Li et. al.\u2019s work) the authors introduce a latent variable, z, over\nwhich a probability distribution is induced as part of the network. At\nprediction time, after encoding utterances 1 to k, a context z is sampled, and\nthe decoder is greedily used to generate a response from this. The evaluation\nshows small improvements in BLEU scores over a vanilla seq2seq model that does\nnot involve learning a probability distribution over contexts and sampling from\nthis.\n\nThe paper is certainly impressive from a technical point of view, i.e. in the\napplication of deep learning methods, specifically conditioned variational auto\nencoders, to the problem of response generation, and its attendant difficulties\nin training such models. Their use of Information-Retrieval techniques to get\nmore than one reference response is also interesting. \n\nI have some conceptual comments on the introduction and the motivations behind\nthe work, some on the model architecture, and the evaluation which I write\nbelow in turn:\n\nComments on the introduction and motivations\u2026. \n\nThe authors seem not fully aware of the long history of this field, and its\nvarious facets, whether from a theoretical perspective, or from an applied one.\n\n1. \u201c[the dialogue manager] typically takes a new utterance and the dialogue\ncontext as input, and generates discourse level decisions.\u201d \n\n        This is not accurate. Traditionally at least, the job of the dialogue\nmanager is to select actions (dialogue acts) in a particular dialogue context.\nThe                    action chosen is then passed to a separate generation\nmodule\nfor\nrealisation. Dialogue management is usually done in the context of task-based\nsystems which are goal driven. The dialogue manager is to choose actions which\nare optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few\nsteps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer\nand colleagues, and various publications from Steve Young, Milica Gasic and\ncolleagues for an overview of the large literature on Reinforcement Learning\nand MDP models for task-based dialogue systems.\n\n2. The authors need to make a clear distinction between task-based,\ngoal-oriented dialogue, and chatbots/social bots, the latter being usually no\nmore than a language model, albeit a sophisticated one (though see Wen et. al.\n2016). What is required from these two types of system is usually distinct.\nWhereas the former is required to complete a task, the latter is, perhaps only\nrequired to keep the user engaged. Indeed the data-driven methods that have\nbeen used to build such systems are usually very different. \n3. The authors refer to \u2018open-domain\u2019 conversation. I would suggest that\nthere is no such thing as open-domain conversation - conversation is always in\nthe context of some activity and for doing/achieving something specific in the\nworld. And it is this overarching goal, the overarching activity, this\noverarching genre, which determines the outward shape of dialogues and\ndetermines what sorts of dialogue structure are coherent. Coherence itself is\nactivity/context-specific. Indeed a human is not capable of open-domain\ndialogue: if they are faced with a conversational topic or genre that they have\nnever participated in, they would embarrass themselves with utterances that\nwould look incoherent and out of place to others already familiar with it.\n(think of a random person on the street trying to follow the conversations at\nsome coffee break at ACL). This is the fundamental problem I see with systems\nthat attempt to use data from an EXTREMELY DIVERSE, open-ended set of\nconversational genres (e.g. movie subtitles) in order to train one model,\nmushing everything together so that what emerges at the other end is just very\ngood grammatical structure. Or very generic responses. \n\nComments on the model architecture:\n\nRather than generate from a single encoded context, the authors induce a\ndistribution over possible contexts, sample f",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders \n\nHere is the abstract of the paper: While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder from word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that capture the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved through introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence of discourse-level decision-making. \n\nHere is the partial review: Review, ACL 2017, paper 256:\n\nThis paper extends the line of work which models generation in dialogue as a\nsequence to sequence generation problem, where the past N-1 utterances (the\n\u2018dialogue context\u2019) are encoded into a context vector (plus potential\nother, hand-crafted features), which is then decoded into a response: the Nth\nturn in the dialogue. As it stands, such models tend to suffer from lack of\ndiversity, specificity and local coherence in the kinds of response they tend\nto produce when trained over large dialogue datasets containing many topics\n(e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce\ndiverse responses using the decoder, e.g. through word-by-word beam search\n(which has been shown not to work very well, even lose crucial information\nabout grammar and valid sequences), or via a different objective function (such\nas in Li et. al.\u2019s work) the authors introduce a latent variable, z, over\nwhich a probability distribution is induced as part of the network. At\nprediction time, after encoding utterances 1 to k, a context z is sampled, and\nthe decoder is greedily used to generate a response from this. The evaluation\nshows small improvements in BLEU scores over a vanilla seq2seq model that does\nnot involve learning a probability distribution over contexts and sampling from\nthis.\n\nThe paper is certainly impressive from a technical point of view, i.e. in the\napplication of deep learning methods, specifically conditioned variational auto\nencoders, to the problem of response generation, and its attendant difficulties\nin training such models. Their use of Information-Retrieval techniques to get\nmore than one reference response is also interesting. \n\nI have some conceptual comments on the introduction and the motivations behind\nthe work, some on the model architecture, and the evaluation which I write\nbelow in turn:\n\nComments on the introduction and motivations\u2026. \n\nThe authors seem not fully aware of the long history of this field, and its\nvarious facets, whether from a theoretical perspective, or from an applied one.\n\n1. \u201c[the dialogue manager] typically takes a new utterance and the dialogue\ncontext as input, and generates discourse level decisions.\u201d \n\n        This is not accurate. Traditionally at least, the job of the dialogue\nmanager is to select actions (dialogue acts) in a particular dialogue context.\nThe                    action chosen is then passed to a separate generation\nmodule\nfor\nrealisation. Dialogue management is usually done in the context of task-based\nsystems which are goal driven. The dialogue manager is to choose actions which\nare optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few\nsteps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer\nand colleagues, and various publications from Steve Young, Milica Gasic and\ncolleagues for an overview of the large literature on Reinforcement Learning\nand MDP models for task-based dialogue systems.\n\n2. The authors need to make a clear distinction between task-based,\ngoal-oriented dialogue, and chatbots/social bots, the latter being usually no\nmore than a language model, albeit a sophisticated one (though see Wen et. al.\n2016). What is required from these two types of system is usually distinct.\nWhereas the former is required to complete a task, the latter is, perhaps only\nrequired to keep the user engaged. Indeed the data-driven methods that have\nbeen used to build such systems are usually very different. \n3. The authors refer to \u2018open-domain\u2019 conversation. I would suggest that\nthere is no such thing as open-domain conversation - conversation is always in\nthe context of some activity and for doing/achieving something specific in the\nworld. And it is this overarching goal, the overarching activity, this\noverarching genre, which determines the outward shape of dialogues and\ndetermines what sorts of dialogue structure are coherent. Coherence itself is\nactivity/context-specific. Indeed a human is not capable of open-domain\ndialogue: if they are faced with a conversational topic or genre that they have\nnever participated in, they would embarrass themselves with utterances that\nwould look incoherent and out of place to others already familiar with it.\n(think of a random person on the street trying to follow the conversations at\nsome coffee break at ACL). This is the fundamental problem I see with systems\nthat attempt to use data from an EXTREMELY DIVERSE, open-ended set of\nconversational genres (e.g. movie subtitles) in order to train one model,\nmushing everything together so that what emerges at the other end is just very\ngood grammatical structure. Or very generic responses. \n\nComments on the model architecture:\n\nRather than generate from a single encoded context, the authors induce a\ndistribution over possible contexts, sample f \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, I find the paper's approach to addressing the lack of diversity in neural encoder-decoder models for open-domain conversations to be innovative and technically impressive. The introduction and motivation, however, could benefit from a better understanding of the existing literature in the field of dialogue systems. The authors should make a clearer distinction between task-based, goal-oriented dialogue and chatbot/social bot systems, as the requirements and methods for building these systems differ. Additionally, the concept of 'open-domain conversation' should be reconsidered, as all conversations are inherently situated in a specific context or activity. Finally, I appreciate the authors' use of conditional variational autoencoders to capture discourse-level diversity and their integration of linguistic prior knowledge for better performance. The evaluation results show some improvements in BLEU scores compared to vanilla seq2seq models, but I would like to see a more comprehensive evaluation that considers metrics such as diversity, specificity, and local coherence. Overall, I believe this paper makes a valuable contribution to the field and addresses an important problem in neural dialog models.\"\n}",
        "human_end_index": 4850
    },
    {
        "id": "26",
        "source": "PeerRead/acl_2017/",
        "title": "An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge",
        "human_review": "26: An End-to-End Model for Question Answering over Knowledge Base with\nCross-Attention Combining Global Knowledge\n\nThis paper presents an approach for factoid question answering over a knowledge\ngraph (Freebase), by using a neural model that attempts to learn a semantic\ncorrelation/correspondence between various \"aspects\" of the candidate answer\n(e.g., answer type, relation to question entity, answer semantic, etc.) and a\nsubset of words of the question. A separate correspondence component is learned\nfor each \"aspect\" of the candidate answers. The two key contributions of this\nwork are: (1) the creation of separate components to capture different aspects\nof the candidate answer, rather than relying on a single semantic\nrepresentation, and (2) incorporating global context (from the KB) of the\ncandidate answers.\n\nThe most interesting aspect of this work, in my opinion, is the separation of\ncandidate answer representation into distinct aspects, which gives us (the\nneural model developer) a little more control over guiding the NN models\ntowards information that would be more beneficial in its decision making. It\nsort of harkens to the more traditional algorithms that rely on feature\nengineering. But in this case the \"feature engineering\" (i.e., aspects) is more\nsubtle, and less onerous. I encourage the authors to continue refining this\nsystem along these lines.\n\nWhile the high-level idea is fairly clear to a reasonably informed reader, the\ndevil in the details would make it hard for some audience to immediately grasp\nkey insights from this work. Some parts of the paper could benefit from more\nexplanation... Specifically:\n\n(1) Context aspect of candidate answers (e_c) is not clearly explained in the\npaper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n\n(2) Mention of OOV in the abstract and introduction need more explanation. As\nsuch, I think the current exposition in the paper assumes a deep understanding\nof prior work by the reader.\n\n(3) The experiments conducted in this paper restrict comparison to IR-based\nsystem -- and the reasoning behind this decision is reasonable. But it is not\nclear then why the work of Yang et al. (2014) -- which is described to be\nSP-based -- is part of the comparison. While, I am all for including more\nsystems in the comparison, there seem to be some inconsistencies in what should\nand should not be compared. Additionally, I see not harm in also mentioning the\ncomparable performance numbers for the best SP-based systems.\n\nI observe in the paper that the embeddings are learned entirely from the\ntraining data. I wonder how much impact the random initialization of these\nembeddings has on the end performance. It would be interesting to determine\n(and list) the variance if any. Additionally, if we were to start with\npre-trained embeddings (e.g., from word2vec) instead of the randomly\ninitialized ones, would that have any impact?\n\nAs I read the paper, one possible direction of future work that occurred to me\nwas to possibly include structured queries (from SP-based methods) as part of\nthe cross-attention mechanism. In other words, in addition to using the various\naspects of the candidate answers as features, one could include structured\nqueries that generate the produce that candidate answer as an additional aspect\nof the candidate answer. An attention mechanism could then also focus on\nvarious parts of the structured query, and its (semantic) matches to the input\nquestion as an additional signal for the NN model. Just a thought.\n\nSome notes regarding the positioning of the paper:\n\nI hesitate to call the model proposed here \"attention\" models, because (per my\nadmittedly limited understanding) attention mechanisms apply to\n\"encoder-decoder\" situations, where semantics expressed in one structured form\n(e.g., image, sentence in one language, natural language question, etc.) are\nencoded into an abstract representation, and then generated into another\nstructured form (e.g., caption, sentence in another language, structured query,\netc.). The attention mechanism allows the \"encoder\" to jump around and attend\nto different parts of the input (instead of sequentially) as the output is\nbeing generated by the decoder. This paper does not appear to fit this notion,\nand may be confusing to a broader audience.\n\n------\n\nThank you for clarifications in the author response.\n- Strengths:\nThis paper contributes to the field of knowledge base-based question answering\n(KB-QA), which is to tackle the problem of retrieving results from a structured\nKB based on a natural language question. KB-QA is an important and challenging\ntask.\n\nThe authors clearly identify the contributions and the novelty of their work,\nprovide a good overview of the previous work and performance comparison of\ntheir approach to the related methods.\n\nPrevious approaches to NN-based KB-QA represent questions and answers as fixed\nlength vectors, merely as a bag of words, which limits the expressiveness of\nthe models. And previous work also don\u2019t leverage unsupervised training over\nKG, which potentially can help a trained model to generalize.\nThis paper makes two major innovative points on the Question Answering problem.\n\n1) The backbone of the architecture of the proposed approach is a\ncross-attention based neural network, where attention is used for capture\ndifferent parts of questions and answer aspects. The cross-attention model\ncontains two parts, benefiting each other. The A-Q attention part tries to\ndynamically capture different aspects of the question, thus leading to\ndifferent embedding representations of the question. And the Q-A attention part\nalso offer different attention weight of the question towards the answer\naspects when computing their Q-A similarity score. \n2) Answer embeddings are not only learnt on the QA task but also modeled using\nTransE which allows to integrate more prior knowledge on the KB side. \nExperimental results are obtained on Web questions and the proposed approach\nexhibits better behavior than state-of-the-art end-to-end methods. The two\ncontributions were made particularly clear by ablation experiment. Both the\ncross-attention mechanism and global information improve QA performance by\nlarge margins.\n\nThe paper contains a lot of contents. The proposed framework is quite\nimpressive and novel compared with the previous works.\n\n- Weaknesses:\nThe paper is well-structured, the language is clear and correct. Some minor\ntypos are provided below.\n1. Page 5, column 1, line 421:                                       re-read               \n   \n\uf0e0 \nreread\n2. Page 5, column 2, line 454: pairs be  \uf0e0  pairs to be\n\n- General Discussion:\nIn Equation 2: the four aspects of candidate answer aspects share the same W\nand b. How about using separate W and b for each aspect? \nI would suggest considering giving a name to your approach instead of \"our\napproach\", something like ANN or CA-LSTM\u2026(yet something different from Table\n2).  \n\nIn general, I think it is a good idea to capture the different aspects for\nquestion answer similarity, and cross-attention based NN model is a novel\nsolution for the above task. The experimental results also demonstrate the\neffectiveness of the authors\u2019 approach. Although the overall performance is\nweaker than SP-based methods or some other integrated systems, I think this\npaper is a good attempt in end-to-end KB-QA area and should be encouraged.",
        "partial_review": "26: An End-to-End Model for Question Answering over Knowledge Base with\nCross-Attention Combining Global Knowledge\n\nThis paper presents an approach for factoid question answering over a knowledge\ngraph (Freebase), by using a neural model that attempts to learn a semantic\ncorrelation/correspondence between various \"aspects\" of the candidate answer\n(e.g., answer type, relation to question entity, answer semantic, etc.) and a\nsubset of words of the question. A separate correspondence component is learned\nfor each \"aspect\" of the candidate answers. The two key contributions of this\nwork are: (1) the creation of separate components to capture different aspects\nof the candidate answer, rather than relying on a single semantic\nrepresentation, and (2) incorporating global context (from the KB) of the\ncandidate answers.\n\nThe most interesting aspect of this work, in my opinion, is the separation of\ncandidate answer representation into distinct aspects, which gives us (the\nneural model developer) a little more control over guiding the NN models\ntowards information that would be more beneficial in its decision making. It\nsort of harkens to the more traditional algorithms that rely on feature\nengineering. But in this case the \"feature engineering\" (i.e., aspects) is more\nsubtle, and less onerous. I encourage the authors to continue refining this\nsystem along these lines.\n\nWhile the high-level idea is fairly clear to a reasonably informed reader, the\ndevil in the details would make it hard for some audience to immediately grasp\nkey insights from this work. Some parts of the paper could benefit from more\nexplanation... Specifically:\n\n(1) Context aspect of candidate answers (e_c) is not clearly explained in the\npaper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n\n(2) Mention of OOV in the abstract and introduction need more explanation. As\nsuch, I think the current exposition in the paper assumes a deep understanding\nof prior work by the reader.\n\n(3) The experiments conducted in this paper restrict comparison to IR-based\nsystem -- and the reasoning behind this decision is reasonable. But it is not\nclear then why the work of Yang et al. (2014) -- which is described to be\nSP-based -- is part of the comparison. While, I am all for including more\nsystems in the comparison, there seem to be some inconsistencies in what should\nand should not be compared. Additionally, I see not harm in also mentioning the\ncomparable performance numbers for the best SP-based systems.\n\nI observe in the paper that the embeddings are learned entirely from the\ntraining data. I wonder how much impact the random initialization of these\nembeddings has on the end performance. It would be interesting to determine\n(and list) the variance if any. Additionally, if we were to start with\npre-trained embeddings (e.g., from word2vec) instead of the randomly\ninitialized ones, would that have any impact?\n\nAs I read the paper, one possible direction of future work that occurred to me\nwas to possibly include structured queries (from SP-based methods) as part of\nthe cross-attention mechanism. In other words, in addition to using the various\naspects of the candidate answers as features, one could include structured\nqueries that generate the produce that candidate answer as an additional aspect\nof the candidate answer. An attention mechanism could then also focus on\nvarious parts of the structured query, and its (semantic) matches to the input\nquestion as an additional signal for the NN model. Just a thought.\n\nSome notes regarding the positioning of the paper:\n\nI hesitate to call the model proposed here \"attention\" models, because (per my\nadmittedly limited understanding) attention mechanisms apply to\n\"encoder-decoder\" situations, where semantics expressed in one structured form\n(e.g., image, sentence in one language",
        "task3_prompt": "\nComplete a partially written peer review about the paper: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge \n\nHere is the abstract of the paper: With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the  promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach. \n\nHere is the partial review: 26: An End-to-End Model for Question Answering over Knowledge Base with\nCross-Attention Combining Global Knowledge\n\nThis paper presents an approach for factoid question answering over a knowledge\ngraph (Freebase), by using a neural model that attempts to learn a semantic\ncorrelation/correspondence between various \"aspects\" of the candidate answer\n(e.g., answer type, relation to question entity, answer semantic, etc.) and a\nsubset of words of the question. A separate correspondence component is learned\nfor each \"aspect\" of the candidate answers. The two key contributions of this\nwork are: (1) the creation of separate components to capture different aspects\nof the candidate answer, rather than relying on a single semantic\nrepresentation, and (2) incorporating global context (from the KB) of the\ncandidate answers.\n\nThe most interesting aspect of this work, in my opinion, is the separation of\ncandidate answer representation into distinct aspects, which gives us (the\nneural model developer) a little more control over guiding the NN models\ntowards information that would be more beneficial in its decision making. It\nsort of harkens to the more traditional algorithms that rely on feature\nengineering. But in this case the \"feature engineering\" (i.e., aspects) is more\nsubtle, and less onerous. I encourage the authors to continue refining this\nsystem along these lines.\n\nWhile the high-level idea is fairly clear to a reasonably informed reader, the\ndevil in the details would make it hard for some audience to immediately grasp\nkey insights from this work. Some parts of the paper could benefit from more\nexplanation... Specifically:\n\n(1) Context aspect of candidate answers (e_c) is not clearly explained in the\npaper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n\n(2) Mention of OOV in the abstract and introduction need more explanation. As\nsuch, I think the current exposition in the paper assumes a deep understanding\nof prior work by the reader.\n\n(3) The experiments conducted in this paper restrict comparison to IR-based\nsystem -- and the reasoning behind this decision is reasonable. But it is not\nclear then why the work of Yang et al. (2014) -- which is described to be\nSP-based -- is part of the comparison. While, I am all for including more\nsystems in the comparison, there seem to be some inconsistencies in what should\nand should not be compared. Additionally, I see not harm in also mentioning the\ncomparable performance numbers for the best SP-based systems.\n\nI observe in the paper that the embeddings are learned entirely from the\ntraining data. I wonder how much impact the random initialization of these\nembeddings has on the end performance. It would be interesting to determine\n(and list) the variance if any. Additionally, if we were to start with\npre-trained embeddings (e.g., from word2vec) instead of the randomly\ninitialized ones, would that have any impact?\n\nAs I read the paper, one possible direction of future work that occurred to me\nwas to possibly include structured queries (from SP-based methods) as part of\nthe cross-attention mechanism. In other words, in addition to using the various\naspects of the candidate answers as features, one could include structured\nqueries that generate the produce that candidate answer as an additional aspect\nof the candidate answer. An attention mechanism could then also focus on\nvarious parts of the structured query, and its (semantic) matches to the input\nquestion as an additional signal for the NN model. Just a thought.\n\nSome notes regarding the positioning of the paper:\n\nI hesitate to call the model proposed here \"attention\" models, because (per my\nadmittedly limited understanding) attention mechanisms apply to\n\"encoder-decoder\" situations, where semantics expressed in one structured form\n(e.g., image, sentence in one language \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents an approach for factoid question answering over a knowledge graph (Freebase), by using a neural model that attempts to learn a semantic correlation/correspondence between various \\\"aspects\\\" of the candidate answer. The separation of candidate answer representation into distinct aspects is the most interesting aspect of this work, allowing for more control over guiding the neural models. However, some parts of the paper could benefit from more explanation, such as the context aspect of candidate answers and the mention of OOV. The comparison to IR-based systems is reasonable, but the inclusion of SP-based systems like Yang et al. (2014) seems inconsistent. The impact of random initialization of embeddings on end performance should be investigated. Additionally, future work could include structured queries as part of the cross-attention mechanism. Overall, the paper provides valuable contributions but could benefit from further explanation and clarification.\"\n}",
        "human_end_index": 3845
    },
    {
        "id": 266,
        "source": "PeerRead/acl_2017/",
        "title": "Improving sentiment classification with task-specific data",
        "human_review": "This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentration of subjective content can\nbe found. The latter type of data are also referred to as \"task-specific data\".\nMoreover, the authors compare different embeddings that combine information\nfrom \"task-specific\" corpora and generic corpora. A combination outperforms\nembeddings just drawn from a single corpus. This combination is not only\nevaluated on English but also on a less resourced language (i.e. Catalan).\n\n- Strengths:\nThe paper addresses an important aspect of sentiment analysis, namely how to\nappropriately induce embeddings for training supervised classifers for polarity\nclassification. The paper is well-structured and well-written. The major claims\nmade by the authors are sufficiently supported by their experiments.\n\n- Weaknesses:\nThe outcome of the experiments is very predictable. The methods that are\nemployed are very simple and ad-hoc. I found hardly any new idea in\nthat paper. Neither are there any significant lessons that the reader learns\nabout embeddings or sentiment analysis. The main idea (i.e. focusing on more\ntask-specific data for training more accurate embeddings) was already published\nin the context of named-entity recognition by Joshi et al. (2015). The\nadditions made in this paper are very incremental in nature.\n\nI find some of the experiments inconclusive as (apparently) no statistical\nsignficance testing between different classifiers has been carried out. In\nTables\n2, 3 and 6, various classifier configurations produce very similar scores. In\nsuch cases, only statistical signficance testing can really give a proper\nindication whether these difference are meaningful. For instance, in Table 3 on\nthe left half reporting results on RT, one may wonder whether there is a\nsignificant difference between \"Wikipedia Baseline\" and any of the\ncombinations. Furthermore, one doubts whether there is any signficant\ndifference between the different combinations (i.e. either using \"subj-Wiki\",\n\"subj-Multiun\" or \"subj-Europarl\") in that table.\nThe improvement by focusing on subjective subsets is plausible in general.\nHowever, I wonder whether in real life, in particular, a situation in which\nresources are sparse this is very helpful. Doing a pre-selection with\nOpinionFinder is some pre-processing step which will not be possible in most\nlanguages other than English. There are no equivalent tools or fine-grained\ndatasets on which such functionality could be learnt. The fact that in the\nexperiments\nfor Catalan, this information is not considered proves that. \n\nMinor details:\n\n- lines 329-334: The discussion of this dataset is confusing. I thought the\ntask is plain polarity classification but the authors here also refer to\n\"opinion holder\" and \"opinion targets\". If these information are not relevant\nto the experiments carried out in this paper, then they should not be mentioned\nhere.\n\n- lines 431-437: The variation of \"splicing\" that the authors explain is not\nvery well motivated. First, why do we need this? In how far should this be more\neffective than simple \"appending\"?\n\n- lines 521-522: How is the subjective information isolated for these\nconfigurations? I assume the authors here again employ OpinionFinder? However,\nthere is no explicit mention of this here.\n\n- lines 580-588: The definitions of variables do not properly match the\nformula (i.e. Equation 3). I do not find n_k in Equation 3.\n\n- lines 689-695: Similar to lines 329-334 it is unclear what precise task is\ncarried out. Do the authors take opinion holders and targets in consideration?\n\n***AFTER AUTHORS' RESPONSE***\nThank you very much for these clarifying remarks.\nI do not follow your explanations regarding the incorporation of opinion\nholders and targets, though.\n\nOverall, I will not change my scores since I think that this work lacks\nsufficient novelty (the things the authors raised in their response are just\ninsufficient to me). This submission is too incremental in nature.\n- Strengths: An interesting and comprehensive study of the effect of using\nspecial-domain corpora for training word embeddings.  Clear explanation of the\nassumptions, contributions, methodology, and results.  Thorough evaluation of\nvarious aspects of the proposal.\n\n- Weaknesses: Some conclusions are not fully backed up by the numerical\nresults.  E.g., the authors claim that for Catalan, the improvements of using\nspecific corpora for training word vectors is more pronounced than English.  I\nam not sure why this conclusion is made based on the results.  E.g., in Table\n6, none of the combination methods outperform the baseline for the\n300-dimension vectors.\n\n- General Discussion: The paper presents a set of simple, yet interesting\nexperiments that suggest word vectors (here trained using the skip-gram method)\nlargely benefit from the use of relevant (in-domain) and subjective corpora. \nThe paper answers important questions that are of benefit to practitioners of\nnatural language processing.  The paper is also very well-written, and very\nclearly organized.",
        "partial_review": "This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentration of subjective content can\nbe found. The latter type of data are also referred to as \"task-specific data\".\nMoreover, the authors compare different embeddings that combine information\nfrom \"task-specific\" corpora and generic corpora. A combination outperforms\nembeddings just drawn from a single corpus. This combination is not only\nevaluated on English but also on a less resourced language (i.e. Catalan).\n\n- Strengths:\nThe paper addresses an important aspect of sentiment analysis, namely how to\nappropriately induce embeddings for training supervised classifers for polarity\nclassification. The paper is well-structured and well-written. The major claims\nmade by the authors are sufficiently supported by their experiments.\n\n- Weaknesses:\nThe outcome of the experiments is very predictable. The methods that are\nemployed are very simple and ad-hoc. I found hardly any new idea in\nthat paper. Neither are there any significant lessons that the reader learns\nabout embeddings or sentiment analysis. The main idea (i.e. focusing on more\ntask-specific data for training more accurate embeddings) was already published\nin the context of named-entity recognition by Joshi et al. (2015). The\nadditions made in this paper are very incremental in nature.\n\nI find some of the experiments inconclusive as (apparently) no statistical\nsignficance testing between different classifiers has been carried out. In\nTables\n2, 3 and 6, various classifier configurations produce very similar scores. In\nsuch cases, only statistical signficance testing can really give a proper\nindication whether these difference are meaningful. For instance, in Table 3 on\nthe left half reporting results on RT, one may wonder whether there is a\nsignificant difference between \"Wikipedia Baseline\" and any of the\ncombinations. Furthermore, one doubts whether there is any signficant\ndifference between the different combinations (i.e. either using \"subj-Wiki\",\n\"subj-Multiun\" or \"subj-Europarl\") in that table.\nThe improvement by focusing on subjective subsets is plausible in general.\nHowever, I wonder whether in real life, in particular, a situation in which\nresources are sparse this is very helpful. Doing a pre-selection with\nOpinionFinder is some pre-processing step which will not be possible in most\n",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Improving sentiment classification with task-specific data \n\nHere is the abstract of the paper: Current state-of-the-art sentiment analysis techniques rely heavily on pre-trained word embeddings. However, the data used to train these embeddings normally comes from large, generic datasets, such as Wikipedia or GoogleNews, which may not include enough task-specific information to create reliable representations. This paper proposes a method to determine the subjectivity of a corpus using available tools and shows that word embeddings trained on task-specific corpora tend to outperform those trained on generic data. We then examine ways to combine information from generic and task-specific datasets and finally demonstrate that our method can work well for under-resourced languages. \n\nHere is the partial review: This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentration of subjective content can\nbe found. The latter type of data are also referred to as \"task-specific data\".\nMoreover, the authors compare different embeddings that combine information\nfrom \"task-specific\" corpora and generic corpora. A combination outperforms\nembeddings just drawn from a single corpus. This combination is not only\nevaluated on English but also on a less resourced language (i.e. Catalan).\n\n- Strengths:\nThe paper addresses an important aspect of sentiment analysis, namely how to\nappropriately induce embeddings for training supervised classifers for polarity\nclassification. The paper is well-structured and well-written. The major claims\nmade by the authors are sufficiently supported by their experiments.\n\n- Weaknesses:\nThe outcome of the experiments is very predictable. The methods that are\nemployed are very simple and ad-hoc. I found hardly any new idea in\nthat paper. Neither are there any significant lessons that the reader learns\nabout embeddings or sentiment analysis. The main idea (i.e. focusing on more\ntask-specific data for training more accurate embeddings) was already published\nin the context of named-entity recognition by Joshi et al. (2015). The\nadditions made in this paper are very incremental in nature.\n\nI find some of the experiments inconclusive as (apparently) no statistical\nsignficance testing between different classifiers has been carried out. In\nTables\n2, 3 and 6, various classifier configurations produce very similar scores. In\nsuch cases, only statistical signficance testing can really give a proper\nindication whether these difference are meaningful. For instance, in Table 3 on\nthe left half reporting results on RT, one may wonder whether there is a\nsignificant difference between \"Wikipedia Baseline\" and any of the\ncombinations. Furthermore, one doubts whether there is any signficant\ndifference between the different combinations (i.e. either using \"subj-Wiki\",\n\"subj-Multiun\" or \"subj-Europarl\") in that table.\nThe improvement by focusing on subjective subsets is plausible in general.\nHowever, I wonder whether in real life, in particular, a situation in which\nresources are sparse this is very helpful. Doing a pre-selection with\nOpinionFinder is some pre-processing step which will not be possible in most\n \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper compares different ways of inducing embeddings for the task of polarity classification. The authors focus on different types of corpora and find that not necessarily the largest corpus provides the most appropriate embeddings for their particular task but it is more effective to consider a corpus (or subcorpus) in which a higher concentration of subjective content can be found. The latter type of data are also referred to as 'task-specific data'. Moreover, the authors compare different embeddings that combine information from 'task-specific' corpora and generic corpora. A combination outperforms embeddings just drawn from a single corpus. This combination is not only evaluated on English but also on a less resourced language (i.e. Catalan).\\n\\nStrengths:\\nThe paper addresses an important aspect of sentiment analysis, namely how to appropriately induce embeddings for training supervised classifiers for polarity classification. The paper is well-structured and well-written. The major claims made by the authors are sufficiently supported by their experiments.\\n\\nWeaknesses:\\nThe outcome of the experiments is very predictable. The methods that are employed are very simple and ad-hoc. I found hardly any new idea in that paper. Neither are there any significant lessons that the reader learns about embeddings or sentiment analysis. The main idea (i.e. focusing on more task-specific data for training more accurate embeddings) was already published in the context of named-entity recognition by Joshi et al. (2015). The additions made in this paper are very incremental in nature.\\n\\nI find some of the experiments inconclusive as (apparently) no statistical significance testing between different classifiers has been carried out. In Tables 2, 3 and 6, various classifier configurations produce very similar scores. In such cases, only statistical significance testing can really give a proper indication whether these differences are meaningful. For instance, in Table 3 on the left half reporting results on RT, one may wonder whether there is a significant difference between 'Wikipedia Baseline' and any of the combinations. Furthermore, one doubts whether there is any significant difference between the different combinations (i.e. either using 'subj-Wiki', 'subj-Multiun' or 'subj-Europarl') in that table.\\n\\nThe improvement by focusing on subjective subsets is plausible in general. However, I wonder whether in real life, in particular, a situation in which resources are sparse this is very helpful. Doing a pre-selection with OpinionFinder is some preprocessing step which will not be possible in most cases.\"\n}",
        "human_end_index": 2630
    },
    {
        "id": "270",
        "source": "PeerRead/acl_2017/",
        "title": "Enhanced LSTM for Natural Language Inference",
        "human_review": "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the tree model does better still.\n\nThe paper is clear, the model is well motivated, and the results are\nimpressive. Everything in the paper is solidly incremental, but I nonetheless\nrecommend acceptance. \n\nMajor issues that I'd like discussed in the response:\n\u2013 You suggest several times that your system can serve as a new baseline for\nfuture work on NLI. This isn't an especially helpful or meaningful claim\u2014it\ncould be said of just about any model for any task. You could argue that your\nmodel is unusually simple or elegant, but I don't think that's really a major\nselling point of the model.\n\u2013 Your model architecture is symmetric in some ways that seem like\noverkill\u2014you compute attention across sentences in both directions, and run a\nseparate inference composition (aggregation) network for each direction. This\npresumably nearly doubles the run time of your model. Is this really necessary\nfor the very asymmetric task of NLI? Have you done ablation studies on this?**\n\u2013 You present results for the full sequential model (ESIM) and the ensemble\nof that model and the tree-based model (HIM). Why don't you present results for\nthe tree-based model on its own?**\n\nMinor issues:\n\u2013 I don't think the Barker and Jacobson quote means quite what you want it to\nmean. In context, it's making a specific and not-settled point about *direct*\ncompositionality in formal grammar. You'd probably be better off with a more\ngeneral claim about the widely accepted principle of compositionality.\n\u2013 The vector difference feature that you use (which has also appeared in\nprior work) is a bit odd, since it gives the model redundant parameters. Any\nmodel that takes vectors a, b, and (a - b) as input to some matrix\nmultiplication is exactly equivalent to some other model that takes in just a\nand b and has a different matrix parameter. There may be learning-related\nreasons why using this feature still makes sense, but it's worth commenting on.\n\u2013 How do you implement the tree-structured components of your model? Are\nthere major issues with speed or scalability there?\n\u001f\u2013 Typo: (Klein and D. Manning, 2003) \n\u2013 Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much\nmore readable parse trees without crossing lines. I'd suggest using them.\n\n---\n\nThanks for the response! I still solidly support publication. This work is not\ngroundbreaking, but it's novel in places, and the results are surprising enough\nto bring some value to the conference.\nThe paper proposes a model for the Stanford Natural Language Inference (SNLI)\ndataset, that builds on top of sentence encoding models and the decomposable\nword level alignment model by Parikh et al. (2016). The proposed improvements\ninclude performing decomposable attention on the output of a BiLSTM and feeding\nthe attention output to another BiLSTM, and augmenting this network with a\nparallel tree variant.\n\n- Strengths:\n\nThis approach outperforms several strong models previously proposed for the\ntask. The authors have tried a large number of experiments, and clearly report\nthe ones that did not work, and the hyperparameter settings of the ones that\ndid. This paper serves as a useful empirical study for a popular problem.\n\n- Weaknesses:\n\nUnfortunately, there are not many new ideas in this work that seem useful\nbeyond the scope the particular dataset used. While the authors claim that the\nproposed network architecture is simpler than many previous models, it is worth\nnoting that the model complexity (in terms of the number of parameters) is\nfairly high. Due to this reason, it would help to see if the empirical gains\nextend to other datasets as well. In terms of ablation studies, it would help\nto see 1) how well the tree-variant of the model does on its own and 2) the\neffect of removing inference composition from the model.\n\nOther minor issues:\n1) The method used to enhance local inference (equations 14 and 15) seem very\nsimilar to the heuristic matching function used by Mou et al., 2015 (Natural\nLanguage Inference by Tree-Based Convolution and Heuristic Matching). You may\nwant to cite them.\n\n2) The first sentence in section 3.2 is an unsupported claim. This either needs\na citation, or needs to be stated as a hypothesis.\n\nWhile the work is not very novel, the the empirical study is rigorous for the\nmost part, and could be useful for researchers working on similar problems.\nGiven these strengths, I am changing my recommendation score to 3. I have read\nthe authors' responses.",
        "partial_review": "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the tree model does better still.\n\nThe paper is clear, the model is well motivated, and the results are\nimpressive. Everything in the paper is solidly incremental, but I nonetheless\nrecommend acceptance. \n\nMajor issues that I'd like discussed in the response:\n\u2013 You suggest several times that your system can serve as a new baseline for\nfuture work on NLI. This isn't an especially helpful or meaningful claim\u2014it\ncould be said of just about any model for any task. You could argue that your\nmodel is unusually simple or elegant, but I don't think that's really a major\nselling point of the model.\n\u2013 Your model architecture is symmetric in some ways that seem like\noverkill\u2014you compute attention across sentences in both directions, and run a\nseparate inference composition (aggregation) network for each direction. This\npresumably nearly doubles the run time of your model. Is this really necessary\nfor the very asymmetric task of NLI? Have you done ablation studies on this?**\n\u2013 You present results for the full sequential model (ESIM) and the ensemble\nof that model and the tree-based model (HIM). Why don't you present results for\nthe tree-based model on its own?**\n\nMinor issues:\n\u2013 I don't think the Barker and Jacobson quote means quite what you want it to\nmean. In context, it's making a specific and not-settled point about *direct*\ncompositionality in formal grammar. You'd probably be better off with a more\ngeneral claim about the widely accepted principle of compositionality.\n\u2013 The vector difference feature that you use (which has also appeared in\nprior work) is a bit odd, since it gives the model redundant parameters. Any\nmodel that takes vectors a, b, and (a - b) as input to some matrix\nmultiplication is exactly equivalent to some other model that takes in just a\nand b and has a different matrix parameter. There may be learning-related\nreasons why using this feature still makes sense, but it's worth commenting on.\n\u2013 How do you implement the tree-structured components of your model? Are\nthere major issues with speed",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Enhanced LSTM for Natural Language Inference \n\nHere is the abstract of the paper: Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model. \n\nHere is the partial review: This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the tree model does better still.\n\nThe paper is clear, the model is well motivated, and the results are\nimpressive. Everything in the paper is solidly incremental, but I nonetheless\nrecommend acceptance. \n\nMajor issues that I'd like discussed in the response:\n\u2013 You suggest several times that your system can serve as a new baseline for\nfuture work on NLI. This isn't an especially helpful or meaningful claim\u2014it\ncould be said of just about any model for any task. You could argue that your\nmodel is unusually simple or elegant, but I don't think that's really a major\nselling point of the model.\n\u2013 Your model architecture is symmetric in some ways that seem like\noverkill\u2014you compute attention across sentences in both directions, and run a\nseparate inference composition (aggregation) network for each direction. This\npresumably nearly doubles the run time of your model. Is this really necessary\nfor the very asymmetric task of NLI? Have you done ablation studies on this?**\n\u2013 You present results for the full sequential model (ESIM) and the ensemble\nof that model and the tree-based model (HIM). Why don't you present results for\nthe tree-based model on its own?**\n\nMinor issues:\n\u2013 I don't think the Barker and Jacobson quote means quite what you want it to\nmean. In context, it's making a specific and not-settled point about *direct*\ncompositionality in formal grammar. You'd probably be better off with a more\ngeneral claim about the widely accepted principle of compositionality.\n\u2013 The vector difference feature that you use (which has also appeared in\nprior work) is a bit odd, since it gives the model redundant parameters. Any\nmodel that takes vectors a, b, and (a - b) as input to some matrix\nmultiplication is exactly equivalent to some other model that takes in just a\nand b and has a different matrix parameter. There may be learning-related\nreasons why using this feature still makes sense, but it's worth commenting on.\n\u2013 How do you implement the tree-structured components of your model? Are\nthere major issues with speed \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a purpose-built neural network architecture for textual entailment/NLI based on a three-step process of encoding, attention-based matching, and aggregation. The model has two variants, one based on TreeRNNs and the other based on sequential BiLSTMs. The sequential model outperforms all published results, and an ensemble with the tree model does better still. The paper is clear, the model is well motivated, and the results are impressive. Everything in the paper is solidly incremental, but I nonetheless recommend acceptance.\n\nMajor issues that I'd like discussed in the response:\n- You suggest several times that your system can serve as a new baseline for future work on NLI. This isn't an especially helpful or meaningful claim\u2014it could be said of just about any model for any task. You could argue that your model is unusually simple or elegant, but I don't think that's really a major selling point of the model.\n- Your model architecture is symmetric in some ways that seem like overkill\u2014you compute attention across sentences in both directions, and run a separate inference composition (aggregation) network for each direction. This presumably nearly doubles the runtime of your model. Is this really necessary for the very asymmetric task of NLI? Have you done ablation studies on this?\n- You present results for the full sequential model (ESIM) and the ensemble of that model and the tree-based model (HIM). Why don't you present results for the tree-based model on its own?\n\nMinor issues:\n- I don't think the Barker and Jacobson quote means quite what you want it to mean. In context, it's making a specific and not-settled point about *direct* compositionality in formal grammar. You'd probably be better off with a more general claim about the widely accepted principle of compositionality.\n- The vector difference feature that you use (which has also appeared in prior work) is a bit odd since it gives the model redundant parameters. Any model that takes vectors a, b, and (a - b) as input to some matrix multiplication is exactly equivalent to some other model that takes in just a and b and has a different matrix parameter. There may be learning-related reasons why using this feature still makes sense, but it's worth commenting on.\n- How do you implement the tree-structured components of your model? Are there major issues with speed?\"\n}",
        "human_end_index": 2386
    },
    {
        "id": "276",
        "source": "PeerRead/acl_2017/",
        "title": "Semi-supervised Multitask Learning for Sequence Labeling",
        "human_review": "The paper proposes an approach to sequence labeling with multitask learning,\nwhere language modeling is uses as the auxiliary objective. Thus, a\nbidirectional neural network architecture learns to predict the output labels\nas well as to predict the previous or next word in the sentence. The joint\nobjectives lead to improvements over the baselines in grammatical error\ndetection, chunking, NER, and POS tagging.\n\n- Strengths:\n\nThe contribution is quite well-written and easy to follow for the most part.\nThe model is exposed in sufficient detail, and the experiments are thorough\nwithin the defined framework. The benefits of introducing an auxiliary\nobjective are nicely exposed.\n\n- Weaknesses:\n\nThe paper shows very limited awareness of the related work, which is extensive\nacross the tasks that the experiments highlight. Tables 1-3 only show the three\nsystems proposed by the contribution (Baseline, +dropout, and +LMcost), while\nsome very limited comparisons are sketched textually.\n\nA contribution claiming novelty and advancements over the previous state of the\nart should document these improvements properly: at least by reporting the\nrelevant scores together with the novel ones, and ideally through replication.\nThe datasets used in the experiments are all freely available, the previous\nresults well-documented, and the previous systems are for the most part\npublicly available.\n\nIn my view, for a long paper, it is a big flaw not to treat the previous work\nmore carefully.\n\nIn that sense, I find this sentence particularly troublesome: \"The baseline\nresults are comparable to the previous best results on each of these\nbenchmarks.\" The reader is here led to believe that the baseline system somehow\nsubsumes all the previous contributions, which is shady on first read, and\nfactually incorrect after a quick lookup in related work.\n\nThe paper states \"new state-of-the-art results for error detection on both FCE\nand CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is\nnot straightforward to discern whether the\nlatter part of the claim does holds true, also as per Rei and Yannakoudakis'\n(2016) paper. The paper should support the claim by inclusion/replication of\nthe related work.\n\n- General Discussion:\n\nThe POS tagging is left as more of an afterthought. The comparison to Plank et\nal. (2016) is at least partly unfair as they test across multiple languages in\nthe Universal Dependencies realm, showing top-level performance across language\nfamilies, which I for one believe to be far more relevant than WSJ\nbenchmarking. How does the proposed system scale up/down to multiple languages,\nlow-resource languages with limited training data, etc.? The paper leaves a lot\nto ask for in that dimension to further substantiate its claims.\n\nI like the idea of including language modeling as an auxiliary task. I like the\narchitecture, and sections 1-4 in general. In my view, there is a big gap\nbetween those sections and the ones describing the experiments (5-8).\n\nI suggest that this nice idea should be further fleshed out before publication.\nThe rework should include at least a more fair treatment of related work, if\nnot replication, and at least a reflection on multilinguality. The data and the\nsystems are all there, as signs of the field's growing maturity. The paper\nshould in my view partake in reflecting this maturity, and not step away from\nit. In faith that these improvements can be implemented before the publication\ndeadline, I vote borderline.\n- Strengths: The article is well written; what was done is clear and\nstraightforward. Given how simple the contribution is, the gains are\nsubstantial, at least in the error correction task.\n\n- Weaknesses: The novelty is fairly limited (essentially, another permutation\nof tasks in multitask learning), and only one way of combining the tasks is\nexplored. E.g., it would have been interesting to see if pre-training is\nsignificantly worse than joint training; one could initialize the weights from\nan existing RNN LM trained on unlabeled data; etc.\n\n- General Discussion: I was hesitating between a 3 and a 4. While the\nexperiments are quite reasonable and the combinations of tasks sometimes new,\nthere's quite a bit of work on multitask learning in RNNs (much of it already\ncited), so it's hard to get excited about this work. I nevertheless recommend\nacceptance because the experimental results may be useful to others.\n\n- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the\npaper.",
        "partial_review": "The paper proposes an approach to sequence labeling with multitask learning,\nwhere language modeling is uses as the auxiliary objective. Thus, a\nbidirectional neural network architecture learns to predict the output labels\nas well as to predict the previous or next word in the sentence. The joint\nobjectives lead to improvements over the baselines in grammatical error\ndetection, chunking, NER, and POS tagging.\n\n- Strengths:\n\nThe contribution is quite well-written and easy to follow for the most part.\nThe model is exposed in sufficient detail, and the experiments are thorough\nwithin the defined framework. The benefits of introducing an auxiliary\nobjective are nicely exposed.\n\n- Weaknesses:\n\nThe paper shows very limited awareness of the related work, which is extensive\nacross the tasks that the experiments highlight. Tables 1-3 only show the three\nsystems proposed by the contribution (Baseline, +dropout, and +LMcost), while\nsome very limited comparisons are sketched textually.\n\nA contribution claiming novelty and advancements over the previous state of the\nart should document these improvements properly: at least by reporting the\nrelevant scores together with the novel ones, and ideally through replication.\nThe datasets used in the experiments are all freely available, the previous\nresults well-documented, and the previous systems are for the most part\npublicly available.\n\nIn my view, for a long paper, it is a big flaw not to treat the previous work\nmore carefully.\n\nIn that sense, I find this sentence particularly troublesome: \"The baseline\nresults are comparable to the previous best results on each of these\nbenchmarks.\" The reader is here led to believe that the baseline system somehow\nsubsumes all the previous contributions, which is shady on first read, and\nfactually incorrect after a quick lookup in related work.\n\nThe paper states \"new state-of-the-art results for error detection on both FCE\nand CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is\nnot straightforward to discern whether the\nlatter part of the claim does holds true, also as per Rei and Yannakoudakis'\n(2016) paper. The paper should support the claim by inclusion/replication of\nthe related work.\n\n- General Discussion:\n\nThe POS tagging is left as m",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Semi-supervised Multitask Learning for Sequence Labeling \n\nHere is the abstract of the paper: We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data. \n\nHere is the partial review: The paper proposes an approach to sequence labeling with multitask learning,\nwhere language modeling is uses as the auxiliary objective. Thus, a\nbidirectional neural network architecture learns to predict the output labels\nas well as to predict the previous or next word in the sentence. The joint\nobjectives lead to improvements over the baselines in grammatical error\ndetection, chunking, NER, and POS tagging.\n\n- Strengths:\n\nThe contribution is quite well-written and easy to follow for the most part.\nThe model is exposed in sufficient detail, and the experiments are thorough\nwithin the defined framework. The benefits of introducing an auxiliary\nobjective are nicely exposed.\n\n- Weaknesses:\n\nThe paper shows very limited awareness of the related work, which is extensive\nacross the tasks that the experiments highlight. Tables 1-3 only show the three\nsystems proposed by the contribution (Baseline, +dropout, and +LMcost), while\nsome very limited comparisons are sketched textually.\n\nA contribution claiming novelty and advancements over the previous state of the\nart should document these improvements properly: at least by reporting the\nrelevant scores together with the novel ones, and ideally through replication.\nThe datasets used in the experiments are all freely available, the previous\nresults well-documented, and the previous systems are for the most part\npublicly available.\n\nIn my view, for a long paper, it is a big flaw not to treat the previous work\nmore carefully.\n\nIn that sense, I find this sentence particularly troublesome: \"The baseline\nresults are comparable to the previous best results on each of these\nbenchmarks.\" The reader is here led to believe that the baseline system somehow\nsubsumes all the previous contributions, which is shady on first read, and\nfactually incorrect after a quick lookup in related work.\n\nThe paper states \"new state-of-the-art results for error detection on both FCE\nand CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is\nnot straightforward to discern whether the\nlatter part of the claim does holds true, also as per Rei and Yannakoudakis'\n(2016) paper. The paper should support the claim by inclusion/replication of\nthe related work.\n\n- General Discussion:\n\nThe POS tagging is left as m \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper proposes an approach to sequence labeling with multitask learning, where language modeling is used as the auxiliary objective. Thus, a bidirectional neural network architecture learns to predict the output labels as well as to predict the previous or next word in the sentence. The joint objectives lead to improvements over the baselines in grammatical error detection, chunking, NER, and POS tagging.\\n\\n- Strengths:\\n\\nThe contribution is quite well-written and easy to follow for the most part. The model is exposed in sufficient detail, and the experiments are thorough within the defined framework. The benefits of introducing an auxiliary objective are nicely exposed.\\n\\n- Weaknesses:\\n\\nThe paper shows very limited awareness of the related work, which is extensive across the tasks that the experiments highlight. Tables 1-3 only show the three systems proposed by the contribution (Baseline, +dropout, and +LMcost), while some very limited comparisons are sketched textually.\\n\\nA contribution claiming novelty and advancements over the previous state of the art should document these improvements properly: at least by reporting the relevant scores together with the novel ones, and ideally through replication. The datasets used in the experiments are all freely available, the previous results well-documented, and the previous systems are for the most part publicly available.\\n\\nIn my view, for a long paper, it is a big flaw not to treat the previous work more carefully.\\n\\nIn that sense, I find this sentence particularly troublesome: \\\"The baseline results are comparable to the previous best results on each of these benchmarks.\\\" The reader is here led to believe that the baseline system somehow subsumes all the previous contributions, which is shady on first read, and factually incorrect after a quick lookup in related work.\\n\\nThe paper states \\\"new state-of-the-art results for error detection on both FCE and CoNLL-14 datasets\\\". Looking into the CoNLL 2014 shared task report, it is not straightforward to discern whether the latter part of the claim does hold true, also as per Rei and Yannakoudakis' (2016) paper. The paper should support the claim by inclusion/replication of the related work.\\n\\n- General Discussion:\\n\\nThe POS tagging is left as m\"\n}",
        "human_end_index": 2271
    },
    {
        "id": 288,
        "source": "PeerRead/acl_2017/",
        "title": "The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task",
        "human_review": "The paper analyzes the story endings (last sentence of a 5-sentence story) in\nthe corpus built for the story cloze task (Mostafazadeh et al. 2016), and\nproposes a model based on character and word n-grams to classify story endings.\nThe paper also shows better performance on the story cloze task proper\n(distinguishing between \"right\" and \"wrong\" endings) than prior work.\n\nWhereas style analysis is an interesting area and you show better results than\nprior work on the story cloze task, there are several issues with the paper.\nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for\ninstance, your section\n\"Results\" actually mixes some results and new experiments) and clarified (see\nbelow for questions/comments): right now, it is quite difficult for the reader\nto follow what data is used for the different experiments, and what data the\ndiscussion refers to.\n\n(1) More details about the data used is necessary in order to assess the claim\nthat \"subtle writing task [...] imposes different styles on the author\" (lines\n729-732). How many stories are you looking at, written by how many different\npersons? And how many stories are there per person? From your description of\nthe post-analysis of coherence, only pairs of stories written by the same\nperson in which one was judged as \"coherent\" and the other one as \"neutral\" are\nchosen. Can you confirm that this is the case? So perhaps your claim is\njustified for your \"Experiment 1\". However my understanding is that in\nexperiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs.\n\"wrong\", we do not have the same writers. So I am not convinced lines 370-373\nare correct.\n\n(2) A lot in the paper is simply stated without any justifications. For\ninstance how are the \"five frequent\" POS and words chosen? Are they the most\nfrequent words/POS? (Also theses tables are puzzling: why two bars in the\nlegend for each category?). Why character *4*-grams? Did you tune that on the\ndevelopment set? If these were not the most frequent features, but some that\nyou chose among frequent POS and words, you need to justify this choice and\nespecially link the choice to \"style\". How are these features reflecting\n\"style\"?\n\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the\nrest of the paper, and to your results. But perhaps this is because I am lost\nin what \"training\" and \"test\" sets refer to here.\n\n(4) It is difficult to understand how your model differs from previous work.\nHow do we reconcile lines 217-219 (\"These results suggest that real\nunderstanding of text is required in order to solve the task\") with your\napproach?\n\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh\net al., but this is a very bad choice of terms. What exactly does a \"right\" or\n\"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")?\nI took a quick look, but couldn't find the exact prompts given to the Turkers.\nI think this needs to be clarified: as it is, the first paragraph of your\nsection \"Story cloze task\" (lines 159-177) is not understandable.\n\nOther questions/comments:\n\nTable 1. Why does the \"original\" story differ from the coherent and incoherent\none? From your description of the corpus, it seems that one Turker saw the\nfirst 4 sentences of the original story and was then ask to write one sentence\nending the story in a \"right\" way (or did they ask to provide a \"coherent\"\nending?) and one sentence ending the story in a \"wrong\" way (or did they ask to\nprovide an \"incoherent\" ending)? I don't find the last sentence of the\n\"incoherent\" story that incoherent... If the only shoes that Kathy finds great\nare $300, I can see how Kathy doesn't like buying shoes ;-) This led me to\nwonder how many Turkers judged the coherence of the story/ending and how\nvariable the judgements were. What criterion was used to judge a story coherent\nor incoherent? Also does one Turker judge the coherence of both the \"right\" and\n\"wrong\" endings, making it a relative judgement? Or was this an absolute\njudgement? This would have huge implications on the ratings.\n\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\n\nLine 398: \"Virtually all sentences\"? Can you quantify this?\n\nTable 5: Could we see the weights of the features? \n\nLine 614: \"compared to ending an existing task\": the Turkers are not ending a\n\"task\"\n\nLine 684-686: \"made sure each pair of endings was written by the same author\"\n-> this is true for the \"right\"/\"wrong\" pairs, but not for the \"original\"-\"new\"\npairs, according to your description.\n\nLine 694: \"shorter text spans\": text about what? This is unclear.\n\nLines 873-875: where is this published?\n- Strengths:\nThe paper has a promising topic (different writing styles in finishing a story)\nthat could appeal to Discourse and Pragmatics area participants.  \n\n- Weaknesses:\nThe paper suffers from a convincing and thorough discussion on writing style\nand implications of the experiments on discourse or pragmatics. \n(1) For example, regarding \"style\", the authors could have sought answers to\nthe following questions: what is the implication of starting an incoherent\nend-of-story sentence with a proper noun (l. 582)? Is this a sign of topic\nshift? What is the implication of ending a story coherently with a past tense\nverb, etc. \n(2) It is not clear to me why studies on deceptive language are similar to\nshort or long answers in the current study. I would have liked to see a more\ncomplete comparison here.\n(3) The use of terms such as \"cognitive load\" (l. 134) and \"mental states\" (l.\n671) appears somewhat vague. \n(4) There is insufficient discussion on the use of coordinators (line 275\nonwards); the paper would benefit from a more thorough discussion of this issue\n(e.g. what is the role of coordinators in these short stories and in discourse\nin general? Does the use of coordinators differ in terms of the genre of the\nstory? How about the use of \"no\" coordinators?)  \n(5) The authors do not seem to make it sufficiently clear who the target\nreaders of this research would be (e.g. language teachers? Crowd-sourcing\nexperiment designers? etc.) \n\nThe paper needs revision in terms of organization\n(there are repetitions throughout the text).  Also, the abbreviations in Table\n5 and 6 are not clear to me. \n\n- General Discussion:\nAll in all, the paper would have to be revised particularly in terms of its\ntheoretical standpoint and implications to discourse and pragmatics.\n\n=====\n\nIn their response to the reviewers' comments, the authors indicate their\nwillingness to update the paper and clarify the issues related to what they\nhave experimented with. However, I would have liked to see a stronger\ncommitment to incorporating the implications of this study to the Discourse and\nPragmatics area.\n- Strengths:\n\n*The paper is very well written\n*It shows how stylometric analysis can help in reasoning-like text\nclassification\n*The results have important implications for design on NLP datasets\n*The results may have important implications for many text classification tasks\n\n- Weaknesses:\n*I see few weaknesses in this paper. The only true one is the absence of a\ndefinition of style, which is a key concept in the paper\n\n- General Discussion:\nThis paper describes two experiments that explore the relationship between\nwriting task and writing style. In particular, controlling for vocabulary and\ntopic, the authors show that features used in authorship attribution/style\nanalysis can go a long way towards distinguishing between 1) a natural ending\nof a story 2) an ending added by a different author and 3) a purposefully\nincoherent ending added by a different author.\n\nThis is a great and fun paper to read and it definitely merits being accepted.\nThe paper is lucidly written and clearly explains what was done and why. The\nauthors use well-known simple features and a simple classifier to prove a\nnon-obvious hypothesis. Intuitively, it is obvious that a writing task greatly\nconstraints style. However, proven in such a clear manner, in such a controlled\nsetting, the findings are impressive.\n\nI particularly like Section 8 and the discussion about the implications on\ndesign of NLP tasks. I think this will be an influential and very well cited\npaper. Great work.  \n\nThe paper is a very good one as is. One minor suggestion I have is defining\nwhat the authors mean by \u201cstyle\u201d early on. The authors seem to mean \u201ca\nset of low-level easily computable lexical and syntactic features\u201d.  As is,\nthe usage is somewhat misleading for anyone outside of computational\nstylometrics. \n\nThe set of chosen stylistic features makes sense. However, were there no other\noptions? Were other features tried and they did not work? I think a short\ndiscussion of the choice of features would be informative.",
        "partial_review": "The paper analyzes the story endings (last sentence of a 5-sentence story) in\nthe corpus built for the story cloze task (Mostafazadeh et al. 2016), and\nproposes a model based on character and word n-grams to classify story endings.\nThe paper also shows better performance on the story cloze task proper\n(distinguishing between \"right\" and \"wrong\" endings) than prior work.\n\nWhereas style analysis is an interesting area and you show better results than\nprior work on the story cloze task, there are several issues with the paper.\nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for\ninstance, your section\n\"Results\" actually mixes some results and new experiments) and clarified (see\nbelow for questions/comments): right now, it is quite difficult for the reader\nto follow what data is used for the different experiments, and what data the\ndiscussion refers to.\n\n(1) More details about the data used is necessary in order to assess the claim\nthat \"subtle writing task [...] imposes different styles on the author\" (lines\n729-732). How many stories are you looking at, written by how many different\npersons? And how many stories are there per person? From your description of\nthe post-analysis of coherence, only pairs of stories written by the same\nperson in which one was judged as \"coherent\" and the other one as \"neutral\" are\nchosen. Can you confirm that this is the case? So perhaps your claim is\njustified for your \"Experiment 1\". However my understanding is that in\nexperiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs.\n\"wrong\", we do not have the same writers. So I am not convinced lines 370-373\nare correct.\n\n(2) A lot in the paper is simply stated without any justifications. For\ninstance how are the \"five frequent\" POS and words chosen? Are they the most\nfrequent words/POS? (Also theses tables are puzzling: why two bars in the\nlegend for each category?). Why character *4*-grams? Did you tune that on the\ndevelopment set? If these were not the most frequent features, but some that\nyou chose among frequent POS and words, you need to justify this choice and\nespecially link the choice to \"style\". How are these features reflecting\n\"style\"?\n\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the\nrest of the paper, and to your results. But perhaps this is because I am lost\nin what \"training\" and \"test\" sets refer to here.\n\n(4) It is difficult to understand how your model differs from previous work.\nHow do we reconcile lines 217-219 (\"These results suggest that real\nunderstanding of text is required in order to solve the task\") with your\napproach?\n\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh\net al., but this is a very bad choice of terms. What exactly does a \"right\" or\n\"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")?\nI took a quick look, but couldn't find the exact prompts given to the Turkers.\nI think this needs to be clarified: as it is, the first paragraph of your\nsection \"Story cloze task\" (lines 159-177) is not understandable.\n\nOther questions/comments:\n\nTable 1. Why does the \"original\" story differ from the coherent and incoherent\none? From your description of the corpus, it seems that one Turker saw the\nfirst 4 sentences of the original story and was then ask to write one sentence\nending the story in a \"right\" way (or did they ask to provide a \"coherent\"\nending?) and one sentence ending the story in a \"wrong\" way (or did they ask to\nprovide an \"incoherent\" ending)? I don't find the last sentence of the\n\"incoherent\" story that incoherent... If the only shoes that Kathy finds great\nare $300, I can see how Kathy doesn't like buying shoes ;-) This led me to\nwonder how many Turkers judged the coherence of the story/ending and how\nvariable the judgements were. What criterion was used to judge a story coherent\nor incoherent? Also does one Turker judge the coherence of both the \"right\" and\n\"wrong\" endings, making it a relative judgement? Or was this an absolute\njudgement? This would have huge implications on the ratings.\n\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\n\nLine 398: \"Virtually all sentences\"? Can you quantify this?\n\nTable 5: Could we see the weights of the features? \n\nLine 614: \"compared to ending an existing task\": the Turkers are not ending a\n\"task\"\n\nLine 684-686: \"made sure each pair of endings was written by the same author\"\n-> this is true for the \"right\"/\"wrong\" pairs, but not for th",
        "task3_prompt": "\nComplete a partially written peer review about the paper: The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task \n\nHere is the abstract of the paper: A writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the  story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed with stylistic features is able to successfully distinguish between the three cases, without even looking at the story context. In addition, our style-based classifier establishes a new state-of-the-art result on the story cloze challenge, substantially higher than previous results based on deep learning models. Our results demonstrate that different task framings can dramatically affect the way people write. \n\nHere is the partial review: The paper analyzes the story endings (last sentence of a 5-sentence story) in\nthe corpus built for the story cloze task (Mostafazadeh et al. 2016), and\nproposes a model based on character and word n-grams to classify story endings.\nThe paper also shows better performance on the story cloze task proper\n(distinguishing between \"right\" and \"wrong\" endings) than prior work.\n\nWhereas style analysis is an interesting area and you show better results than\nprior work on the story cloze task, there are several issues with the paper.\nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for\ninstance, your section\n\"Results\" actually mixes some results and new experiments) and clarified (see\nbelow for questions/comments): right now, it is quite difficult for the reader\nto follow what data is used for the different experiments, and what data the\ndiscussion refers to.\n\n(1) More details about the data used is necessary in order to assess the claim\nthat \"subtle writing task [...] imposes different styles on the author\" (lines\n729-732). How many stories are you looking at, written by how many different\npersons? And how many stories are there per person? From your description of\nthe post-analysis of coherence, only pairs of stories written by the same\nperson in which one was judged as \"coherent\" and the other one as \"neutral\" are\nchosen. Can you confirm that this is the case? So perhaps your claim is\njustified for your \"Experiment 1\". However my understanding is that in\nexperiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs.\n\"wrong\", we do not have the same writers. So I am not convinced lines 370-373\nare correct.\n\n(2) A lot in the paper is simply stated without any justifications. For\ninstance how are the \"five frequent\" POS and words chosen? Are they the most\nfrequent words/POS? (Also theses tables are puzzling: why two bars in the\nlegend for each category?). Why character *4*-grams? Did you tune that on the\ndevelopment set? If these were not the most frequent features, but some that\nyou chose among frequent POS and words, you need to justify this choice and\nespecially link the choice to \"style\". How are these features reflecting\n\"style\"?\n\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the\nrest of the paper, and to your results. But perhaps this is because I am lost\nin what \"training\" and \"test\" sets refer to here.\n\n(4) It is difficult to understand how your model differs from previous work.\nHow do we reconcile lines 217-219 (\"These results suggest that real\nunderstanding of text is required in order to solve the task\") with your\napproach?\n\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh\net al., but this is a very bad choice of terms. What exactly does a \"right\" or\n\"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")?\nI took a quick look, but couldn't find the exact prompts given to the Turkers.\nI think this needs to be clarified: as it is, the first paragraph of your\nsection \"Story cloze task\" (lines 159-177) is not understandable.\n\nOther questions/comments:\n\nTable 1. Why does the \"original\" story differ from the coherent and incoherent\none? From your description of the corpus, it seems that one Turker saw the\nfirst 4 sentences of the original story and was then ask to write one sentence\nending the story in a \"right\" way (or did they ask to provide a \"coherent\"\nending?) and one sentence ending the story in a \"wrong\" way (or did they ask to\nprovide an \"incoherent\" ending)? I don't find the last sentence of the\n\"incoherent\" story that incoherent... If the only shoes that Kathy finds great\nare $300, I can see how Kathy doesn't like buying shoes ;-) This led me to\nwonder how many Turkers judged the coherence of the story/ending and how\nvariable the judgements were. What criterion was used to judge a story coherent\nor incoherent? Also does one Turker judge the coherence of both the \"right\" and\n\"wrong\" endings, making it a relative judgement? Or was this an absolute\njudgement? This would have huge implications on the ratings.\n\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\n\nLine 398: \"Virtually all sentences\"? Can you quantify this?\n\nTable 5: Could we see the weights of the features? \n\nLine 614: \"compared to ending an existing task\": the Turkers are not ending a\n\"task\"\n\nLine 684-686: \"made sure each pair of endings was written by the same author\"\n-> this is true for the \"right\"/\"wrong\" pairs, but not for th \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper analyzes the story endings (last sentence of a 5-sentence story) in the corpus built for the story cloze task (Mostafazadeh et al. 2016), and proposes a model based on character and word n-grams to classify story endings. The paper also shows better performance on the story cloze task proper (distinguishing between 'right' and 'wrong' endings) than prior work. However, there are several issues with the paper. Firstly, the definition of 'style' is not clear and needs to be explicitly stated. The paper also needs to be restructured and clarified to make it easier for the reader to follow the data used for different experiments. More details about the data used are necessary to fully assess the claim that different writing tasks impose different styles on the author. The choice of features and the justification for selecting them should be provided, especially in relation to how they reflect 'style'. The section on the 'Design of NLP tasks' needs to be better connected to the rest of the paper and to the results. It is also unclear how the model in this paper differs from previous work. The terminology of 'right' and 'wrong' endings needs clarification and the prompts given to the Turkers should be included. Additionally, further details about the corpus and the criteria used to judge coherence are needed. Lastly, some statements and figures need clarification and quantification. Overall, the paper has potential, but these issues need to be addressed and clarified to improve the clarity and validity of the findings.\"\n}",
        "human_end_index": 4509
    },
    {
        "id": 31,
        "source": "PeerRead/acl_2017/",
        "title": "Event Factuality Identification via Deep Neural Networks",
        "human_review": "Update after author response: \n\n1. My major concern about the optimization of model's hyperparameter (which are\nnumerous) has not been addressed. This is very important, considering that you\nreport results from folded cross-validation. \n\n2. The explanation that benefits of their method are experimentally confirmed\nwith 2% difference -- while evaluating via 5-fold CV on 200 examples -- is\nquite unconvincing.\n\n========================================================================\n\nSummary:\n\nIn this paper authors present a complex neural model for detecting factuality\nof event mentions in text. The authors combine the following in their complex\nmodel:                          (1) a set of traditional classifiers for detecting\nevent\nmentions,\nfactuality sources, and source introducing predicates (SIPs), (2) A\nbidirectional attention-based LSTM model that learns latent representations for\nelements on different dependency paths used as input, (2) A CNN that uses\nrepresentations from the LSTM and performs two output predictions (one to\ndetect specific from underspecified cases and another to predict the actual\nfactuality class). \n\nFrom the methodological point of view, the authors are combining a reasonably\nfamiliar methods (att-BiLSTM and CNN) into a fairly complex model. However,\nthis model does not take raw text (sequence of word embeddings) as input, but\nrather hand-crafted features (e.g., different dependency paths combining\nfactuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted\nfeatures is somewhat surprising if coupled with complex deep model. The\nevaluation seems a bit tainted as the authors report the results from folded\ncross-validation but do not report how they optimized the hyperparameters of\nthe model. Finally, the results are not too convincing -- considering the\ncomplexity of the model and the amount of preprocessing required (extraction of\nevent mentions, SIPs, and clues), a 2% macro-average gain over the rule-based\nbaseline and overall 44% performance seems modest, at best (looking at\nMicro-average, the proposed model doesn't outperform simple MaxEnt classifier).\n\nThe paper is generally well-written and fairly easy to understand. Altogether,\nI find this paper to be informative to an extent, but in it's current form not\na great read for a top-tier conference.   \n\nRemarks:\n\n1. You keep mentioning that the LSTM and CNN in your model are combined\n\"properly\" -- what does that actually mean? How does this \"properness\"\nmanifest? What would be the improper way to combine the models?\n\n2. I find the motivation/justification for the two output design rather weak: \n    - the first argument that it allows for later addition of cues (i.e\nmanually-designed features) kind of beats the \"learning representations\"\nadvantage of using deep models. \n        - the second argument about this design tackling the imbalance in the\ntraining set is kind of hand-wavy as there is no experimental support for this\nclaim. \n\n3. You first motivate the usage of your complex DL architecture with learning\nlatent representations and avoiding manual design and feature computation.  And\nthen you define a set of manually designed features (several dependency paths\nand lexical features) as input for the model. Do you notice the discrepancy? \n\n4. The LSTMs (bidirectional, and also with attention) have by now already\nbecome a standard model for various NLP tasks. Thus I find the detailed\ndescription of the attention-based bidirectional LSTM unnecessary. \n5. What you present as a baseline in Section 3 is also part of your model (as\nit generates input to your model). Thus, I think that calling it a baseline\nundermines the understandability of the paper. \n\n6. The results reported originate from a 5-fold CV. However, the model contains\nnumerous hyperparameters that need to be optimized (e.g., number of filters and\nfilter sizes for CNNs). How do you optimize these values? Reporting results\nfrom a folded cross-validation doesn't allow for a fair optimization of the\nhypeparameters: either you're not optimizing the model's hyperparameters at\nall, or you're optimizing their values on the test set (which is unfair). \n\n7. \"Notice that some values are non-application (NA) grammatically, e.g., PRu,\nPSu, U+/-\" -- why is underspecification in ony one dimension (polarity or\ncertainty) not an option? I can easily think of a case where it is clear the\nevent is negative, but it is not specified whether the absence of an event is\ncertain, probable, or possible. \n\nLanguage & style:\n\n1. \"to a great degree\" -> \"great degree\" is an unusual construct, use either\n\"great extent\" or \"large degree\"\n2. \"events that can not\" -> \"cannot\" or \"do not\"\n3. \"describes out networks...in details shown in Figure 3.\" -> \"...shown in\nFigure 3 in details.\"\nComments after author response\n\n- Thank you for clarifying that the unclear \"two-step framework\" reference was\nnot about the two facets. I still do not find this use of a pipeline to be a\nparticularly interesting contribution.\n- You state that \"5. de Marneffe (2012) used additional annotated features in\ntheir system. For fair comparison, we re-implement their system with annotated\ninformation in FactBank.\" But the de Marneffe et al. feature cited in the\npaper, \"Predicate Classes\" requires only a dependency parser and vocabulary\nlists from Roser Saur\u00ed's PhD thesis; \"general classes of event\" might be\nreferring to FactML event classes, and while I admit it is not particularly\nclear in their work, I am sure they could clarify.\n- I continue to find the use of \"combined properly\" to be obscure. I agree that\nusing LSTM and CNN where respectively appropriate is valuable, but you seem to\nimply that some prior work has been improper, and that it is their combination\nwhich must be proper.\n- Thank you for reporting on separate LSTMs for each of the paths. I am curious\nas to why this combination may less effective. In any case, experiments with\nthis kind of alternative structure deserve to be reported.\n\n---\n\nThis paper introduces deep neural net technologies to the task of factuality\nclassification as defined by FactBank, with performance exceeding alternative\nneural net models and baselines reimplemented from the literature.\n\n- Strengths:\n\nThis paper is very clear in its presentation of a sophisticated model for\nfactuality classification and of its evaluation.  It shows that the use of\nattentional features and BiLSTM clearly provide benefit over alternative\npooling strategies, and that the model also exceeds the performance of a more\ntraditional feature-based log-linear model.  Given the small amount of training\ndata in FactBank, this kind of highly-engineered model seems appropriate. It is\ninteresting to see that the BiLSTM/CNN model is able to provide benefit despite\nlittle training data.\n\n- Weaknesses:\n\nMy main concerns with this work regard its (a) apparent departure from the\nevaluation procedure in the prior literature; (b) failure to present prior work\nas a strong baseline; and (c) novelty.\n\nWhile I feel that the work is original in engineering deep neural nets for the\nfactuality classification task, and that such work is valuable, its approach is\nnot particularly novel, and \"the proposal of a two-step supervised framework\"\n(line 087) is not particularly interesting given that FactBank was always\ndescribed in terms of two facets (assuming I am correct to interpret \"two-step\"\nas referring to these facets, which I may not be).\n\nThe work cites Saur\u00ed and Pustejovsky (2012), but presents their much earlier\n(2008) and weaker system as a baseline; nor does it consider Qian et al.'s\n(IALP 2015) work which compares to the former.              Both these works are\ndeveloped\non the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT\nTimeBank section, while the present work does not report results on a held-out\nset.\n\nde Marneffe et al.'s (2012) system is also chosen as a baseline, but not all\ntheir features are implemented, nor is the present system evaluated on their\nPragBank corpus (or other alternative representations of factuality proposed in\nPrabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is\ntherefore somewhat lacking in comparability to prior work.\n\nThere were also important questions left unanswered in evaluation, such as the\neffect of using gold standard events or SIPs.\n\nGiven the famed success of BiLSTMs with little feature engineering, it is\nsomewhat disappointing that this work does not attempt to consider a more\nminimal system employing deep neural nets on this task with, for instance, only\nthe dependency path from a candidate event to its SIP plus a bag of modifiers\nto that path. The inclusion of heterogeneous information in one BiLSTM was an\ninteresting feature, which deserved more experimentation: what if the order of\ninputs were permuted? what if delimiters were used in concatenating the\ndependency paths in RS instead of the strange second \"nsubj\" in the RS chain of\nline 456? What if each of SIP_path, RS_path, Cue_path were input to a separate\nLSTM and combined? The attentional features were evaluated together for the CNN\nand BiLSTM components, but it might be worth reporting whether it was\nbeneficial for each of these components. Could you benefit from providing path\ninformation for the aux words? Could you benefit from character-level\nembeddings to account for morphology's impact on factuality via tense/aspect?\nProposed future work is lacking in specificity seeing as there are many\nquestions raised by this model and a number of related tasks to consider\napplying it to.\n\n- General Discussion:\n\n194: Into what classes are you classifying events?\n\n280: Please state which are parameters of the model.\n\n321: What do you mean by \"properly\"? You use the same term in 092 and it's not\nclear which work you consider improper nor why.\n\n353: Is \"the chain form\" defined anywhere? Citation? The repetition of nsubj in\nthe example of line 456 seems an unusual feature for the LSTM to learn.\n\n356: It may be worth footnoting here that each cue is classified separately.\n\n359: \"distance\" -> \"surface distance\"\n\n514: How many SIPs? Cues? Perhaps add to Table 3.\n\nTable 2. Would be good if augmented by the counts for embedded and author\nevents. Percentages can be removed if necessary.\n\n532: Why 5-fold? Given the small amount of training data, surely 10-fold would\nbe more useful and not substantially increase training costs.\n\n594: It's not clear that this benefit comes from PSen, nor that the increase is\nsignificant or substantial.  Does it affect overall results substantially?\n\n674: Is this significance across all metrics?\n\n683: Is the drop of F1 due to precision, recall or both?\n\n686: Not clear what this sentence is trying to say.\n\nTable 4: From the corpus sizes, it seems you should only report 2 significant\nfigures for most columns (except CT+, Uu and Micro-A).\n\n711: It seems unsurprising that RS_path is insufficient given that the task is\nwith respect to a SIP and other inputs do not encode that information. It would\nbe more interesting to see performance of SIP_path alone.\n\n761: This claim is not precise, to my understanding. de Marneffe et al (2012)\nevaluates on PragBank, not FactBank.\n\nMinor issues in English usage:\n\n112: \"non-application\" -> \"not applicable\"\n\n145: I think you mean \"relevant\" -> \"relative\"\n\n154: \"can be displayed by a simple source\" is unclear\n\n166: Not sure what you mean by \"basline\". Do you mean \"pipeline\"?\nThis paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms\nstate-of-the-art systems on the FactBank corpus, particularly in three classes\n(CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an\nattention-based two-step deep neural model for event factuality identification\nusing bidirectional long short-term memory (BiLSTM) and convolutional neural\nnetwork (CNN).\n\n[Strengths:]\n\n- The structure of the paper is (not perfectly but) well organized.\n\n- The empirical results show convincing (statistically significant) performance\ngains of the proposed model over strong baseline.\n\n[Weaknesses:]\n\nSee below for details of the following weaknesses:\n\n- Novelties of the paper are relatively unclear.\n\n- No detailed error analysis is provided.\n\n- A feature comparison with prior work is shallow, missing two relevant papers.\n\n- The paper has several obscure descriptions, including typos.\n\n[General Discussion:]\n\nThe paper would be more impactful if it states novelties more explicitly.  Is\nthe paper presenting the first neural network based approach for event\nfactuality identification?  If this is the case, please state that.\n\nThe paper would crystallize remaining challenges in event factuality\nidentification and facilitate future research better if it provides detailed\nerror analysis regarding the results of Table 3 and 4.              What are dominant\nsources of errors made by the best system BiLSTM+CNN(Att)?  What impacts do\nerrors in basic factor extraction (Table 3) have on the overall performance of\nfactuality identification (Table 4)?  The analysis presented in Section 5.4 is\nmore like a feature ablation study to show how useful some additional features\nare.\n\nThe paper would be stronger if it compares with prior work in terms of\nfeatures.  Does the paper use any new features which have not been explored\nbefore?  In other words, it is unclear whether main advantages of the proposed\nsystem come purely from deep learning, or from a combination of neural networks\nand some new unexplored features.  As for feature comparison, the paper is\nmissing two relevant papers:\n\n- Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection\nand Factuality Assessment with Non-Expert Supervision. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing, pages\n1643-1648.\n\n- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.\nModeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics, pages 415-420.\n\nThe paper would be more understandable if more examples are given to illustrate\nthe underspecified modality (U) and the underspecified polarity (u).  There are\ntwo reasons for that.  First, the definition of 'underspecified' is relatively\nunintuitive as compared to other classes such as 'probable' or 'positive'. \nSecond, the examples would be more helpful to understand the difficulties of Uu\ndetection reported in line 690-697.  Among the seven examples (S1-S7), only S7\ncorresponds to Uu, and its explanation is quite limited to illustrate the\ndifficulties.\n\nA minor comment is that the paper has several obscure descriptions, including\ntypos, as shown below:\n\n- The explanations for features in Section 3.2 are somewhat intertwined and\nthus confusing.  The section would be more coherently organized with more\nseparate paragraphs dedicated to each of lexical features and sentence-level\nfeatures, by:\n\n  - (1) stating that the SIP feature comprises two features (i.e.,\nlexical-level\nand sentence-level) and introduce their corresponding variables (l and c) *at\nthe beginning*;\n\n  - (2) moving the description of embeddings of the lexical feature in line\n280-283\nto the first paragraph; and\n\n  - (3) presenting the last paragraph about relevant source identification in a\nseparate subsection because it is not about SIP detection.\n\n- The title of Section 3 ('Baseline') is misleading.  A more understandable\ntitle would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because\nthe section is about how to extract basic factors (features), not about a\nbaseline end-to-end system for event factuality identification.\n\n- The presented neural network architectures would be more convincing if it\ndescribes how beneficial the attention mechanism is to the task.\n\n- Table 2 seems to show factuality statistics only for all sources.  The table\nwould be more informative along with Table 4 if it also shows factuality\nstatistics for 'Author' and 'Embed'.\n\n- Table 4 would be more effective if the highest system performance with\nrespect to each combination of the source and the factuality value is shown in\nboldface.\n\n- Section 4.1 says, \"Aux_Words can describe the *syntactic* structures of\nsentences,\" whereas section 5.4 says, \"they (auxiliary words) can reflect the\n*pragmatic* structures of sentences.\"  These two claims do not consort with\neach other well, and neither of them seems adequate to summarize how useful the\ndependency relations 'aux' and 'mark' are for the task.\n\n- S7 seems to be another example to support the effectiveness of auxiliary\nwords, but the explanation for S7 is thin, as compared to the one for S6.  What\nis the auxiliary word for 'ensure' in S7?\n\n- Line 162: 'event go in S1' should be 'event go in S2'.\n\n- Line 315: 'in details' should be 'in detail'.\n\n- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.\n\n- Line 771: 'recent researches' should be 'recent research' or 'recent\nstudies'.  'Research' is an uncountable noun.\n\n- Line 903: 'Factbank' should be 'FactBank'.",
        "partial_review": "Update after author response: \n\n1. My major concern about the optimization of model's hyperparameter (which are\nnumerous) has not been addressed. This is very important, considering that you\nreport results from folded cross-validation. \n\n2. The explanation that benefits of their method are experimentally confirmed\nwith 2% difference -- while evaluating via 5-fold CV on 200 examples -- is\nquite unconvincing.\n\n========================================================================\n\nSummary:\n\nIn this paper authors present a complex neural model for detecting factuality\nof event mentions in text. The authors combine the following in their complex\nmodel:                          (1) a set of traditional classifiers for detecting\nevent\nmentions,\nfactuality sources, and source introducing predicates (SIPs), (2) A\nbidirectional attention-based LSTM model that learns latent representations for\nelements on different dependency paths used as input, (2) A CNN that uses\nrepresentations from the LSTM and performs two output predictions (one to\ndetect specific from underspecified cases and another to predict the actual\nfactuality class). \n\nFrom the methodological point of view, the authors are combining a reasonably\nfamiliar methods (att-BiLSTM and CNN) into a fairly complex model. However,\nthis model does not take raw text (sequence of word embeddings) as input, but\nrather hand-crafted features (e.g., different dependency paths combining\nfactuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted\nfeatures is somewhat surprising if coupled with complex deep model. The\nevaluation seems a bit tainted as the authors report the results from folded\ncross-validation but do not report how they optimized the hyperparameters of\nthe model. Finally, the results are not too convincing -- considering the\ncomplexity of the model and the amount of preprocessing required (extraction of\nevent mentions, SIPs, and clues), a 2% macro-average gain over the rule-based\nbaseline and overall 44% performance seems modest, at best (looking at\nMicro-average, the proposed model doesn't outperform simple MaxEnt classifier).\n\nThe paper is generally well-written and fairly easy to understand. Altogether,\nI find this paper to be informative to an extent, but in it's current form not\na great read for a top-tier conference.   \n\nRemarks:\n\n1. You keep mentioning that the LSTM and CNN in your model are combined\n\"properly\" -- what does that actually mean? How does this \"properness\"\nmanifest? What would be the improper way to combine the models?\n\n2. I find the motivation/justification for the two output design rather weak: \n    - the first argument that it allows for later addition of cues (i.e\nmanually-designed features) kind of beats the \"learning representations\"\nadvantage of using deep models. \n        - the second argument about this design tackling the imbalance in the\ntraining set is kind of hand-wavy as there is no experimental support for this\nclaim. \n\n3. You first motivate the usage of your complex DL architecture with learning\nlatent representations and avoiding manual design and feature computation.  And\nthen you define a set of manually designed features (several dependency paths\nand lexical features) as input for the model. Do you notice the discrepancy? \n\n4. The LSTMs (bidirectional, and also with attention) have by now already\nbecome a standard model for various NLP tasks. Thus I find the detailed\ndescription of the attention-based bidirectional LSTM unnecessary. \n5. What you present as a baseline in Section 3 is also part of your model (as\nit generates input to your model). Thus, I think that calling it a baseline\nundermines the understandability of the paper. \n\n6. The results reported originate from a 5-fold CV. However, the model contains\nnumerous hyperparameters that need to be optimized (e.g., number of filters and\nfilter sizes for CNNs). How do you optimize these values? Reporting results\nfrom a folded cross-validation doesn't allow for a fair optimization of the\nhypeparameters: either you're not optimizing the model's hyperparameters at\nall, or you're optimizing their values on the test set (which is unfair). \n\n7. \"Notice that some values are non-application (NA) grammatically, e.g., PRu,\nPSu, U+/-\" -- why is underspecification in ony one dimension (polarity or\ncertainty) not an option? I can easily think of a case where it is clear the\nevent is negative, but it is not specified whether the absence of an event is\ncertain, probable, or possible. \n\nLanguage & style:\n\n1. \"to a great degree\" -> \"great degree\" is an unusual construct, use either\n\"great extent\" or \"large degree\"\n2. \"events that can not\" -> \"cannot\" or \"do not\"\n3. \"describes out networks...in details shown in Figure 3.\" -> \"...shown in\nFigure 3 in details.\"\nComments after author response\n\n- Thank you for clarifying that the unclear \"two-step framework\" reference was\nnot about the two facets. I still do not find this use of a pipeline to be a\nparticularly interesting contribution.\n- You state that \"5. de Marneffe (2012) used additional annotated features in\ntheir system. For fair comparison, we re-implement their system with annotated\ninformation in FactBank.\" But the de Marneffe et al. feature cited in the\npaper, \"Predicate Classes\" requires only a dependency parser and vocabulary\nlists from Roser Saur\u00ed's PhD thesis; \"general classes of event\" might be\nreferring to FactML event classes, and while I admit it is not particularly\nclear in their work, I am sure they could clarify.\n- I continue to find the use of \"combined properly\" to be obscure. I agree that\nusing LSTM and CNN where respectively appropriate is valuable, but you seem to\nimply that some prior work has been improper, and that it is their combination\nwhich must be proper.\n- Thank you for reporting on separate LSTMs for each of the paths. I am curious\nas to why this combination may less effective. In any case, experiments with\nthis kind of alternative structure deserve to be reported.\n\n---\n\nThis paper introduces deep neural net technologies to the task of factuality\nclassification as defined by FactBank, with performance exceeding alternative\nneural net models and baselines reimplemented from the literature.\n\n- Strengths:\n\nThis paper is very clear in its presentation of a sophisticated model for\nfactuality classification and of its evaluation.  It shows that the use of\nattentional features and BiLSTM clearly provide benefit over alternative\npooling strategies, and that the model also exceeds the performance of a more\ntraditional feature-based log-linear model.  Given the small amount of training\ndata in FactBank, this kind of highly-engineered model seems appropriate. It is\ninteresting to see that the BiLSTM/CNN model is able to provide benefit despite\nlittle training data.\n\n- Weaknesses:\n\nMy main concerns with this work regard its (a) apparent departure from the\nevaluation procedure in the prior literature; (b) failure to present prior work\nas a strong baseline; and (c) novelty.\n\nWhile I feel that the work is original in engineering deep neural nets for the\nfactuality classification task, and that such work is valuable, its approach is\nnot particularly novel, and \"the proposal of a two-step supervised framework\"\n(line 087) is not particularly interesting given that FactBank was always\ndescribed in terms of two facets (assuming I am correct to interpret \"two-step\"\nas referring to these facets, which I may not be).\n\nThe work cites Saur\u00ed and Pustejovsky (2012), but presents their much earlier\n(2008) and weaker system as a baseline; nor does it consider Qian et al.'s\n(IALP 2015) work which compares to the former.              Both these works are\ndeveloped\non the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT\nTimeBank section, while the present work does not report results on a held-out\nset.\n\nde Marneffe et al.'s (2012) system is also chosen as a baseline, but not all\ntheir features are implemented, nor is the present system evaluated on their\nPragBank corpus (or other alternative representations of factuality proposed in\nPrabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is\ntherefore somewhat lacking in comparability to prior work.\n\nThere were also important questions left unanswered in evaluation, such as the\neffect of using gold standard events or SIPs.\n\nGiven the famed success of BiLSTMs with little feature engineering, it is\nsomewhat disappointing that this work does not attempt to consider a more\nminimal system employing deep neural nets on t",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Event Factuality Identification via Deep Neural Networks \n\nHere is the abstract of the paper: Event factuality identification plays an important role in deep NLP applications. In this paper, we propose a deep learning framework for this task which first extracts essential information from raw texts as the inputs and then identifies the factuality of events via a deep neural network with a proper combination of Bidirectional Long Short-Term Memory (BiLSTM) neural network and Convolutional Neural Network (CNN). The experimental results on FactBank show that our framework significantly outperforms several state-of-the-art baselines. \n\nHere is the partial review: Update after author response: \n\n1. My major concern about the optimization of model's hyperparameter (which are\nnumerous) has not been addressed. This is very important, considering that you\nreport results from folded cross-validation. \n\n2. The explanation that benefits of their method are experimentally confirmed\nwith 2% difference -- while evaluating via 5-fold CV on 200 examples -- is\nquite unconvincing.\n\n========================================================================\n\nSummary:\n\nIn this paper authors present a complex neural model for detecting factuality\nof event mentions in text. The authors combine the following in their complex\nmodel:                          (1) a set of traditional classifiers for detecting\nevent\nmentions,\nfactuality sources, and source introducing predicates (SIPs), (2) A\nbidirectional attention-based LSTM model that learns latent representations for\nelements on different dependency paths used as input, (2) A CNN that uses\nrepresentations from the LSTM and performs two output predictions (one to\ndetect specific from underspecified cases and another to predict the actual\nfactuality class). \n\nFrom the methodological point of view, the authors are combining a reasonably\nfamiliar methods (att-BiLSTM and CNN) into a fairly complex model. However,\nthis model does not take raw text (sequence of word embeddings) as input, but\nrather hand-crafted features (e.g., different dependency paths combining\nfactuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted\nfeatures is somewhat surprising if coupled with complex deep model. The\nevaluation seems a bit tainted as the authors report the results from folded\ncross-validation but do not report how they optimized the hyperparameters of\nthe model. Finally, the results are not too convincing -- considering the\ncomplexity of the model and the amount of preprocessing required (extraction of\nevent mentions, SIPs, and clues), a 2% macro-average gain over the rule-based\nbaseline and overall 44% performance seems modest, at best (looking at\nMicro-average, the proposed model doesn't outperform simple MaxEnt classifier).\n\nThe paper is generally well-written and fairly easy to understand. Altogether,\nI find this paper to be informative to an extent, but in it's current form not\na great read for a top-tier conference.   \n\nRemarks:\n\n1. You keep mentioning that the LSTM and CNN in your model are combined\n\"properly\" -- what does that actually mean? How does this \"properness\"\nmanifest? What would be the improper way to combine the models?\n\n2. I find the motivation/justification for the two output design rather weak: \n    - the first argument that it allows for later addition of cues (i.e\nmanually-designed features) kind of beats the \"learning representations\"\nadvantage of using deep models. \n        - the second argument about this design tackling the imbalance in the\ntraining set is kind of hand-wavy as there is no experimental support for this\nclaim. \n\n3. You first motivate the usage of your complex DL architecture with learning\nlatent representations and avoiding manual design and feature computation.  And\nthen you define a set of manually designed features (several dependency paths\nand lexical features) as input for the model. Do you notice the discrepancy? \n\n4. The LSTMs (bidirectional, and also with attention) have by now already\nbecome a standard model for various NLP tasks. Thus I find the detailed\ndescription of the attention-based bidirectional LSTM unnecessary. \n5. What you present as a baseline in Section 3 is also part of your model (as\nit generates input to your model). Thus, I think that calling it a baseline\nundermines the understandability of the paper. \n\n6. The results reported originate from a 5-fold CV. However, the model contains\nnumerous hyperparameters that need to be optimized (e.g., number of filters and\nfilter sizes for CNNs). How do you optimize these values? Reporting results\nfrom a folded cross-validation doesn't allow for a fair optimization of the\nhypeparameters: either you're not optimizing the model's hyperparameters at\nall, or you're optimizing their values on the test set (which is unfair). \n\n7. \"Notice that some values are non-application (NA) grammatically, e.g., PRu,\nPSu, U+/-\" -- why is underspecification in ony one dimension (polarity or\ncertainty) not an option? I can easily think of a case where it is clear the\nevent is negative, but it is not specified whether the absence of an event is\ncertain, probable, or possible. \n\nLanguage & style:\n\n1. \"to a great degree\" -> \"great degree\" is an unusual construct, use either\n\"great extent\" or \"large degree\"\n2. \"events that can not\" -> \"cannot\" or \"do not\"\n3. \"describes out networks...in details shown in Figure 3.\" -> \"...shown in\nFigure 3 in details.\"\nComments after author response\n\n- Thank you for clarifying that the unclear \"two-step framework\" reference was\nnot about the two facets. I still do not find this use of a pipeline to be a\nparticularly interesting contribution.\n- You state that \"5. de Marneffe (2012) used additional annotated features in\ntheir system. For fair comparison, we re-implement their system with annotated\ninformation in FactBank.\" But the de Marneffe et al. feature cited in the\npaper, \"Predicate Classes\" requires only a dependency parser and vocabulary\nlists from Roser Saur\u00ed's PhD thesis; \"general classes of event\" might be\nreferring to FactML event classes, and while I admit it is not particularly\nclear in their work, I am sure they could clarify.\n- I continue to find the use of \"combined properly\" to be obscure. I agree that\nusing LSTM and CNN where respectively appropriate is valuable, but you seem to\nimply that some prior work has been improper, and that it is their combination\nwhich must be proper.\n- Thank you for reporting on separate LSTMs for each of the paths. I am curious\nas to why this combination may less effective. In any case, experiments with\nthis kind of alternative structure deserve to be reported.\n\n---\n\nThis paper introduces deep neural net technologies to the task of factuality\nclassification as defined by FactBank, with performance exceeding alternative\nneural net models and baselines reimplemented from the literature.\n\n- Strengths:\n\nThis paper is very clear in its presentation of a sophisticated model for\nfactuality classification and of its evaluation.  It shows that the use of\nattentional features and BiLSTM clearly provide benefit over alternative\npooling strategies, and that the model also exceeds the performance of a more\ntraditional feature-based log-linear model.  Given the small amount of training\ndata in FactBank, this kind of highly-engineered model seems appropriate. It is\ninteresting to see that the BiLSTM/CNN model is able to provide benefit despite\nlittle training data.\n\n- Weaknesses:\n\nMy main concerns with this work regard its (a) apparent departure from the\nevaluation procedure in the prior literature; (b) failure to present prior work\nas a strong baseline; and (c) novelty.\n\nWhile I feel that the work is original in engineering deep neural nets for the\nfactuality classification task, and that such work is valuable, its approach is\nnot particularly novel, and \"the proposal of a two-step supervised framework\"\n(line 087) is not particularly interesting given that FactBank was always\ndescribed in terms of two facets (assuming I am correct to interpret \"two-step\"\nas referring to these facets, which I may not be).\n\nThe work cites Saur\u00ed and Pustejovsky (2012), but presents their much earlier\n(2008) and weaker system as a baseline; nor does it consider Qian et al.'s\n(IALP 2015) work which compares to the former.              Both these works are\ndeveloped\non the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT\nTimeBank section, while the present work does not report results on a held-out\nset.\n\nde Marneffe et al.'s (2012) system is also chosen as a baseline, but not all\ntheir features are implemented, nor is the present system evaluated on their\nPragBank corpus (or other alternative representations of factuality proposed in\nPrabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is\ntherefore somewhat lacking in comparability to prior work.\n\nThere were also important questions left unanswered in evaluation, such as the\neffect of using gold standard events or SIPs.\n\nGiven the famed success of BiLSTMs with little feature engineering, it is\nsomewhat disappointing that this work does not attempt to consider a more\nminimal system employing deep neural nets on t \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper introduces deep neural net technologies to the task of factuality classification as defined by FactBank, with performance exceeding alternative neural net models and baselines reimplemented from the literature.\\n\\n- Strengths:\\n\\nThis paper is very clear in its presentation of a sophisticated model for factuality classification and of its evaluation.  It shows that the use of attentional features and BiLSTM clearly provide benefit over alternative pooling strategies, and that the model also exceeds the performance of a more traditional feature-based log-linear model.  Given the small amount of training data in FactBank, this kind of highly-engineered model seems appropriate. It is interesting to see that the BiLSTM/CNN model is able to provide benefit despite little training data.\\n\\n- Weaknesses:\\n\\nMy main concerns with this work regard its (a) apparent departure from the evaluation procedure in the prior literature; (b) failure to present prior work as a strong baseline; and (c) novelty.\\n\\nWhile I feel that the work is original in engineering deep neural nets for the factuality classification task, and that such work is valuable, its approach is not particularly novel, and \\u201cthe proposal of a two-step supervised framework\\u201d (line 087) is not particularly interesting given that FactBank was always described in terms of two facets (assuming I am correct to interpret \\u201ctwo-step\\u201d as referring to these facets, which I may not be).\\n\\nThe work cites Saur\\u00ed and Pustejovsky (2012), but presents their much earlier (2008) and weaker system as a baseline; nor does it consider Qian et al.\\u2019s (IALP 2015) work which compares to the former.              Both these works are developed\\non the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT\\nTimeBank section, while the present work does not report results on a held-out\\nset.\\n\\nde Marneffe et al.\\u2019s (2012) system is also chosen as a baseline, but not all\\ntheir features are implemented, nor is the present system evaluated on their\\nPragBank corpus (or other alternative representations of factuality proposed in\\nPrabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is\\ntherefore somewhat lacking in comparability to prior work.\\n\\nThere were also important questions left unanswered in evaluation, such as the\\neffect of using gold standard events or SIPs.\\n\\nGiven the famed success of BiLSTMs with little feature engineering, it is\\nsomewhat disappointing that this work does not attempt to consider a more\\nminimal system employing deep neural nets on t \"\n}",
        "human_end_index": 8560
    },
    {
        "id": "318",
        "source": "PeerRead/acl_2017/",
        "title": "Improved Word Representation Learning with Sememes",
        "human_review": "This work showed that word representation learning can benefit from sememes\nwhen used in an appropriate attention scheme. Authors hypothesized that sememes\ncan act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\nmodel which detects word senses and learn representations simultaneously.\nThough experimental results indicate that WRL benefits, exact gains for WSI are\nunclear since a qualitative case study of a couple of examples has only been\ndone. Overall, paper is well-written and well-structured.\n\nIn the last paragraph of introduction section, authors tried to tell three\ncontributions of this work. (1) and (2) are more of novelties of the work\nrather than contributions. I see the main contribution of the work to be the\nresults which show that we can learn better word representations (unsure about\nWSI) by modeling sememe information than other competitive baselines. (3) is\nneither a contribution nor a novelty.\n\nThe three strategies tried for SE-WRL modeling makes sense and can be\nintuitively ranked in terms of how well they will work. Authors did a good job\nexplaining that and experimental results supported the intuition but the\nreviewer also sees MST as a fourth strategy rather than a baseline inspired by\nChen et al. 2014 (many WSI systems assume one sense per word given a context).\nMST many times performed better than SSA and SAC. Unless authors missed to\nclarify otherwise, MST seems to be exactly like SAT with a difference that\ntarget word is represented by the most probable sense rather than taking an\nattention weighted average over all its senses. MST is still an attention based\nscheme where sense with maximum attention weight is chosen though it has not\nbeen clearly mentioned if target word is represented by chosen sense embedding\nor some function of it.\n\nAuthors did not explain the selection of datasets for training and evaluation\ntasks. Reference page to Sogou-T text corpus did not help as reviewer does not\nknow Chinese language. It was unclear which exact dataset was used as there are\nseveral datasets mentioned on that page. Why two word similarity datasets were\nused and how they are different  (like does one has more rare words than\nanother) since different models performed differently on these datasets. The\nchoice of these datasets did not allow evaluating against results of other\nworks which makes the reviewer wonder about next question.\n\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\nusing CBOW word embeddings.\n\nReviewer needs clarification on some model parameters like vocabulary sizes for\nwords (Does Sogou-T contains 2.7 billion unique words) and word senses (how\nmany word types from HowNet). Because of the notation used it is not clear if\nembeddings for senses and sememes for different words were shared. Reviewer\nhopes that is the case but then why 200 dimensional embeddings were used for\nonly 1889 sememes. It would be better if complexity of model parameters can\nalso be discussed.\n\nMay be due to lack of space but experiment results discussion lack insight into\nobservations other than SAT performing the best. Also, authors claimed that\nwords with lower frequency were learned better with sememes without evaluating\non a rare words dataset.\n\nI have read author's response.\n- Strengths:\n\nThis paper proposes the use of HowNet to enrich embedings. The idea is\ninteresting and gives good results.\n\n- Weaknesses:\nThe paper is interesting, but I am not sure the contibution is important enough\nfor a long paper. Also, the comparision with other works may not be fair:\nauthors should compare to other systems that use manually developed resources.\n\nThe paper is understandable, but it would help some improvement on the English.\n\n- General Discussion:\n- Strengths:\n\n1. The proposed models are shown to lead to rather substantial and consistent\nimprovements over reasonable baselines on two different tasks (word similarity\nand word analogy), which not only serves to demonstrate the effectiveness of\nthe models but also highlights the potential utility of incorporating sememe\ninformation from available knowledge resources for improving word\nrepresentation learning.\n2. The paper contributes to ongoing efforts in the community to account for\npolysemy in word representation learning. It builds nicely on previous work and\nproposes some new ideas and improvements that could be of interest to the\ncommunity, such as applying an attention scheme to incorporate a form of soft\nword sense disambiguation into the learning procedure.\n\n- Weaknesses:\n\n1. Presentation and clarity: important details with respect to the proposed\nmodels are left out or poorly described (more details below). Otherwise, the\npaper generally reads fairly well; however, the manuscript would need to be\nimproved if accepted.\n2. The evaluation on the word analogy task seems a bit unfair given that the\nsemantic relations are explicitly encoded by the sememes, as the authors\nthemselves point out (more details below).\n\n- General Discussion:\n\n1. The authors stress the importance of accounting for polysemy and learning\nsense-specific representations. While polysemy is taken into account by\ncalculating sense distributions for words in particular contexts in the\nlearning procedure, the evaluation tasks are entirely context-independent,\nwhich means that, ultimately, there is only one vector per word -- or at least\nthis is what is evaluated. Instead, word sense disambiguation and sememe\ninformation are used for improving the learning of word representations. This\nneeds to be clarified in the paper.\n2. It is not clear how the sememe embeddings are learned and the description of\nthe SSA model seems to assume the pre-existence of sememe embeddings. This is\nimportant for understanding the subsequent models. Do the SAC and SAT models\nrequire pre-training of sememe embeddings?\n3. It is unclear how the proposed models compare to models that only consider\ndifferent senses but not sememes. Perhaps the MST baseline is an example of\nsuch a model? If so, this is not sufficiently described (emphasis is instead\nput on soft vs. hard word sense disambiguation). The paper would be stronger\nwith the inclusion of more baselines based on related work.\n4. A reasonable argument is made that the proposed models are particularly\nuseful for learning representations for low-frequency words (by mapping words\nto a smaller set of sememes that are shared by sets of words). Unfortunately,\nno empirical evidence is provided to test the hypothesis. It would have been\ninteresting for the authors to look deeper into this. This aspect also does not\nseem to explain the improvements much since, e.g., the word similarity data\nsets contain frequent word pairs.\n5. Related to the above point, the improvement gains seem more attributable to\nthe incorporation of sememe information than word sense disambiguation in the\nlearning procedure. As mentioned earlier, the evaluation involves only the use\nof context-independent word representations. Even if the method allows for\nlearning sememe- and sense-specific representations, they would have to be\naggregated to carry out the evaluation task.\n6. The example illustrating HowNet (Figure 1) is not entirely clear, especially\nthe modifiers of \"computer\".\n7. It says that the models are trained using their best parameters. How exactly\nare these determined? It is also unclear how K is set -- is it optimized for\neach model or is it randomly chosen for each target word observation? Finally,\nwhat is the motivation for setting K' to 2?",
        "partial_review": "This work showed that word representation learning can benefit from sememes\nwhen used in an appropriate attention scheme. Authors hypothesized that sememes\ncan act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\nmodel which detects word senses and learn representations simultaneously.\nThough experimental results indicate that WRL benefits, exact gains for WSI are\nunclear since a qualitative case study of a couple of examples has only been\ndone. Overall, paper is well-written and well-structured.\n\nIn the last paragraph of introduction section, authors tried to tell three\ncontributions of this work. (1) and (2) are more of novelties of the work\nrather than contributions. I see the main contribution of the work to be the\nresults which show that we can learn better word representations (unsure about\nWSI) by modeling sememe information than other competitive baselines. (3) is\nneither a contribution nor a novelty.\n\nThe three strategies tried for SE-WRL modeling makes sense and can be\nintuitively ranked in terms of how well they will work. Authors did a good job\nexplaining that and experimental results supported the intuition but the\nreviewer also sees MST as a fourth strategy rather than a baseline inspired by\nChen et al. 2014 (many WSI systems assume one sense per word given a context).\nMST many times performed better than SSA and SAC. Unless authors missed to\nclarify otherwise, MST seems to be exactly like SAT with a difference that\ntarget word is represented by the most probable sense rather than taking an\nattention weighted average over all its senses. MST is still an attention based\nscheme where sense with maximum attention weight is chosen though it has not\nbeen clearly mentioned if target word is represented by chosen sense embedding\nor some function of it.\n\nAuthors did not explain the selection of datasets for training and evaluation\ntasks. Reference page to Sogou-T text corpus did not help as reviewer does not\nknow Chinese language. It was unclear which exact dataset was used as there are\nseveral datasets mentioned on that page. Why two word similarity datasets were\nused and how they are different  (like does one has more rare words than\nanother) since different models performed differently on these datasets. The\nchoice of these datasets did not allow evaluating against results of other\nworks which makes the reviewer wonder about next question.\n\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\nusing CBOW word embeddings.\n\nReviewer needs clarification on some model parameters like vocabulary sizes for\nwords (Does Sogou-T contains 2.7 billion unique words) and word senses (how\nmany word types from HowNet). Because of the notation used it is not clear if\nembeddings for senses and sememes for different words were shared. Reviewer\nhopes that is the case but then why 200 dimensional embeddings were used for\nonly 1889 sememes. It would be better if complexity of model parameters can\nalso be discussed.\n\nMay be due to lack of space but experiment results discussion lack insight into\nobservations other than SAT performing the best. Also, authors claimed that\nwords with lower frequency were learned better with sememes without evaluating\non a rare words dataset.\n\nI have read author's response.\n- Strengths:\n\nThis paper proposes the use of HowNet to enrich embedings. The idea is\ninteresting and gives good results.\n\n- Weaknesses:\nThe paper is interesting, but I am not sure the contibution is important enough\nfor a long paper. Also, the comparision with other works may not be fair:\nauthors should compare to other systems that use manually developed resources.\n\nThe paper is understandable, but it would help some improvement on the En",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Improved Word Representation Learning with Sememes \n\nHere is the abstract of the paper: Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information. \n\nHere is the partial review: This work showed that word representation learning can benefit from sememes\nwhen used in an appropriate attention scheme. Authors hypothesized that sememes\ncan act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\nmodel which detects word senses and learn representations simultaneously.\nThough experimental results indicate that WRL benefits, exact gains for WSI are\nunclear since a qualitative case study of a couple of examples has only been\ndone. Overall, paper is well-written and well-structured.\n\nIn the last paragraph of introduction section, authors tried to tell three\ncontributions of this work. (1) and (2) are more of novelties of the work\nrather than contributions. I see the main contribution of the work to be the\nresults which show that we can learn better word representations (unsure about\nWSI) by modeling sememe information than other competitive baselines. (3) is\nneither a contribution nor a novelty.\n\nThe three strategies tried for SE-WRL modeling makes sense and can be\nintuitively ranked in terms of how well they will work. Authors did a good job\nexplaining that and experimental results supported the intuition but the\nreviewer also sees MST as a fourth strategy rather than a baseline inspired by\nChen et al. 2014 (many WSI systems assume one sense per word given a context).\nMST many times performed better than SSA and SAC. Unless authors missed to\nclarify otherwise, MST seems to be exactly like SAT with a difference that\ntarget word is represented by the most probable sense rather than taking an\nattention weighted average over all its senses. MST is still an attention based\nscheme where sense with maximum attention weight is chosen though it has not\nbeen clearly mentioned if target word is represented by chosen sense embedding\nor some function of it.\n\nAuthors did not explain the selection of datasets for training and evaluation\ntasks. Reference page to Sogou-T text corpus did not help as reviewer does not\nknow Chinese language. It was unclear which exact dataset was used as there are\nseveral datasets mentioned on that page. Why two word similarity datasets were\nused and how they are different  (like does one has more rare words than\nanother) since different models performed differently on these datasets. The\nchoice of these datasets did not allow evaluating against results of other\nworks which makes the reviewer wonder about next question.\n\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\nusing CBOW word embeddings.\n\nReviewer needs clarification on some model parameters like vocabulary sizes for\nwords (Does Sogou-T contains 2.7 billion unique words) and word senses (how\nmany word types from HowNet). Because of the notation used it is not clear if\nembeddings for senses and sememes for different words were shared. Reviewer\nhopes that is the case but then why 200 dimensional embeddings were used for\nonly 1889 sememes. It would be better if complexity of model parameters can\nalso be discussed.\n\nMay be due to lack of space but experiment results discussion lack insight into\nobservations other than SAT performing the best. Also, authors claimed that\nwords with lower frequency were learned better with sememes without evaluating\non a rare words dataset.\n\nI have read author's response.\n- Strengths:\n\nThis paper proposes the use of HowNet to enrich embedings. The idea is\ninteresting and gives good results.\n\n- Weaknesses:\nThe paper is interesting, but I am not sure the contibution is important enough\nfor a long paper. Also, the comparision with other works may not be fair:\nauthors should compare to other systems that use manually developed resources.\n\nThe paper is understandable, but it would help some improvement on the En \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This work showed that word representation learning can benefit from sememes when used in an appropriate attention scheme. Authors hypothesized that sememes can act as an essential regularizer for WRL and WSI tasks and proposed SE-WL model which detects word senses and learn representations simultaneously. Though experimental results indicate that WRL benefits, exact gains for WSI are unclear since a qualitative case study of a couple of examples has only been done. Overall, paper is well-written and well-structured.\\n\\nIn the last paragraph of introduction section, authors tried to tell three contributions of this work. (1) and (2) are more of novelties of the work rather than contributions. I see the main contribution of the work to be the results which show that we can learn better word representations (unsure about WSI) by modeling sememe information than other competitive baselines. (3) is neither a contribution nor a novelty.\\n\\nThe three strategies tried for SE-WRL modeling makes sense and can be intuitively ranked in terms of how well they will work. Authors did a good job explaining that and experimental results supported the intuition but the reviewer also sees MST as a fourth strategy rather than a baseline inspired by Chen et al. 2014 (many WSI systems assume one sense per word given a context). MST many times performed better than SSA and SAC. Unless authors missed to clarify otherwise, MST seems to be exactly like SAT with a difference that target word is represented by the most probable sense rather than taking an attention weighted average over all its senses. MST is still an attention based scheme where sense with maximum attention weight is chosen though it has not been clearly mentioned if target word is represented by chosen sense embedding or some function of it.\\n\\nAuthors did not explain the selection of datasets for training and evaluation tasks. Reference page to Sogou-T text corpus did not help as reviewer does not know Chinese language. It was unclear which exact dataset was used as there are several datasets mentioned on that page. Why two word similarity datasets were used and how they are different (like does one has more rare words than another) since different models performed differently on these datasets. The choice of these datasets did not allow evaluating against results of other works which makes the reviewer wonder about next question.\\n\\nAre proposed SAT model results state of the art for Chinese word similarity? E.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by using CBOW word embeddings.\\n\\nReviewer needs clarification on some model parameters like vocabulary sizes for words (Does Sogou-T contains 2.7 billion unique words) and word senses (how many word types from HowNet). Because of the notation used it is not clear if embeddings for senses and sememes for different words were shared. Reviewer hopes that is the case but then why 200 dimensional embeddings were used for only 1889 sememes. It would be better if complexity of model parameters can also be discussed.\\n\\nMay be due to lack of space but experiment results discussion lack insight into observations other than SAT performing the best. Also, authors claimed that words with lower frequency were learned better with sememes without evaluating on a rare words dataset.\"\n}",
        "human_end_index": 3812
    },
    {
        "id": "326",
        "source": "PeerRead/acl_2017/",
        "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation",
        "human_review": "- Strengths:\n\nThe authors use established neural network methods (adversarial networks --\nGoodfellow et al, NIPS-2014) to take advantage of 8 different Chinese work\nbreaking test sets, with 8 different notions of what counts as a word in\nChinese.\n\nThis paper could have implications for many NLP tasks where we have slightly\ndifferent notions of what counts as correct.  We have been thinking of that\nproblem in terms of adaptation, but it is possible that Goodfellow et al is a\nmore useful way of thinking about this problem.\n\n- Weaknesses:\n\nWe need a name for the problem mentioned above.  How about: the elusive gold\nstandard.  I prefer that term to multi-criteria.\n\nThe motivation seems to be unnecessarily narrow.  The elusive gold standard\ncomes up in all sorts of applications, not just Chinese Word Segmentation.\n\nThe motivation makes unnecessary assumptions about how much the reader knows\nabout Chinese.              When you don't know much about something, you think it is\neasier than it is.  Many non-Chinese readers (like this reviewer) think that\nChinese is simpler than it is.              It is easy to assume that Chinese Word\nSegmentation is about as easy as tokenizing English text into strings delimited\nby white space.  But my guess is that IAA (inter-annotator agreement) is pretty\nlow in Chinese.  The point you are trying to make in Table 1 is that there is\nconsiderable room for disagreement among native speakers of Chinese.\n\nI think it would help if you could point out that there are many NLP tasks\nwhere there is considerable room for disagreement.  Some tasks like machine\ntranslation, information retrieval and web search have so much room for\ndisagreement that the metrics for those tasks have been designed to allow for\nmultiple correct answers.  For other tasks, like part of speech tagging, we\ntend to sweep the elusive gold standard problem under a rug, and hope it will\njust go away.  But in fact, progress on tagging has stalled because we don't\nknow how to distinguish differences of opinions from errors.  When two\nannotators return two different answers, it is a difference of opinion.  But\nwhen a machine returns a different answer, the machine is almost always wrong.\n\nThis reader got stuck on the term: adversary.  I think the NIPS paper used that\nbecause it was modeling noise under \"murphy's law.\"  It is often wise to assume\nthe worst.\n\nBut I don't think it is helpful to think of differences of opinion as an\nadversarial game like chess.  In chess, it makes sense to think that your\nopponent is out to get you, but I'm not sure that's the most helpful way to\nthink about differences of opinion.\n\nI think it would clarify what you are doing to say that you are applying an\nestablished method from NIPS (that uses the term \"adversarial\") to deal with\nthe elusive gold standard problem.  And then point out that the elusive gold\nstandard problem is a very common problem.  You will study it in the context of\na particular problem in Chinese, but the problem is much more general than\nthat.\n\n- General Discussion:\n\nI found much of the paper unnecessarily hard going.  I'm not up on Chinese or\nthe latest in NIPS, which doesn't help.  But even so, there are some small\nissues with English, and some larger problems with exposition.\n\nConsider Table 4.  Line 525 makes an assertion about the first block and depth\nof networks.  Specifically, which lines in Table 4 support that assertion.\n\nI assume that P and R refer to precision and recall, but where is that\nexplained.  I assume that F is the standard F measure, and OOV is\nout-of-vocabulary, but again, I shouldn't have to assume such things.\n\nThere are many numbers in Table 4.  What counts as significance?  Which numbers\nare even comparable?  Can we compare numbers across cols?  Is performance on\none collection comparable to performance on another?  Line 560 suggests that\nthe adversarial method is not significant.  What should I take away from Table\n4?  Line 794 claims that you have a significant solution to what I call the\nelusive gold standard problem.              But which numbers in Table 4 justify that\nclaim?\n\nSmall quibbles about English:\n\nworks --> work (in many places).  Work is a  mass noun, not a count noun\n(unlike \"conclusion\").              One can say one conclusion, two conclusions, but\nmore/less/some work (not one work, two works).\n\nline 493: each dataset, not each datasets\n\nline 485: Three datasets use traditional Chinese (AS, CITY, CKIP) and the other\nfive use simplified Chinese.\n\nline 509: random --> randomize\nThe paper proposes a method to train models for Chinese word segmentation (CWS)\non datasets having multiple segmentation criteria.\n\n- Strengths:\n1. Multi-criteria learning is interesting and promising.\n2. The proposed model is also interesting and achieves a large improvement from\nbaselines.\n\n- Weaknesses:\n1. The proposed method is not compared with other CWS models. The baseline\nmodel (Bi-LSTM) is proposed in [1] and [2]. However, these model is proposed\nnot for CWS but for POS tagging and NE tagging. The description \"In this paper,\nwe employ the state-of-the-art architecture ...\" (in Section 2) is misleading.\n2. The purpose of experiments in Section 6.4 is unclear. In Sec. 6.4, the\npurpose is that investigating \"datasets in traditional Chinese and simplified\nChinese could help each other.\" However, in the experimental setting, the model\nis separately trained on simplified Chinese and traditional Chinese, and the\nshared parameters are fixed after training on simplified Chinese. What is\nexpected to fixed shared parameters?\n\n- General Discussion:\nThe paper should be more interesting if there are more detailed discussion\nabout the datasets that adversarial multi-criteria learning does not boost the\nperformance.\n\n[1] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for\nsequence tagging. arXiv preprint arXiv:1508.01991.\n[2] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via\nbi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354 .",
        "partial_review": "- Strengths:\n\nThe authors use established neural network methods (adversarial networks --\nGoodfellow et al, NIPS-2014) to take advantage of 8 different Chinese work\nbreaking test sets, with 8 different notions of what counts as a word in\nChinese.\n\nThis paper could have implications for many NLP tasks where we have slightly\ndifferent notions of what counts as correct.  We have been thinking of that\nproblem in terms of adaptation, but it is possible that Goodfellow et al is a\nmore useful way of thinking about this problem.\n\n- Weaknesses:\n\nWe need a name for the problem mentioned above.  How about: the elusive gold\nstandard.  I prefer that term to multi-criteria.\n\nThe motivation seems to be unnecessarily narrow.  The elusive gold standard\ncomes up in all sorts of applications, not just Chinese Word Segmentation.\n\nThe motivation makes unnecessary assumptions about how much the reader knows\nabout Chinese.              When you don't know much about something, you think it is\neasier than it is.  Many non-Chinese readers (like this reviewer) think that\nChinese is simpler than it is.              It is easy to assume that Chinese Word\nSegmentation is about as easy as tokenizing English text into strings delimited\nby white space.  But my guess is that IAA (inter-annotator agreement) is pretty\nlow in Chinese.  The point you are trying to make in Table 1 is that there is\nconsiderable room for disagreement among native speakers of Chinese.\n\nI think it would help if you could point out that there are many NLP tasks\nwhere there is considerable room for disagreement.  Some tasks like machine\ntranslation, information retrieval and web search have so much room for\ndisagreement that the metrics for those tasks have been designed to allow for\nmultiple correct answers.  For other tasks, like part of speech tagging, we\ntend to sweep the elusive gold standard problem under a rug, and hope it will\njust go away.  But in fact, progress on tagging has stalled because we don't\nknow how to distinguish differences of opinions from errors.  When two\nannotators return two different answers, it is a difference of opinion.  But\nwhen a machine returns a different answer, the machine is almost always wrong.\n\nThis reader got stuck on the term: adversary.  I think the NIPS paper used that\nbecause it was modeling noise under \"murphy's law.\"  It is often wise to assume\nthe worst.\n\nBut I don't think it is helpful to think of differences of opinion as an\nadversarial game like chess.  In chess, it makes sense to think that your\nopponent is out to get you, but I'm not sure that's the most helpful way to\nthink about differences of opinion.\n\nI think it would clarify what you are doing to say that you are applying an\nestablished method from NIPS (that uses the term \"adversarial\") to deal with\nthe elusive gold standard problem.  And then point out that the elusive gold\nstandard problem is a very common problem.  You will study it in the context of\na particular problem in Chinese, but the problem is much more general than\nt",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Adversarial Multi-Criteria Learning for Chinese Word Segmentation \n\nHere is the abstract of the paper: Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria.  Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github. \n\nHere is the partial review: - Strengths:\n\nThe authors use established neural network methods (adversarial networks --\nGoodfellow et al, NIPS-2014) to take advantage of 8 different Chinese work\nbreaking test sets, with 8 different notions of what counts as a word in\nChinese.\n\nThis paper could have implications for many NLP tasks where we have slightly\ndifferent notions of what counts as correct.  We have been thinking of that\nproblem in terms of adaptation, but it is possible that Goodfellow et al is a\nmore useful way of thinking about this problem.\n\n- Weaknesses:\n\nWe need a name for the problem mentioned above.  How about: the elusive gold\nstandard.  I prefer that term to multi-criteria.\n\nThe motivation seems to be unnecessarily narrow.  The elusive gold standard\ncomes up in all sorts of applications, not just Chinese Word Segmentation.\n\nThe motivation makes unnecessary assumptions about how much the reader knows\nabout Chinese.              When you don't know much about something, you think it is\neasier than it is.  Many non-Chinese readers (like this reviewer) think that\nChinese is simpler than it is.              It is easy to assume that Chinese Word\nSegmentation is about as easy as tokenizing English text into strings delimited\nby white space.  But my guess is that IAA (inter-annotator agreement) is pretty\nlow in Chinese.  The point you are trying to make in Table 1 is that there is\nconsiderable room for disagreement among native speakers of Chinese.\n\nI think it would help if you could point out that there are many NLP tasks\nwhere there is considerable room for disagreement.  Some tasks like machine\ntranslation, information retrieval and web search have so much room for\ndisagreement that the metrics for those tasks have been designed to allow for\nmultiple correct answers.  For other tasks, like part of speech tagging, we\ntend to sweep the elusive gold standard problem under a rug, and hope it will\njust go away.  But in fact, progress on tagging has stalled because we don't\nknow how to distinguish differences of opinions from errors.  When two\nannotators return two different answers, it is a difference of opinion.  But\nwhen a machine returns a different answer, the machine is almost always wrong.\n\nThis reader got stuck on the term: adversary.  I think the NIPS paper used that\nbecause it was modeling noise under \"murphy's law.\"  It is often wise to assume\nthe worst.\n\nBut I don't think it is helpful to think of differences of opinion as an\nadversarial game like chess.  In chess, it makes sense to think that your\nopponent is out to get you, but I'm not sure that's the most helpful way to\nthink about differences of opinion.\n\nI think it would clarify what you are doing to say that you are applying an\nestablished method from NIPS (that uses the term \"adversarial\") to deal with\nthe elusive gold standard problem.  And then point out that the elusive gold\nstandard problem is a very common problem.  You will study it in the context of\na particular problem in Chinese, but the problem is much more general than\nt \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Strengths:\\n\\nThe authors use established neural network methods (adversarial networks -- Goodfellow et al, NIPS-2014) to take advantage of 8 different Chinese word breaking test sets, with 8 different notions of what counts as a word in Chinese. This paper could have implications for many NLP tasks where we have slightly different notions of what counts as correct. We have been thinking of that problem in terms of adaptation, but it is possible that Goodfellow et al is a more useful way of thinking about this problem.\\n\\nWeaknesses:\\n\\n- We need a name for the problem mentioned above. How about: the elusive gold standard. I prefer that term to multi-criteria.\\n- The motivation seems to be unnecessarily narrow. The elusive gold standard comes up in all sorts of applications, not just Chinese Word Segmentation.\\n- The motivation makes unnecessary assumptions about how much the reader knows about Chinese. It is easy to assume that Chinese Word Segmentation is about as easy as tokenizing English text into strings delimited by white space, but my guess is that IAA (inter-annotator agreement) is pretty low in Chinese, indicating considerable room for disagreement among native speakers.\\n- It would be helpful to point out that there are many NLP tasks where there is considerable room for disagreement. Some tasks, like machine translation, information retrieval, and web search, have so much room for disagreement that the metrics for those tasks have been designed to allow for multiple correct answers. For other tasks, like part of speech tagging, progress has stalled due to the difficulty of distinguishing differences of opinions from errors.\\n- The term 'adversary' may be confusing and not the most helpful way to think about differences of opinion. It would be clearer to state that an established method from NIPS, which uses the term 'adversarial', is being applied to deal with the elusive gold standard problem. Highlight that the elusive gold standard problem is a common and general problem, which will be studied in the context of Chinese Word Segmentation.\\n\\nOverall, the paper introduces an interesting approach to leveraging multiple criteria for Chinese Word Segmentation. However, the motivation and problem statement could be broader to encompass the wider applicability of the elusive gold standard problem, and limitations of Chinese Word Segmentation techniques for native speakers could be more explicitly acknowledged.\"\n}",
        "human_end_index": 3031
    },
    {
        "id": "33",
        "source": "PeerRead/acl_2017/",
        "title": "Linguistically Regularized LSTM for Sentiment Classification",
        "human_review": "Strengths:\n\n- Innovative idea: sentiment through regularization\n- Experiments appear to be done well from a technical point of view\n- Useful in-depth analysis of the model\n\nWeaknesses:\n\n- Very close to distant supervision\n- Mostly poorly informed baselines\n\nGeneral Discussion:\n\nThis paper presents an extension of the vanilla LSTM model that\nincorporates sentiment information through regularization.  The\nintroduction presents the key claims of the paper: Previous CNN\napproaches are bad when no phrase-level supervision is present.\nPhrase-level annotation is expensive. The contribution of this paper is\ninstead a \"simple model\" using other linguistic resources.\n\nThe related work section provides a good review of sentiment\nliterature. However, there is no mention of previous attempts at\nlinguistic regularization (e.g., [YOG14]).\n\nThe explanation of the regularizers in section 4 is rather lengthy and\nrepetitive. The listing on p. 3 could very well be merged with the\nrespective subsection 4.1-4.4. Notation in this section is inconsistent\nand generally hard to follow. Most notably, p is sometimes used with a\nsubscript and sometimes with a superscript.  The parameter \\beta is\nnever explicitly mentioned in the text. It is not entirely clear to me\nwhat constitutes a \"position\" t in the terminology of the paper. t is a\nparameter to the LSTM output, so it seems to be the index of a\nsentence. Thus, t-1 is the preceding sentence, and p_t is the prediction\nfor this sentence. However, the description of the regularizers talks\nabout preceding words, not sentences, but still uses. My assumption here\nis that p_t is actually overloaded and may either mean the sentiment of\na sentence or a word. However, this should be made clearer in the text.\n\nOne dangerous issue in this paper is that the authors tread a fine line\nbetween regularization and distant supervision in their work. The\nproblem here is that there are many other ways to integrate lexical\ninformation from about polarity, negation information, etc. into a model\n(e.g., by putting the information into the features). The authors\ncompare against a re-run or re-implementation of Teng et al.'s NSCL\nmodel. Here, it would be important to know whether the authors used the\nsame lexicons as in their own work. If this is not the case, the\ncomparison is not fair. Also, I do not understand why the authors cannot\nrun NSCL on the MR dataset when they have access to an implementation of\nthe model. Would this not just be a matter of swapping the datasets? The\nremaining baselines do not appear to be using lexical information, which\nmakes them rather poor. I would very much like to see a vanilla LSTM run\nwhere lexical information is simply appended to the word vectors.\n\nThe authors end the paper with some helpful analysis of the\nmodels. These experiments show that the model indeed learns\nintensification and negation to some extent. In these experiments, it\nwould be interesting to know how the model behaves with\nout-of-vocabulary words (with respect to the lexicons). Does the model\nlearn beyond memorization, and does generalization happen for words that\nthe model has not seen in training? Minor remark here: the figures and\ntables are too small to be read in print.\n\nThe paper is mostly well-written apart from the points noted above.  It\ncould benefit from some proofreading as there are some grammatical\nerrors and typos left. In particular, the beginning of the abstract is\nhard to read.\n\nOverall, the paper pursues a reasonable line of research. The largest\npotential issue I see is a somewhat shaky comparison to related\nwork. This could be fixed by including some stronger baselines in the\nfinal model. For me, it would be crucial to establish whether\ncomparability is given in the experiments, and I hope that the authors\ncan shed some light on this in their response.\n\n[YOG14] http://www.aclweb.org/anthology/P14-1074\n\n--------------\n\nUpdate after author response\n\nThank you for clarifying the concerns about the experimental setup. \n\nNSCL: I do now believe that the comparison is with Teng et al. is fair.\n\nLSTM: Good to know that you did this. However, this is a crucial part of the\npaper. As it stands, the baselines are weak. Marginal improvement is still too\nvague, better would be an open comparison including a significance test.\n\nOOV: I understand how the model is defined, but what is the effect on OOV\nwords? This would make for a much more interesting additional experiment than\nthe current regularization experiments.\n- Strengths:\nThis paper proposes a nice way to combine the neural model (LSTM) with\nlinguistic knowledge (sentiment lexicon, negation and intensity). The method is\nsimple yet effective. It achieves the state-of-the-art performance on Movie\nReview dataset and is competitive against the best models on SST dataset.    \n\n- Weaknesses:\nSimilar idea has also been used in (Teng et al., 2016). Though this work is \nmore elegant in the framework design and mathematical representation, the\nexperimental comparison with (Teng et al., 2016) is not as convincing as the\ncomparisons with the rest methods. The authors only reported the\nre-implementation results on the sentence level experiment of SST and did not\nreport their own phrase-level results.\n\nSome details are not well explained, see discussions below.\n\n- General Discussion:\n\nThe reviewer has the following questions/suggestions about this work,\n\n1. Since the SST dataset has phrase-level annotations, it is better to show the\nstatistics of the times that negation or intensity words actually take effect.\nFor example, how many times the word \"nothing\" appears and how many times it\nchanges the polarity of the context.\n\n2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to\npredict the sentiment label?\n\n3. The authors claimed that \"we only use the sentence-level annotation since\none of\nour goals is to avoid expensive phrase-level annotation\". However, the reviewer\nstill suggest to add the results. Please report them in the rebuttal phase if\npossible.\n\n4. \"s_c is a parameter to be optimized but could also be set fixed with prior\nknowledge.\"  The reviewer didn't find the specific definition of s_c in the\nexperiment section, is it learned or set fixed?  What is the learned or fixed\nvalue?\n\n5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment\nwith part of the SST dataset where only phrases with negation/intensity words\nare included. Report the results on this sub-dataset with and without the\ncorresponding regularizer can be more convincing.",
        "partial_review": "Strengths:\n\n- Innovative idea: sentiment through regularization\n- Experiments appear to be done well from a technical point of view\n- Useful in-depth analysis of the model\n\nWeaknesses:\n\n- Very close to distant supervision\n- Mostly poorly informed baselines\n\nGeneral Discussion:\n\nThis paper presents an extension of the vanilla LSTM model that\nincorporates sentiment information through regularization.  The\nintroduction presents the key claims of the paper: Previous CNN\napproaches are bad when no phrase-level supervision is present.\nPhrase-level annotation is expensive. The contribution of this paper is\ninstead a \"simple model\" using other linguistic resources.\n\nThe related work section provides a good review of sentiment\nliterature. However, there is no mention of previous attempts at\nlinguistic regularization (e.g., [YOG14]).\n\nThe explanation of the regularizers in section 4 is rather lengthy and\nrepetitive. The listing on p. 3 could very well be merged with the\nrespective subsection 4.1-4.4. Notation in this section is inconsistent\nand generally hard to follow. Most notably, p is sometimes used with a\nsubscript and sometimes with a superscript.  The parameter \\beta is\nnever explicitly mentioned in the text. It is not entirely clear to me\nwhat constitutes a \"position\" t in the terminology of the paper. t is a\nparameter to the LSTM output, so it seems to be the index of a\nsentence. Thus, t-1 is the preceding sentence, and p_t is the prediction\nfor this sentence. However, the description of the regularizers talks\nabout preceding words, not sentences, but still uses. My assumption here\nis that p_t is actually overloaded and may either mean the sentiment of\na sentence or a word. However, this should be made clearer in the text.\n\nOne dangerous issue in this paper is that the authors tread a fine line\nbetween regularization and distant supervision in their work. The\nproblem here is that there are many other ways to integrate lexical\ninformation from about polarity, negation information, etc. into a model\n(e.g., by putting the information into the features). The authors\ncompare against a re-run or re-implementation of Teng et al.'s NSCL\nmodel. Here, it would be important to know whether the authors used the\nsame lexicons as in their own work. If this is not the case, the\ncomparison is not fair. Also, I do not understand why the authors cannot\nrun NSCL on the MR dataset when they have access to an implementation of\nthe model. Would this not just be a matter of swapping the datasets? The\nremaining baselines do not appear to be using lexical information, which\nmakes them rather poor. I would very much like to see a vanilla LSTM run\nwhere lexical information is simply appended to the word vectors.\n\nThe authors end the paper with some helpful analysis of the\nmodels. These experiments show that the model indeed learns\nintensification and negation to some extent. In these experiments, it\nwould be interesting to know how the model behaves with\nout-of-vocabulary words (with respect to the lexicons). Does the model\nlearn beyond memorization, and does generalization happen for words that\nthe model has not seen in training? Minor remark here: the figures and\ntables are too small to be read in print.\n\nThe paper is mostly well-wri",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Linguistically Regularized LSTM for Sentiment Classification \n\nHere is the abstract of the paper: This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression. \n\nHere is the partial review: Strengths:\n\n- Innovative idea: sentiment through regularization\n- Experiments appear to be done well from a technical point of view\n- Useful in-depth analysis of the model\n\nWeaknesses:\n\n- Very close to distant supervision\n- Mostly poorly informed baselines\n\nGeneral Discussion:\n\nThis paper presents an extension of the vanilla LSTM model that\nincorporates sentiment information through regularization.  The\nintroduction presents the key claims of the paper: Previous CNN\napproaches are bad when no phrase-level supervision is present.\nPhrase-level annotation is expensive. The contribution of this paper is\ninstead a \"simple model\" using other linguistic resources.\n\nThe related work section provides a good review of sentiment\nliterature. However, there is no mention of previous attempts at\nlinguistic regularization (e.g., [YOG14]).\n\nThe explanation of the regularizers in section 4 is rather lengthy and\nrepetitive. The listing on p. 3 could very well be merged with the\nrespective subsection 4.1-4.4. Notation in this section is inconsistent\nand generally hard to follow. Most notably, p is sometimes used with a\nsubscript and sometimes with a superscript.  The parameter \\beta is\nnever explicitly mentioned in the text. It is not entirely clear to me\nwhat constitutes a \"position\" t in the terminology of the paper. t is a\nparameter to the LSTM output, so it seems to be the index of a\nsentence. Thus, t-1 is the preceding sentence, and p_t is the prediction\nfor this sentence. However, the description of the regularizers talks\nabout preceding words, not sentences, but still uses. My assumption here\nis that p_t is actually overloaded and may either mean the sentiment of\na sentence or a word. However, this should be made clearer in the text.\n\nOne dangerous issue in this paper is that the authors tread a fine line\nbetween regularization and distant supervision in their work. The\nproblem here is that there are many other ways to integrate lexical\ninformation from about polarity, negation information, etc. into a model\n(e.g., by putting the information into the features). The authors\ncompare against a re-run or re-implementation of Teng et al.'s NSCL\nmodel. Here, it would be important to know whether the authors used the\nsame lexicons as in their own work. If this is not the case, the\ncomparison is not fair. Also, I do not understand why the authors cannot\nrun NSCL on the MR dataset when they have access to an implementation of\nthe model. Would this not just be a matter of swapping the datasets? The\nremaining baselines do not appear to be using lexical information, which\nmakes them rather poor. I would very much like to see a vanilla LSTM run\nwhere lexical information is simply appended to the word vectors.\n\nThe authors end the paper with some helpful analysis of the\nmodels. These experiments show that the model indeed learns\nintensification and negation to some extent. In these experiments, it\nwould be interesting to know how the model behaves with\nout-of-vocabulary words (with respect to the lexicons). Does the model\nlearn beyond memorization, and does generalization happen for words that\nthe model has not seen in training? Minor remark here: the figures and\ntables are too small to be read in print.\n\nThe paper is mostly well-wri \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper is mostly well-written and presents an innovative idea of sentiment through regularization. The experiments are done well technically and the in-depth analysis of the model is useful. However, there are some weaknesses in the paper. The paper is very close to distant supervision, and it would be beneficial to include a discussion on previous attempts at linguistic regularization. The explanation of the regularizers in section 4 is lengthy and repetitive, and the notation is inconsistent and hard to follow. The authors should clarify the meaning of the term 'position' and the use of the variable 't'. There is a concern regarding the fine line between regularization and distant supervision, and it is essential to understand if the authors used the same lexicons as in the comparison model for a fair evaluation. It is also suggested to include a baseline with a vanilla LSTM model where lexical information is appended to the word vectors. Furthermore, the authors could provide more analysis on how the model handles out-of-vocabulary words and generalization for words not seen in training. Finally, it would be helpful to improve the readability of the figures and tables. Overall, the paper has potential but requires some revisions and additional experiments to address the weaknesses and provide a more comprehensive evaluation.\"\n}",
        "human_end_index": 3267
    },
    {
        "id": 331,
        "source": "PeerRead/acl_2017/",
        "title": "Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps",
        "human_review": "- Strengths:\n\nDetailed guidelines and explicit illustrations.\n\n- Weaknesses:\n\nThe document-independent crowdsourcing annotation is unreliable. \n\n- General Discussion:\n\nThis work creates a new benchmark corpus for concept-map-based MDS. It is well\norganized and written clearly. The supplement materials are sufficient. I have\ntwo questions here.\n1)              Is it necessary to treat concept map extraction as a separate\ntask?\nOn\nthe one hand, many generic summarization systems build a similar knowledge\ngraph and then generate summaries accordingly. On the other hand, with the\nincrease of the node number, the concept map becomes growing hard to\ndistinguish. Thus, the general summaries should be more readable.\n2)              How can you determine the importance of a concept independent of\nthe\ndocuments? The definition of summarization is to reserve the main concepts of\ndocuments. Therefore, the importance of a concept highly depends on the\ndocuments. For example, in the given topic of coal mining accidents, assume\nthere are two concepts: A) an instance of coal mining accidents and B) a cause\nof coal mining accidents. Then, if the document describes a series of coal\nmining accidents, A is more important than B. In comparison, if the document\nexplores why coal mining accidents happen, B is more significant than A.\nTherefore, just given the topic and two concepts A&B, it is impossible to judge\ntheir relative importance.\n\nI appreciate the great effort spent by authors to build this dataset. However,\nthis dataset is more like a knowledge graph based on common sense rather than\nsummary.\nStrengths:\n\nThis paper presents an approach to creating concept maps using crowdsourcing.\nThe general ideas are interesting and the main contribution lies in the\ncollection of the dataset. As such, I imagine that the dataset will be a\nvaluable resource for further research in this field. Clearly a lot of effort\nhas gone into this work.\n\nWeaknesses:\n\nOverall I felt this paper a bit overstated in placed. As an example, the\nauthors claim a new crowdsourcing scheme as one of their contributions. This\nclaims is quite strong though and it reads more like the authors are applying\nbest practice in crowdsourcing to their work. This isn\u2019t a novel methods\nthen, it\u2019s rather a well thought and sound application of existing knowledge.\n\nSimilarly, the authors claim that they develop and present a new corpus. This\nseems true and I can see how a lot of effort was invested in its preparation,\nbut then Section 4.1 reveals that actually this is based on an existing\ndataset. \n\nThis is more a criticism of the presentation than the work though.\n\nGeneral discussion:\n\nWhere do the summary sentences come from for the crowdsource task? Aren\u2019t\nthey still quite subjective?\n\nWhere do the clusters come from? Are they part of the TAC2008b dataset? \n\nIn 4.6 expert annotators are used to create the gold standard concept maps.\nMore information is needed in this section I would say as it seems to be quite\ncrucial. How were they trained, what made them experts?",
        "partial_review": "- Strengths:\n\nDetailed guidelines and explicit illustrations.\n\n- Weaknesses:\n\nThe document-independent crowdsourcing annotation is unreliable. \n\n- General Discussion:\n\nThis work creates a new benchmark corpus for concept-map-based MDS. It is well\norganized and written clearly. The supplement materials are sufficient. I have\ntwo questions here.\n1)              Is it necessary to treat concept map extraction as a separate\ntask?\nOn\nthe one hand, many generic summarization systems build a similar knowledge\ngraph and then generate summaries accordingly. On the other hand, with the\nincrease of the node number, the concept map becomes growing hard to\ndistinguish. Thus, the general summaries should be more readable.\n2)              How can you determine the importance of a concept independent of\nthe\ndocuments? The definition of summarization is to reserve the main concepts of\ndocuments. Therefore, the importance of a concept highly depends on the\ndocuments. For example, in the given topic of coal mining accidents, assume\nthere are two concepts: A) an instance of coal mining accidents and B) a cause\nof coal mining accidents. Then, if the document describes a series of coal\nmining accidents, A is more important than B. In comparison, if the document\nexplores why coal mining accidents happen, B is more significant than A.\nTherefore, just given the topic and two concepts A&B, it is impossible to judge\ntheir relative importance.\n\nI appreciate the great effort spent by authors t",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps \n\nHere is the abstract of the paper: Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. \n\nHere is the partial review: - Strengths:\n\nDetailed guidelines and explicit illustrations.\n\n- Weaknesses:\n\nThe document-independent crowdsourcing annotation is unreliable. \n\n- General Discussion:\n\nThis work creates a new benchmark corpus for concept-map-based MDS. It is well\norganized and written clearly. The supplement materials are sufficient. I have\ntwo questions here.\n1)              Is it necessary to treat concept map extraction as a separate\ntask?\nOn\nthe one hand, many generic summarization systems build a similar knowledge\ngraph and then generate summaries accordingly. On the other hand, with the\nincrease of the node number, the concept map becomes growing hard to\ndistinguish. Thus, the general summaries should be more readable.\n2)              How can you determine the importance of a concept independent of\nthe\ndocuments? The definition of summarization is to reserve the main concepts of\ndocuments. Therefore, the importance of a concept highly depends on the\ndocuments. For example, in the given topic of coal mining accidents, assume\nthere are two concepts: A) an instance of coal mining accidents and B) a cause\nof coal mining accidents. Then, if the document describes a series of coal\nmining accidents, A is more important than B. In comparison, if the document\nexplores why coal mining accidents happen, B is more significant than A.\nTherefore, just given the topic and two concepts A&B, it is impossible to judge\ntheir relative importance.\n\nI appreciate the great effort spent by authors t \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n   \"complete_review\": \"The paper, 'Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps', presents a novel approach to multi-document summarization by generating concept map summaries. The authors have created a new corpus of concept maps using a crowdsourcing approach and provide a baseline system and evaluation protocol for further research. The strengths of the paper include detailed guidelines and explicit illustrations. However, a weakness of the work is the reliance on document-independent crowdsourcing annotation, which may introduce unreliability. \n\nIn the general discussion, I have two questions for the authors. Firstly, is it necessary to treat concept map extraction as a separate task? Considering that many generic summarization systems build similar knowledge graphs and generate summaries based on them, it may be more practical to focus on improving the readability of general summaries. Secondly, how can the importance of a concept be determined independently of the documents? Summarization aims to preserve the main concepts of the documents, and the importance of a concept highly depends on the context of the documents. Therefore, without considering the specific documents, it is challenging to judge the relative importance of concepts.\n\nOverall, I appreciate the authors' efforts in creating a benchmark corpus and providing a clear presentation of their work.\" \n}",
        "human_end_index": 1489
    },
    {
        "id": "333",
        "source": "PeerRead/acl_2017/",
        "title": "Selective Encoding for Abstractive Sentence Summarization",
        "human_review": "- Strengths:\n\nThe authors propose a selective encoding model as extension to the\nsequence-to-sequence framework for abstractive sentence summarization. The\npaper is very well written and the methods are clearly described. The proposed\nmethods are evaluated on standard benchmarks and comparison to other\nstate-of-the-art tools are presented, including significance scores. \n\n- Weaknesses:\n\nThere are some few details on the implementation and on the systems to which\nthe authors compared their work that need to be better explained. \n\n- General Discussion:\n\n* Major review:\n\n- I wonder if the summaries obtained using the proposed methods are indeed\nabstractive. I understand that the target vocabulary is build out of the words\nwhich appear in the summaries in the training data. But given the example shown\nin Figure 4, I have the impression that the summaries are rather extractive.\nThe authors should choose a better example for Figure 4 and give some\nstatistics on the number of words in the output sentences which were not\npresent in the input sentences for all test sets.\n\n- page 2, lines 266-272: I understand the mathematical difference between the\nvector hi and s, but I still have the feeling that there is a great overlap\nbetween them. Both \"represent the meaning\". Are both indeed necessary? Did you\ntrying using only one of them.\n\n- Which neural network library did the authors use for implementing the system?\nThere is no details on the implementation.\n\n- page 5, section 44: Which training data was used for each of the systems that\nthe authors compare to? Diy you train any of them yourselves?\n\n* Minor review:\n\n- page 1, line 44: Although the difference between abstractive and extractive\nsummarization is described in section 2, this could be moved to the\nintroduction section. At this point, some users might no be familiar with this\nconcept.\n\n- page 1, lines 93-96: please provide a reference for this passage: \"This\napproach achieves huge success in tasks like neural machine translation, where\nalignment between all parts of the input and output are required.\"\n\n- page 2, section 1, last paragraph: The contribution of the work is clear but\nI think the authors should emphasize that such a selective encoding model has\nnever been proposed before (is this true?). Further, the related work section\nshould be moved to before the methods section.\n\n- Figure 1 vs. Table 1: the authors show two examples for abstractive\nsummarization but I think that just one of them is enough. Further, one is\ncalled a figure while the other a table.\n\n- Section 3.2, lines 230-234 and 234-235: please provide references for the\nfollowing two passages: \"In the sequence-to-sequence machine translation (MT)\nmodel, the encoder and decoder are responsible for encoding input sentence\ninformation and decoding the sentence representation to generate an output\nsentence\"; \"Some previous works apply this framework to summarization\ngeneration tasks.\"\n\n- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n\n- page 3, lines 289-290: the sigmoid function and the element-wise\nmultiplication are not defined for the formulas in section 3.1.\n\n- page 4, first column: many elements of the formulas are not defined: b\n(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation\n15).\n\n- page 4, line 326: the readout state rt is not depicted in Figure 2\n(workflow).\n\n- Table 2: what does \"#(ref)\" mean?\n\n- Section 4.3, model parameters and training. Explain how you achieved the\nvalues to the many parameters: word embedding size, GRU hidden states, alpha,\nbeta 1 and 2, epsilon, beam size.\n\n- Page 5, line 450: remove \"the\" word in this line? \"SGD as our optimizing\nalgorithms\" instead of \"SGD as our the optimizing algorithms.\"\n\n- Page 5, beam search: please include a reference for beam search.\n\n- Figure 4: Is there a typo in the true sentence? \"council of europe again\nslams french prison conditions\" (again or against?)\n\n- typo \"supper script\" -> \"superscript\" (4 times)\n- Strengths:\n\nThe paper is very clear and well-written. It proposes a novel approach to\nabstractive sentence summarization; basically sentence compression that is not\nconstrained to having the words in the output be present in the input. \n\n- Excellent comparison with many baseline systems. \n\n- Very thorough related work. \n\n- Weaknesses:\n\nThe criticisms are very minor:\n\n- It would be best to report ROUGE F-Score for all three datasets. The reasons\nfor reporting recall on one are understandable (the summaries are all the same\nlength), but in that case you could simply report both recall and F-Score. \n\n- The Related Work should come earlier in the paper. \n\n- The paper could use some discussion of the context of the work, e.g. how the\nsummaries / compressions are intended to be used, or why they are needed. \n\n- General Discussion:\n\n- ROUGE is fine for this paper, but ultimately you would want human evaluations\nof these compressions, e.g. on readability and coherence metrics, or an\nextrinsic evaluation.\nThe paper presents a new neural approach for summarization. They build on a\nstandard encoder-decoder with attention framework but add a network that gates\nevery encoded hidden state based on summary vectors from initial encoding\nstages. Overall, the method seems to outperform standard seq2seq methods by 1-2\npoints on three different evaluation sets.\n\nOverall, the technical sections of the paper are reasonably clear. Equation 16\nneeds more explanation, I could not understand the notation. The specific\ncontribution,  the selective mechanism, seems novel and could potentially be\nused in other contexts. \n\nThe evaluation is extensive and does demonstrate consistent improvement. One\nwould imagine that adding an additional encoder layer instead of the selective\nlayer is the most reasonable baseline (given the GRU baseline uses only one\nbi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My\none concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch? \n\nThe quality of the writing, especially in the intro/abstract/related work is\nquite bad. This paper does not make a large departure from previous work, and\ntherefore a related work nearby the introduction seems more appropriate. In\nrelated work, one common good approach is highlighting similarities and\ndifferences between your work and previous work, in words before they are\npresented in equations. Simply listing works without relating them to your work\nis not that useful. Placement of the related work near the intro will allow you\nto relieve the intro of significant background detail and instead focus on more\nhigh level.",
        "partial_review": "- Strengths:\n\nThe authors propose a selective encoding model as extension to the\nsequence-to-sequence framework for abstractive sentence summarization. The\npaper is very well written and the methods are clearly described. The proposed\nmethods are evaluated on standard benchmarks and comparison to other\nstate-of-the-art tools are presented, including significance scores. \n\n- Weaknesses:\n\nThere are some few details on the implementation and on the systems to which\nthe authors compared their work that need to be better explained. \n\n- General Discussion:\n\n* Major review:\n\n- I wonder if the summaries obtained using the proposed methods are indeed\nabstractive. I understand that the target vocabulary is build out of the words\nwhich appear in the summaries in the training data. But given the example shown\nin Figure 4, I have the impression that the summaries are rather extractive.\nThe authors should choose a better example for Figure 4 and give some\nstatistics on the number of words in the output sentences which were not\npresent in the input sentences for all test sets.\n\n- page 2, lines 266-272: I understand the mathematical difference between the\nvector hi and s, but I still have the feeling that there is a great overlap\nbetween them. Both \"represent the meaning\". Are both indeed necessary? Did you\ntrying using only one of them.\n\n- Which neural network library did the authors use for implementing the system?\nThere is no details on the implementation.\n\n- page 5, section 44: Which training data was used for each of the systems that\nthe authors compare to? Diy you train any of them yourselves?\n\n* Minor review:\n\n- page 1, line 44: Although the difference between abstractive and extractive\nsummarization is described in section 2, this could be moved to the\nintroduction section. At this point, some users might no be familiar with this\nconcept.\n\n- page 1, lines 93-96: please provide a reference for this passage: \"This\napproach achieves huge success in tasks like neural machine translation, where\nalignment between all parts of the input and output are required.\"\n\n- page 2, section 1, last paragraph: The contribution of the work is clear but\nI think the authors should emphasize that such a selective encoding model has\nnever been proposed before (is this true?). Further, the related work section\nshould be moved to before the methods section.\n\n- Figure 1 vs. Table 1: the authors show two examples for abstractive\nsummarization but I think that just one of them is enough. Further, one is\ncalled a figure while the other a table.\n\n- Section 3.2, lines 230-234 and 234-235: please provide references for the\nfollowing two passages: \"In the sequence-to-sequence machine translation (MT)\nmodel, the encoder and decoder are responsible for encoding input sentence\ninformation and decoding the sentence representation to generate an output\nsentence\"; \"Some previous works apply this framework to summarization\ngeneration tasks.\"\n\n- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n\n- page 3, lines 289-290: the sigmoid function and the element-wise\nmultiplication are not defined for the formulas in section 3.1.\n\n- page 4, first column: many elements of the formulas are not defined: b\n(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation\n15).\n\n- page 4, line 326: the readout stat",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Selective Encoding for Abstractive Sentence Summarization \n\nHere is the abstract of the paper: We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models. \n\nHere is the partial review: - Strengths:\n\nThe authors propose a selective encoding model as extension to the\nsequence-to-sequence framework for abstractive sentence summarization. The\npaper is very well written and the methods are clearly described. The proposed\nmethods are evaluated on standard benchmarks and comparison to other\nstate-of-the-art tools are presented, including significance scores. \n\n- Weaknesses:\n\nThere are some few details on the implementation and on the systems to which\nthe authors compared their work that need to be better explained. \n\n- General Discussion:\n\n* Major review:\n\n- I wonder if the summaries obtained using the proposed methods are indeed\nabstractive. I understand that the target vocabulary is build out of the words\nwhich appear in the summaries in the training data. But given the example shown\nin Figure 4, I have the impression that the summaries are rather extractive.\nThe authors should choose a better example for Figure 4 and give some\nstatistics on the number of words in the output sentences which were not\npresent in the input sentences for all test sets.\n\n- page 2, lines 266-272: I understand the mathematical difference between the\nvector hi and s, but I still have the feeling that there is a great overlap\nbetween them. Both \"represent the meaning\". Are both indeed necessary? Did you\ntrying using only one of them.\n\n- Which neural network library did the authors use for implementing the system?\nThere is no details on the implementation.\n\n- page 5, section 44: Which training data was used for each of the systems that\nthe authors compare to? Diy you train any of them yourselves?\n\n* Minor review:\n\n- page 1, line 44: Although the difference between abstractive and extractive\nsummarization is described in section 2, this could be moved to the\nintroduction section. At this point, some users might no be familiar with this\nconcept.\n\n- page 1, lines 93-96: please provide a reference for this passage: \"This\napproach achieves huge success in tasks like neural machine translation, where\nalignment between all parts of the input and output are required.\"\n\n- page 2, section 1, last paragraph: The contribution of the work is clear but\nI think the authors should emphasize that such a selective encoding model has\nnever been proposed before (is this true?). Further, the related work section\nshould be moved to before the methods section.\n\n- Figure 1 vs. Table 1: the authors show two examples for abstractive\nsummarization but I think that just one of them is enough. Further, one is\ncalled a figure while the other a table.\n\n- Section 3.2, lines 230-234 and 234-235: please provide references for the\nfollowing two passages: \"In the sequence-to-sequence machine translation (MT)\nmodel, the encoder and decoder are responsible for encoding input sentence\ninformation and decoding the sentence representation to generate an output\nsentence\"; \"Some previous works apply this framework to summarization\ngeneration tasks.\"\n\n- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n\n- page 3, lines 289-290: the sigmoid function and the element-wise\nmultiplication are not defined for the formulas in section 3.1.\n\n- page 4, first column: many elements of the formulas are not defined: b\n(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation\n15).\n\n- page 4, line 326: the readout stat \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The authors propose a selective encoding model as an extension to the sequence-to-sequence framework for abstractive sentence summarization. The paper is very well written and the methods are clearly described. The proposed methods are evaluated on standard benchmarks, and comparison to other state-of-the-art tools is presented, including significance scores. However, there are some details on the implementation and the comparison systems that need to be better explained. \n\nBased on Figure 4, it seems that the summaries obtained using the proposed methods may be more extractive than abstractive. It would be helpful if the authors provided some statistics on the number of words in the output sentences that were not present in the input sentences for all test sets. Additionally, it is not clear if both the vector hi and s are necessary, as they both represent the meaning. Exploring the impact of using just one of them would be beneficial.\n\nThe authors should provide more details on the neural network library used for implementing the system. In section 4.4, it would be helpful to know which training data was used for each of the systems compared and if any of them were trained by the authors themselves.\n\nIn terms of minor review points, it would be useful to move the explanation of the difference between abstractive and extractive summarization to the introduction section. Additionally, a reference should be provided for the statement on page 1, lines 93-96. It would be beneficial for the authors to highlight that the selective encoding model proposed in this work is novel and has not been proposed before. Moreover, the related work section should be moved before the methods section.\n\nRegarding the figures, it would be more concise to include only one example for abstractive summarization. Furthermore, the authors should ensure consistent naming, as one is referred to as a figure and the other as a table.\n\nThe authors should provide references for the passages on page 2, section 1, last paragraph and section 3.2, lines 230-234 and 234-235. It is unclear what \"MLP\" refers to in Figure 2, and it is missing from the paper. The sigmoid function and element-wise multiplication should be clearly defined in section 3.1, lines 289-290. Additionally, many elements of the formulas on page 4 are not defined, such as b, W, U, and V.\n\nOverall, the paper presents a valuable contribution with the proposed selective encoding model for abstractive sentence summarization. However, addressing the mentioned weaknesses and clarifying the minor points will significantly enhance the quality of the paper.\"\n}",
        "human_end_index": 3343
    },
    {
        "id": "335",
        "source": "PeerRead/acl_2017/",
        "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
        "human_review": "This work describes a gated attention-based recurrent neural network method for\nreading comprehension and question answering. This method employs a\nself-matching attention technique to counterbalance the limited context\nknowledge of gated attention-based recurrent neural networks when processing\npassages. Finally, authors use pointer networks  with signals from the question\nattention-based vector to predict the beginning and ending of the answer.\nExperimental results with the SQuAD dataset offer state-of-the-art performance\ncompared with several recent approaches. \n\nThe paper is well-written, structured and explained. As far as I know, the\nmathematics look also good. In my opinion, this is a very interesting work\nwhich may be useful for the question answering community.\n\nI was wondering if the authors have plans to release the code of this approach.\nFrom that perspective, I miss a bit of information about the technology used\nfor the implementation (theano, CUDA, CuDNN...), which may be useful for\nreaders.\n\nI would appreciate if authors could perform a test of statistical significance\nof the results. That would highlight even more the quality of your results.\n\nFinally, I know that the space may be a constraint, but an evaluation including\nsome additional dataset would validate more your work.\nThis paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurrent networks to determine the\nimportance of passage parts and attend to the ones relevant to the question.\nHere they use word as well as character embeddings to handle OOV words.\nOverall, this component is inspired from Wang and Jiang 2016.\n\n(b) Then the paper proposes a self-matching attention mechanism to improve the\nrepresentation for the question and passage by looking at wider passage context\nnecessary to infer the answer. This component is completely novel in the paper.\n\n(c) At the output layer, the paper uses pointer networks to locate answer\nboundaries. This is also inspired from Wang and Jiang 2016\n\nOverall, I like the paper and think that it makes a nice contribution.\n\n- Strengths:\n\nThe paper clearly breaks the network into three component for descriptive\npurposes, relates each of them to prior work and mentions its novelties with\nrespect to them. It does a sound empirical analysis by describing the impact of\neach component by doing an ablation study. This is appreciated.\n\nThe results are impressive!\n\n- Weaknesses:\n\nThe paper describes the results on a single model and an ensemble model. I\ncould not find any details of the ensemble and how was it created. I believe it\nmight be the ensemble of the character based and word based model. Can the\nauthors please describe this in the rebuttal and the paper.\n\n- General Discussion:\n\nAlong with the ablation study, it would be nice if we can have a\nqualitative analysis describing some example cases where the components of\ngating, character embedding, self embedding, etc. become crucial ... where a\nsimple model doesn't get the question right but adding one or more of these\ncomponents helps. This can go in some form of appendix or supplementary.",
        "partial_review": "This work describes a gated attention-based recurrent neural network method for\nreading comprehension and question answering. This method employs a\nself-matching attention technique to counterbalance the limited context\nknowledge of gated attention-based recurrent neural networks when processing\npassages. Finally, authors use pointer networks  with signals from the question\nattention-based vector to predict the beginning and ending of the answer.\nExperimental results with the SQuAD dataset offer state-of-the-art performance\ncompared with several recent approaches. \n\nThe paper is well-written, structured and explained. As far as I know, the\nmathematics look also good. In my opinion, this is a very interesting work\nwhich may be useful for the question answering community.\n\nI was wondering if the authors have plans to release the code of this approach.\nFrom that perspective, I miss a bit of information about the technology used\nfor the implementation (theano, CUDA, CuDNN...), which may be useful for\nreaders.\n\nI would appreciate if authors could perform a test of statistical significance\nof the results. That would highlight even more the quality of your results.\n\nFinally, I know that the space may be a constraint, but an evaluation including\nsome additional dataset would validate more your work.\nThis paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurrent networks to determine the\nimportance of passage parts and attend to the ones releva",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Gated Self-Matching Networks for Reading Comprehension and Question Answering \n\nHere is the abstract of the paper: In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model. \n\nHere is the partial review: This work describes a gated attention-based recurrent neural network method for\nreading comprehension and question answering. This method employs a\nself-matching attention technique to counterbalance the limited context\nknowledge of gated attention-based recurrent neural networks when processing\npassages. Finally, authors use pointer networks  with signals from the question\nattention-based vector to predict the beginning and ending of the answer.\nExperimental results with the SQuAD dataset offer state-of-the-art performance\ncompared with several recent approaches. \n\nThe paper is well-written, structured and explained. As far as I know, the\nmathematics look also good. In my opinion, this is a very interesting work\nwhich may be useful for the question answering community.\n\nI was wondering if the authors have plans to release the code of this approach.\nFrom that perspective, I miss a bit of information about the technology used\nfor the implementation (theano, CUDA, CuDNN...), which may be useful for\nreaders.\n\nI would appreciate if authors could perform a test of statistical significance\nof the results. That would highlight even more the quality of your results.\n\nFinally, I know that the space may be a constraint, but an evaluation including\nsome additional dataset would validate more your work.\nThis paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurrent networks to determine the\nimportance of passage parts and attend to the ones releva \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"nt to the question. This is a novel contribution that enhances the representation of the passage based on the question.\\n\\n(b) The paper proposes a self-matching attention mechanism that allows the passage to be matched against itself, effectively encoding information from the entire passage. This technique addresses the limitation of the gated attention-based recurrent network in terms of capturing context knowledge. The self-matching attention mechanism is well-explained and contributes to improving the performance of the model.\\n\\n(c) The authors employ pointer networks to accurately locate the positions of answers from the passages. This is a suitable choice for the task at hand, as it allows the model to directly output the answer spans instead of selecting from a predefined set of answers.\\n\\nOverall, the paper is well-written and the proposed approach is explained in a clear manner. The experiments conducted on the SQuAD dataset demonstrate state-of-the-art performance, with the single model achieving 71.3% in exact match on the hidden test set and the ensemble model further improving the results to 75.9%. It is notable that the proposed model holds the first place on the SQuAD leaderboard for both single and ensemble models at the time of submission. The paper would benefit from the authors releasing the code of their approach and providing information about the technology used for implementation. Additionally, performing a statistical significance test on the results would add further credibility to the findings. Finally, expanding the evaluation to include additional datasets would strengthen the validation of the proposed work.\" \n}",
        "human_end_index": 1749
    },
    {
        "id": "338",
        "source": "PeerRead/acl_2017/",
        "title": "Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors",
        "human_review": "- Strengths:\n     - The related work is quite thorough and the comparison with the approach\npresented in this paper makes the hypothesis of the paper stronger. The\nevaluation section is also extensive and thus, the experiments are convincing.\n\n- Weaknesses:\n     - In Section 3 it is not clear what is exactly the dataset that you used\nfor training the SVM and your own model. Furthermore, you only give the\nstarting date for collecting the testing data, but there is no other\ninformation related to the size of the dataset or the time frame when the data\nwas collected. This might also give some insight for the results and statistics\ngiven in Section 3.2.\n     - In Table 3 we can see that the number of reviewers is only slightly\nlower than the number of reviews posted (at least for hotels), which means that\nonly a few reviewers posted more than one review, in the labeled dataset. How\ndoes this compare with the full dataset in Table 2? What is the exact number of\nreviewers in Table 2 (to know what is the percentage of labeled reviewers)? It\nis also interesting to know how many reviews are made by one person on average.\nIf there are only a few reviewers that post more than one review (i.e., not\nthat much info to learn from), the results would benefit from a thorough\ndiscussion. \n\n- General Discussion:\n     This paper focuses on identifying spam reviews under the assumption that\nwe deal with a cold-start problem, i.e., we do not have enough information to\ndraw a conclusion. The paper proposes a neural network model that learns how to\nrepresent new reviews by jointly using embedded textual information and\nbehaviour information. Overall, the paper is very well written and the results\nare compelling.\n\n- Typos and/or grammar:                                 \n     - The new reviewer only provide us                                        \n\n     - Jindal and Liu (2008) make the first step -> the work is quite old, you\ncould use past tense to refer to it\n     - Usage of short form \u201ccan\u2019t\u201d, \u201ccouldn\u2019t\u201d, \u201cwhat\u2019s\u201d\ninstead of the prefered long form\n     - The following sentence is not clear and should be rephrased: \u201cThe new\nreviewer just posted one review and we have to filter it out immediately, there\nis not any historical reviews provided to us.\u201c\nThis paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively and quantitatively analyze the cold-start problem.\nThey observe that there is no enough prior data from a new user in this\nrealistic scenario. The traditional features fail to help to identify review\nspam. Instead, they turn to rely on the abundant textual and behavioral\ninformation of the existing reviewer to augment the information of a new user.\nIn specific, they propose a neural network to represent the review of the new\nreviewer with the learnt word embedding and jointly encoded behavioral\ninformation. In the experiments, the authors make comparisons with traditional\nmethods, and show the effectiveness of their model.\n\n- Strengths:\n\nThe paper is well organized and clearly written. The idea of jointly encoding\ntexts and behaviors is interesting. The cold-start problem is actually an\nurgent problem to several online review analysis applications. In my knowledge,\nthe previous work has not yet attempted to tackle this problem. This paper is\nmeaningful and presents a reasonable analysis. And the results of the proposed\nmodel can also be available for downstream detection models.\n\n- Weaknesses:\n\nIn experiments, the author set the window width of the filters in the CNN\nmodule to 2. Did the author try other window widths, for example width `1' to\nextract unigram features, `3' to trigram, or use them together? \nThe authors may add more details about the previous work in the related work\nsection. More specifically description would help the readers to understand the\ntask clearly.\n\nThere are also some typos to be corrected:\nSec 1: ``...making purchase decision...'' should be ``making a/the purchase\ndecision''\nSec 1: ``...are devoted to explore... '' should be `` are devoted to\nexploring''\nSec 1: ``...there is on sufficient behaviors...'' should be \u201cthere are no\nsufficient behaviors''\nSec 1: ``...on business trip...'' should be ``on a business trip''\nSec 1: ``...there are abundant behavior information...'' should be ``there is\nabundant behavior''\nSec 3: ``The new reviewer only provide us...'' should be ``...The new reviewer\nonly provides us...''\nSec 3: ``...features need not to take much...'' should be ``...features need\nnot take much...''\nSec 4: ``...there is not any historical reviews...'' should be ``...there are\nnot any historical reviews...''\nSec 4: ``...utilizing a embedding learning model...'' should be ``...utilizing\nan embedding learning model...''\nSec 5.2 ``...The experiment results proves...'' should be ``...The experiment\nresults prove...''\n\n- General Discussion:\n\nIt is a good paper and should be accepted by ACL.",
        "partial_review": "- Strengths:\n     - The related work is quite thorough and the comparison with the approach\npresented in this paper makes the hypothesis of the paper stronger. The\nevaluation section is also extensive and thus, the experiments are convincing.\n\n- Weaknesses:\n     - In Section 3 it is not clear what is exactly the dataset that you used\nfor training the SVM and your own model. Furthermore, you only give the\nstarting date for collecting the testing data, but there is no other\ninformation related to the size of the dataset or the time frame when the data\nwas collected. This might also give some insight for the results and statistics\ngiven in Section 3.2.\n     - In Table 3 we can see that the number of reviewers is only slightly\nlower than the number of reviews posted (at least for hotels), which means that\nonly a few reviewers posted more than one review, in the labeled dataset. How\ndoes this compare with the full dataset in Table 2? What is the exact number of\nreviewers in Table 2 (to know what is the percentage of labeled reviewers)? It\nis also interesting to know how many reviews are made by one person on average.\nIf there are only a few reviewers that post more than one review (i.e., not\nthat much info to learn from), the results would benefit from a thorough\ndiscussion. \n\n- General Discussion:\n     This paper focuses on identifying spam reviews under the assumption that\nwe deal with a cold-start problem, i.e., we do not have enough information to\ndraw a conclusion. The paper proposes a neural network model that learns how to\nrepresent new reviews by jointly using embedded textual information and\nbehaviour information. Overall, the paper is very well written and the results\nare compelling.\n\n- Typos and/or grammar:                                 \n     - The new reviewer only provide us                                        \n\n     - Jindal and Liu (2008) make the first step -> the work is quite old, you\ncould use past tense to refer to it\n     - Usage of short form \u201ccan\u2019t\u201d, \u201ccouldn\u2019t\u201d, \u201cwhat\u2019s\u201d\ninstead of the prefered long form\n     - The following sentence is not clear and should be rephrased: \u201cThe new\nreviewer just posted one review and we have to filter it out immediately, there\nis not any historical reviews provided to us.\u201c\nThis paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively and quantitatively analyze the cold-start problem.\nThey observe that there is no enough prior data from a new user in ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors \n\nHere is the abstract of the paper: Solving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers' review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way. \n\nHere is the partial review: - Strengths:\n     - The related work is quite thorough and the comparison with the approach\npresented in this paper makes the hypothesis of the paper stronger. The\nevaluation section is also extensive and thus, the experiments are convincing.\n\n- Weaknesses:\n     - In Section 3 it is not clear what is exactly the dataset that you used\nfor training the SVM and your own model. Furthermore, you only give the\nstarting date for collecting the testing data, but there is no other\ninformation related to the size of the dataset or the time frame when the data\nwas collected. This might also give some insight for the results and statistics\ngiven in Section 3.2.\n     - In Table 3 we can see that the number of reviewers is only slightly\nlower than the number of reviews posted (at least for hotels), which means that\nonly a few reviewers posted more than one review, in the labeled dataset. How\ndoes this compare with the full dataset in Table 2? What is the exact number of\nreviewers in Table 2 (to know what is the percentage of labeled reviewers)? It\nis also interesting to know how many reviews are made by one person on average.\nIf there are only a few reviewers that post more than one review (i.e., not\nthat much info to learn from), the results would benefit from a thorough\ndiscussion. \n\n- General Discussion:\n     This paper focuses on identifying spam reviews under the assumption that\nwe deal with a cold-start problem, i.e., we do not have enough information to\ndraw a conclusion. The paper proposes a neural network model that learns how to\nrepresent new reviews by jointly using embedded textual information and\nbehaviour information. Overall, the paper is very well written and the results\nare compelling.\n\n- Typos and/or grammar:                                 \n     - The new reviewer only provide us                                        \n\n     - Jindal and Liu (2008) make the first step -> the work is quite old, you\ncould use past tense to refer to it\n     - Usage of short form \u201ccan\u2019t\u201d, \u201ccouldn\u2019t\u201d, \u201cwhat\u2019s\u201d\ninstead of the prefered long form\n     - The following sentence is not clear and should be rephrased: \u201cThe new\nreviewer just posted one review and we have to filter it out immediately, there\nis not any historical reviews provided to us.\u201c\nThis paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively and quantitatively analyze the cold-start problem.\nThey observe that there is no enough prior data from a new user in  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Section 3 of the paper lacks clarity about the dataset used for training the SVM and the proposed model. It would be helpful to include information about the size of the dataset and the time frame when the data was collected. Additionally, it would be insightful to compare the number of reviewers in the labeled dataset (Table 3) with the full dataset (Table 2) to understand the percentage of labeled reviewers. The paper could also provide information on the average number of reviews made by one person to assess the amount of information available for learning. Regarding the typos and grammar, there are a few instances that need attention, such as the use of short forms and unclear phrasing in a sentence. Overall, the paper is well-written with a thorough analysis of the cold-start problem and convincing experimental results.\"\n}",
        "human_end_index": 2490
    },
    {
        "id": "343",
        "source": "PeerRead/acl_2017/",
        "title": "Neural Word Segmentation with Rich Pretraining",
        "human_review": "- Strengths:\ni. Well organized and easy to understand\nii. Provides detailed comparisons under various experimental settings and shows\nthe state-of-the-art performances\n\n- Weaknesses:\ni. In experiments, this paper compares previous supervised approaches, but the\nproposed method is the semi-supervised approach even if the training data is\nenough to train.\n\n- General Discussion:\nThis paper adopts a pre-training approach to improve Chinese word segmentation.\nBased on the transition-based neural word segmentation, this paper aims to\npre-train incoming characters with external resources (punctuation, soft\nsegmentation, POS, and heterogeneous training data) through multi-task\nlearning. That is, this paper casts each external source as an auxiliary\nclassification task. The experimental results show that the proposed method\nachieves the state-of-the-art performances in six out of seven datasets.\u00a0\n\nThis paper is well-written and easy to understand. A number of experiments\nprove the effectiveness of the proposed method. However, there exist an issue\nin this paper. The proposed method is a semi-supervised learning that uses\nexternal resources to pre-train the characters. Furthermore, this paper uses\nanother heterogeneous training datasets even if it uses the datasets only for\npre-training. Nevertheless, the baselines in the experiments are based on\nsupervised learning. In general, the performance of semi-supervised learning is\nbetter than that of supervised learning because semi-supervised learning makes\nuse of plentiful auxiliary information. In the experiments, this paper should\nhave compared the proposed method with semi-supervised approaches.\n\nPOST AUTHOR RESPONSE\n\nWhat the reviewer concerned is that this paper used additional\n\u201cgold-labeled\u201d dataset to pretrain the character embeddings. Some baselines\nin the experiments used label information, where the labels are predicted\nautomatically by their base models as the authors pointed out. When insisting\nsuperiority of a method, all circumstances should be same. Thus, even if the\ngold dataset isn\u2019t used to train the segmentation model directly, it seems to\nme that it is an unfair comparison because the proposed method used another\n\u201cgold\u201d dataset to train the character embeddings.",
        "partial_review": "- Strengths:\ni. Well organized and easy to understand\nii. Provides detailed comparisons under various experimental settings and shows\nthe state-of-the-art performances\n\n- Weaknesses:\ni. In experiments, this paper compares previous supervised approaches, but the\nproposed method is the semi-supervised approach even if the training data is\nenough to train.\n\n- General Discussion:\nThis paper adopts a pre-training approach to improve Chinese word segmentation.\nBased on the transition-based neural word segmentation, this paper aims to\npre-train incoming characters with external resources (punctuation, soft\nsegmentation, POS, and heterogeneous training data) through multi-task\nlearning. That is, this paper casts each external source as an auxiliary\nclassification task. The experimental results show that the proposed method\nachieves the state-of-the-art performances in six out of seven datasets.\u00a0\n\nThis paper is well-written and easy to understand. A number of experiments\nprove the effectiveness of the proposed method. However, there exist an issue\nin this paper. The proposed method is a semi-supervised learning that uses\nexternal resources to pre-train t",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Neural Word Segmentation with Rich Pretraining \n\nHere is the abstract of the paper: Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks. \n\nHere is the partial review: - Strengths:\ni. Well organized and easy to understand\nii. Provides detailed comparisons under various experimental settings and shows\nthe state-of-the-art performances\n\n- Weaknesses:\ni. In experiments, this paper compares previous supervised approaches, but the\nproposed method is the semi-supervised approach even if the training data is\nenough to train.\n\n- General Discussion:\nThis paper adopts a pre-training approach to improve Chinese word segmentation.\nBased on the transition-based neural word segmentation, this paper aims to\npre-train incoming characters with external resources (punctuation, soft\nsegmentation, POS, and heterogeneous training data) through multi-task\nlearning. That is, this paper casts each external source as an auxiliary\nclassification task. The experimental results show that the proposed method\nachieves the state-of-the-art performances in six out of seven datasets.\u00a0\n\nThis paper is well-written and easy to understand. A number of experiments\nprove the effectiveness of the proposed method. However, there exist an issue\nin this paper. The proposed method is a semi-supervised learning that uses\nexternal resources to pre-train t \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper is well-written and easy to understand. A number of experiments prove the effectiveness of the proposed method. However, there exist an issue in this paper. The proposed method is a semi-supervised learning that uses external resources to pre-train the model, but the experiments compare it with previous supervised approaches. It would be more appropriate to compare the proposed method with other semi-supervised approaches, as they share similar characteristics in utilizing external resources for training. Overall, the paper presents a well-organized study and provides detailed comparisons, showcasing state-of-the-art performances in six out of seven datasets.\"\n}",
        "human_end_index": 1163
    },
    {
        "id": "350",
        "source": "PeerRead/acl_2017/",
        "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
        "human_review": "- Strengths:\n\nImproves over the state-of-the-art. Method might be applicable for other\ndomains.\n\n- Weaknesses:\n\nNot much novelty in method.  Not quite clear if data set is general enough for\nother domains.\n\n- General Discussion:\n\nThis paper describes a rule-based method for generating additional\nweakly labeled data for event extraction.  The method has three main\nstages.  First, it uses Freebase to find important slot fillers\nfor matching sentences in Wikipedia (using all slot fillers is too\nstringent resulting in too few matches).  Next, it uses FrameNet to\nto improve reliability of labeling trigger verbs and to find nominal\ntriggers.  Lastly, it uses a multi-instance learning to deal with\nthe noisily generated training data.\n\nWhat I like about this paper is that it improves over the\nstate-of-the-art on a non-trival benchmark.  The rules involved\ndon't seem too obfuscated, so I think it might be useful for the\npractitioner who is interested to improve IE systems for other domains.  On\nthe other hand, some some manual effort is still needed, for example for\nmapping Freebase\nevent types to ACE event types (as written in Section 5.3 line 578).  This also\nmakes it difficult for future work to calibrate apple-to-apple against this\npaper.              Apart\nfrom this, the method also doesn't seem too novel.\n\nOther comments:\n\n- I'm also concern with the generalizability of this method to other\n  domains.  Section 2 line 262 says that 21 event types are selected\n  from Freebase.  How are they selected?  What is the coverage on the 33 event\ntypes\nin the ACE data.\n\n- The paper is generally well-written although I have some\n  suggestions for improvement.              Section 3.1 line 316 uses \"arguments\nliked time, location...\".  If you mean roles or arguments, or maybe\nyou want to use actual realizations of time and location as\nexamples.  There are minor typos, for e.g. line 357 is missing a\n\"that\", but this is not a major concern I have for this paper.",
        "partial_review": "- Strengths:\n\nImproves over the state-of-the-art. Method might be applicable for other\ndomains.\n\n- Weaknesses:\n\nNot much novelty in method.  Not quite clear if data set is general enough for\nother domains.\n\n- General Discussion:\n\nThis paper describes a rule-based method for generating additional\nweakly labeled data for event extraction.  The method has three main\nstages.  First, it uses Freebase to find important slot fillers\nfor matching sentences in Wikipedia (using all slot fillers is too\nstringent resulting in too few matches).  Next, it uses FrameNet to\nto improve reliability of labeling trigger verbs and to find nominal\ntriggers.  Lastly, it uses a multi-instance learning to deal with\nthe noisily generated training data.\n\nWhat I like about this paper is that it improves over the\nstate-of-the-art on a non-trival benchmark.  The rules involved\ndon't seem too obfuscated, so I think it might be useful for the\npractitioner who is interested to improve IE systems",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Automatically Labeled Data Generation for Large Scale Event Extraction \n\nHere is the abstract of the paper: Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data. \n\nHere is the partial review: - Strengths:\n\nImproves over the state-of-the-art. Method might be applicable for other\ndomains.\n\n- Weaknesses:\n\nNot much novelty in method.  Not quite clear if data set is general enough for\nother domains.\n\n- General Discussion:\n\nThis paper describes a rule-based method for generating additional\nweakly labeled data for event extraction.  The method has three main\nstages.  First, it uses Freebase to find important slot fillers\nfor matching sentences in Wikipedia (using all slot fillers is too\nstringent resulting in too few matches).  Next, it uses FrameNet to\nto improve reliability of labeling trigger verbs and to find nominal\ntriggers.  Lastly, it uses a multi-instance learning to deal with\nthe noisily generated training data.\n\nWhat I like about this paper is that it improves over the\nstate-of-the-art on a non-trival benchmark.  The rules involved\ndon't seem too obfuscated, so I think it might be useful for the\npractitioner who is interested to improve IE systems \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a rule-based method for automatically labeling training data for event extraction. The method utilizes world knowledge and linguistic knowledge to detect key arguments and trigger words for each event type, and applies these to label events in texts automatically. The experimental results demonstrate that the quality of the automatically labeled data is competitive with human-labeled data, and the incorporation of both types of data improves model performance. The paper does a good job of describing the methodology, and the rules involved are not overly complex, making the approach accessible to practitioners interested in improving information extraction systems. One potential weakness is the lack of novelty in the method, as similar approaches have been proposed in the past. Additionally, it is not clear if the dataset used in the experiments is general enough to be applicable to other domains. Further clarification on these points would enhance the contribution of the paper. Overall, this work is a significant step towards tackling the data labeling problem for event extraction and provides valuable insights for future research in this area.\"\n}",
        "human_end_index": 977
    },
    {
        "id": "365",
        "source": "PeerRead/acl_2017/",
        "title": "Learning attention for historical text normalization by learning to pronounce",
        "human_review": "[update after reading author response: the alignment of the hidden units does\nnot match with my intuition and experience, but I'm willing to believe I'm\nwrong in this case.  Discussing the alignment in the paper is important (and\nmaybe just sanity-checking that the alignment goes away if you initialize with\na different seed).  If what you're saying about how the new model is very\ndifferent but only a little better performing -- a 10% error reduction -- then\nI wonder about an ensemble of the new model and the old one.  Seems like\nensembling would provide a nice boost if the failures across models are\ndistinct, right?  Anyhow this is a solid paper and I appreciate the author\nresponse, I raise my review score to a 4.]\n\n- Strengths:\n\n  1)  Evidence of the attention-MTL connection is interesting\n\n  2)  Methods are appropriate, models perform well relative to state-of-the-art\n\n- Weaknesses:\n\n  1)  Critical detail is not provided in the paper\n\n  2)  Models are not particularly novel\n\n- General Discussion:\n\nThis paper presents a new method for historical text normalization.  The model\nperforms well, but the primary contribution of the paper ends up being a\nhypothesis that attention mechanisms in the task can be learned via multi-task\nlearning, where the auxiliary task is a pronunciation task.  This connection\nbetween attention and MTL is interesting.\n\nThere are two major areas for improvement in this paper.  The first is that we\nare given almost no explanation as to why the pronunciation task would somehow\nrequire an attention mechanism similar to that used for the normalization task.\n Why the two tasks (normalization and pronunciation) are related is mentioned\nin the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism\n(and in fact, one that is then only hampered by the inclusion of an explicit\nattention mechanism?).                    This remains a mystery.  The paper can\nleave some\nquestions unanswered, but at least a suggestion of an answer to this one would\nstrengthen the paper.\n\nThe other concern is clarity.  While the writing in this paper is clear, a\nnumber of details are omitted.                    The most important one is the\ndescription\nof\nthe attention mechanism itself.  Given the central role that method plays, it\nshould be described in detail in the paper rather than referring to previous\nwork.  I did not understand the paragraph about this in Sec 3.4.\n\nOther questions included why you can compare the output vectors of two models\n(Figure 4), while the output dimensions are the same I don't understand why the\nhidden layer dimensions of two models would ever be comparable.  Usually how\nthe hidden states are \"organized\" is completely different for every model, at\nthe very least it is permuted.                    So I really did not understand\nFigure 4.\n\nThe Kappa statistic for attention vs. MTL needs to be compared to the same\nstatistic for each of those models vs. the base model.\n\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\n\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL\napproaches make large changes to the model (comparing e.g. Fig 5) but the\nexperimental improvements in accuracy for either model are quite small (2%),\nwhich seems like a bit of a contradiction.\nSummary:\n\nThe paper applies a sequence to sequence (seq2seq) approach for German\nhistorical text normalization, and showed that using a grapheme-to-phoneme\ngeneration as an auxiliary task in a multi-task learning (MTL) seq2seq\nframework improves performance. The authors argue that the MTL approach\nreplaces the need for an attention menchanism, showing experimentally that the\nattention mechanism harms the MTL performance. The authors also tried to show\nstatistical correlation between the weights of an MTL normalizer and an\nattention-based one.\n\nStrengths:\n\n1) Novel application of seq2seq to historical text correction, although it has\nbeen applied recently to sentence grammatical error identification [1]. \n\n2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting\nimproves text normalization accuracy.\n\nWeaknesses:\n\n1) Instead of arguing that the MTL approach replaces the attention mechanism, I\nthink the authors should investigate why attention did not work on MTL, and\nperhaps modify the attention mechanism so that it would not harm performance.\n\n2) I think the authors should reference past seq2seq MTL work, such as [2] and\n[3]. The MTL work in [2] also worked on non-attention seq2seq models.\n\n3) This paper only tested on one German historical text data set of 44\ndocuments. It would be interesting if the authors can evaluate the same\napproach in another language or data set.\n\nReferences:\n\n[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.\nSentence-level grammatical error identification as sequence-to-sequence\ncorrection. In Proceedings of the 11th Workshop on Innovative Use of NLP for\nBuilding Educational Applications.\n\n[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz\nKaiser. Multi-task Sequence to Sequence Learning. ICLR\u201916. \n\n[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. \nMulti-task learning for multiple language translation. ACL'15\n\n---------------------------\nHere is my reply to the authors' rebuttal:\n\nI am keeping my review score of 3, which means I do not object to accepting the\npaper. However, I am not raising my score for 2 reasons:\n\n* the authors did not respond to my questions about other papers on seq2seq\nMTL, which also avoided using attention mechanism. So in terms of novelty, the\nmain novelty lies in applying it to text normalization.\n\n* it is always easier to show something (i.e. attention in seq2seq MTL) is not\nworking, but the value would lie in finding out why it fails and changing the\nattention mechanism so that it works.\n- Strengths: well written, solid experimental setup and intriguing qualitative\nanalysis\n\n- Weaknesses: except for the qualitative analysis, the paper may belong better\nto the applications area, since the models are not particularly new but the\napplication itself is most of its novelty\n\n- General Discussion: This paper presents a \"sequence-to-sequence\" model with\nattention mechanisms and an auxiliary phonetic prediction task to tackle\nhistorical text normalization. None of the used models or techniques are new by\nthemselves, but they seem to have never been used in this problem before,\nshowing and improvement over the state-of-the-art. \n\nMost of the paper seem like a better fit for the applications track, except for\nthe final analysis where the authors link attention with multi-task learning,\nclaiming that the two produce similar effects. The hypothesis is intriguing,\nand it's supported with a wealth of evidence, at least for the presented task. \nI do have some questions on this analysis though:\n\n1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two\nmodels are aligned? Is it safe to do so?\n\n2) Section 5.2, I don't get what you mean by the errors that each of the models\nresolve independently of each other. This is like symmetric-difference? That\nis, if we combine the two models these errors are not resolved anymore?\n\nOn a different vein, 3) Why is there no comparison with Azawi's model?\n\n========\n\nAfter reading the author's response.\n\nI'm feeling more concerned than I was before about your claims of alignment in\nthe hidden space of the two models. If accepted, I would strongly encourage the\nauthors to make clear\nin the paper the discussion you have shared with us for why you think that\nalignment holds in practice.",
        "partial_review": "[update after reading author response: the alignment of the hidden units does\nnot match with my intuition and experience, but I'm willing to believe I'm\nwrong in this case.  Discussing the alignment in the paper is important (and\nmaybe just sanity-checking that the alignment goes away if you initialize with\na different seed).  If what you're saying about how the new model is very\ndifferent but only a little better performing -- a 10% error reduction -- then\nI wonder about an ensemble of the new model and the old one.  Seems like\nensembling would provide a nice boost if the failures across models are\ndistinct, right?  Anyhow this is a solid paper and I appreciate the author\nresponse, I raise my review score to a 4.]\n\n- Strengths:\n\n  1)  Evidence of the attention-MTL connection is interesting\n\n  2)  Methods are appropriate, models perform well relative to state-of-the-art\n\n- Weaknesses:\n\n  1)  Critical detail is not provided in the paper\n\n  2)  Models are not particularly novel\n\n- General Discussion:\n\nThis paper presents a new method for historical text normalization.  The model\nperforms well, but the primary contribution of the paper ends up being a\nhypothesis that attention mechanisms in the task can be learned via multi-task\nlearning, where the auxiliary task is a pronunciation task.  This connection\nbetween attention and MTL is interesting.\n\nThere are two major areas for improvement in this paper.  The first is that we\nare given almost no explanation as to why the pronunciation task would somehow\nrequire an attention mechanism similar to that used for the normalization task.\n Why the two tasks (normalization and pronunciation) are related is mentioned\nin the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism\n(and in fact, one that is then only hampered by the inclusion of an explicit\nattention mechanism?).                    This remains a mystery.  The paper can\nleave some\nquestions unanswered, but at least a suggestion of an answer to this one would\nstrengthen the paper.\n\nThe other concern is clarity.  While the writing in this paper is clear, a\nnumber of details are omitted.                    The most important one is the\ndescription\nof\nthe attention mechanism itself.  Given the central role that method plays, it\nshould be described in detail in the paper rather than referring to previous\nwork.  I did not understand the paragraph about this in Sec 3.4.\n\nOther questions included why you can compare the output vectors of two models\n(Figure 4), while the output dimensions are the same I don't understand why the\nhidden layer dimensions of two models would ever be comparable.  Usually how\nthe hidden states are \"organized\" is completely different for every model, at\nthe very least it is permuted.                    So I really did not understand\nFigure 4.\n\nThe Kappa statistic for attention vs. MTL needs to be compared to the same\nstatistic for each of those models vs. the base model.\n\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\n\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL\napproaches make large changes to the model (comparing e.g. Fig 5) but the\nexperimental improvements in accuracy for either model are quite small (2%),\nwhich seems like a bit of a contradiction.\nSummary:\n\nThe paper applies a sequence to sequence (seq2seq) approach for German\nhistorical text normalization, and showed that using a grapheme-to-phoneme\ngeneration as an auxiliary task in a multi-task learning (MTL) seq2seq\nframework improves performance. The authors argue that the MTL approach\nreplaces the need for an attention menchanism, showing experimentally that the\nattention mechanism harms the MTL performance. The authors also ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Learning attention for historical text normalization by learning to pronounce \n\nHere is the abstract of the paper: Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works. \n\nHere is the partial review: [update after reading author response: the alignment of the hidden units does\nnot match with my intuition and experience, but I'm willing to believe I'm\nwrong in this case.  Discussing the alignment in the paper is important (and\nmaybe just sanity-checking that the alignment goes away if you initialize with\na different seed).  If what you're saying about how the new model is very\ndifferent but only a little better performing -- a 10% error reduction -- then\nI wonder about an ensemble of the new model and the old one.  Seems like\nensembling would provide a nice boost if the failures across models are\ndistinct, right?  Anyhow this is a solid paper and I appreciate the author\nresponse, I raise my review score to a 4.]\n\n- Strengths:\n\n  1)  Evidence of the attention-MTL connection is interesting\n\n  2)  Methods are appropriate, models perform well relative to state-of-the-art\n\n- Weaknesses:\n\n  1)  Critical detail is not provided in the paper\n\n  2)  Models are not particularly novel\n\n- General Discussion:\n\nThis paper presents a new method for historical text normalization.  The model\nperforms well, but the primary contribution of the paper ends up being a\nhypothesis that attention mechanisms in the task can be learned via multi-task\nlearning, where the auxiliary task is a pronunciation task.  This connection\nbetween attention and MTL is interesting.\n\nThere are two major areas for improvement in this paper.  The first is that we\nare given almost no explanation as to why the pronunciation task would somehow\nrequire an attention mechanism similar to that used for the normalization task.\n Why the two tasks (normalization and pronunciation) are related is mentioned\nin the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism\n(and in fact, one that is then only hampered by the inclusion of an explicit\nattention mechanism?).                    This remains a mystery.  The paper can\nleave some\nquestions unanswered, but at least a suggestion of an answer to this one would\nstrengthen the paper.\n\nThe other concern is clarity.  While the writing in this paper is clear, a\nnumber of details are omitted.                    The most important one is the\ndescription\nof\nthe attention mechanism itself.  Given the central role that method plays, it\nshould be described in detail in the paper rather than referring to previous\nwork.  I did not understand the paragraph about this in Sec 3.4.\n\nOther questions included why you can compare the output vectors of two models\n(Figure 4), while the output dimensions are the same I don't understand why the\nhidden layer dimensions of two models would ever be comparable.  Usually how\nthe hidden states are \"organized\" is completely different for every model, at\nthe very least it is permuted.                    So I really did not understand\nFigure 4.\n\nThe Kappa statistic for attention vs. MTL needs to be compared to the same\nstatistic for each of those models vs. the base model.\n\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\n\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL\napproaches make large changes to the model (comparing e.g. Fig 5) but the\nexperimental improvements in accuracy for either model are quite small (2%),\nwhich seems like a bit of a contradiction.\nSummary:\n\nThe paper applies a sequence to sequence (seq2seq) approach for German\nhistorical text normalization, and showed that using a grapheme-to-phoneme\ngeneration as an auxiliary task in a multi-task learning (MTL) seq2seq\nframework improves performance. The authors argue that the MTL approach\nreplaces the need for an attention menchanism, showing experimentally that the\nattention mechanism harms the MTL performance. The authors also  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a new method for historical text normalization using a multi-task learning (MTL) approach. The model performs well and achieves a 2% increase in performance compared to the state-of-the-art. The primary contribution of the paper is the hypothesis that MTL can learn attention mechanisms similar to recently proposed ones. However, the paper lacks critical details and explanations. The connection between the pronunciation task and the attention mechanism used for the normalization task is unclear, and the attention mechanism itself should be described in more detail. Furthermore, the comparison of output vectors in Figure 4 is not well explained, and the Kappa statistic for attention vs. MTL needs to be compared to the same statistic for each of those models vs. the base model. Lastly, the experimental improvements in accuracy for either model are quite small, which contradicts the analysis that suggests large changes in the models. Overall, while the paper presents an interesting hypothesis and achieves good results, there are areas for improvement in terms of clarity and detail.\" \n}",
        "human_end_index": 3832
    },
    {
        "id": 367,
        "source": "PeerRead/acl_2017/",
        "title": "From BLEU to RAINBOW: Why We Need New Metrics for NLG.",
        "human_review": "- Strengths:\n\nThe paper addresses a long standing problem concerning automatic evaluation of\nthe output of generation/translation systems.\n\nThe analysis of all the available metrics is thorough and comprehensive.\n\nThe authors demonstrate a new metric with a higher correlation with human\njudgements\n\nThe bibliography will help new entrants into the field.\n\n- Weaknesses:\n\nThe paper is written as a numerical analysis paper, with very little insights\nto linguistic issues in generation, the method of generation, the differences\nin the output from a different systems and human generated reference.\n\nIt is unclear if the crowd source generated references serve well in the\ncontext of an application that needs language generation.\n\n- General Discussion:\n\nOverall, the paper could use some linguistic examples (and a description of the\ndifferent systems) at the risk of dropping a few tables to help the reader with\nintuitions.",
        "partial_review": "- Strengths:\n\nThe paper addresses a long standing problem concerning automatic evaluation of\nthe output of generation/translation systems.\n\nThe analysis of all the available metrics is thorough and comprehensive.\n\nThe authors demonstrate a new metric with a higher correlation with human\njudgements\n\nThe bibliography will help new entrants into the fi",
        "task3_prompt": "\nComplete a partially written peer review about the paper: From BLEU to RAINBOW: Why We Need New Metrics for NLG. \n\nHere is the abstract of the paper: The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we investigate a wide range of these metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. A detailed error analysis shows that automatic metrics are particularly bad in distinguishing outputs of medium and good quality, which can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance is data and system specific.  We then suggest an alternative metric, called RAINBOW, combining the individual strengths of different automatic scores. This new metric achieves up to rho=.81 correlation with human judgements on the sentence level (compared to a maximum of rho=.33  for existing metrics) and achieves stable results across systems and data sets. \n\nHere is the partial review: - Strengths:\n\nThe paper addresses a long standing problem concerning automatic evaluation of\nthe output of generation/translation systems.\n\nThe analysis of all the available metrics is thorough and comprehensive.\n\nThe authors demonstrate a new metric with a higher correlation with human\njudgements\n\nThe bibliography will help new entrants into the fi \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"eld of NLG evaluation.\"\n}",
        "human_end_index": 351
    },
    {
        "id": 369,
        "source": "PeerRead/acl_2017/",
        "title": "Morphology Generation for Statistical Machine Translation using Deep Learning Techniques",
        "human_review": "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\nSpanish) in a two-step process. First, a system translates into a simplified\nversion of the target language. Second, a system chooses morphological features\nfor each generated target word, and inflects the words based on those features.\n\nWhile I wish the authors would apply the work to more than one language pair, I\nbelieve the issue addressed by this work is one of the most important and\nunder-addressed problems with current MT systems. The approach taken by the\nauthors is very different than many modern approaches based on BPE and\ncharacter-level models, and instead harkens back to approaches such as\n\"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into\nMorphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013),\nboth of which are unfortunately uncited.\n\nI am also rather suspicious of the fact that the authors present only METEOR\nresults and no BLEU or qualitative improvements. If BLEU scores do not rise,\nperhaps the authors could argue why they believe their approach is still a net\nplus, and back the claim up with METEOR and example sentences.\n\nFurthermore, the authors repeatedly talk about gender and number as the two\nlinguistic features they seek to correctly handle, but seem to completely\noverlook person. Perhaps this is because first and second person pronouns and\nverbs rarely occur in news, but certainly this point at least merits brief\ndiscussion. I would also like to see some discussion of why rescoring hurts\nwith gender. If the accuracy is very good, shouldn the reranker learn to just\nkeep the 1-best?\n\nFinally, while the content of this paper is good overall, it has a huge amount\nof spelling, grammar, word choice, and style errors that render it unfit for\npublication in its current form. Below is dump of some errors that I found.\n\nOverall, I would like to this work in a future conference, hopefully with more\nthan one language pair, more evaluation metrics, and after further\nproofreading.\n\nGeneral error dump:\nLine 062: Zhand --> Zhang\nLine 122: CFR --> CRF\nWhole related work section: consistent use of \\cite when \\newcite is\nappropriate\nIt feels like there's a lot of filler: \"it is important to mention that\", \"it\nis worth mentioning that\", etc\nLine 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in\ngeneral?\nLine 191: \"a software\"\nLine 196: \"academic and commercial level\" -- this should definitely be\npluralized, but are these even levels?\nLine 210: \"a morphology-based simplified target\" makes it sound like this\nsimplified target uses morphology. Perhaps the authors mean \"a morphologically\nsimplified target\"?\nLine 217: \"decide on the morphological simplifications\"?\nTable 1: extra space in \"cuesti\u00f3n\" on the first line and \"titulado\" in the\nlast line.\nTable 1: Perhaps highlight differences between lines in this table somehow?\nHow is the simplification carried out? Is this simplifier hand written by the\nauthors, or does it use an existing tool?\nLine 290: i.e. --> e.g.\nLine 294: \"train on\" or \"train for\"\nLine 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires\nour architecture\"\nLine 324: drop this comma\nLine 338: This equation makes it look like all words share the same word vector\nW\nLine 422: This could also be \"casas blancas\", right? How does the system choose\nbetween the sg. and pl. forms? Remind the reader of the source side\nconditioning here.\nLine 445: This graph is just a lattice, or perhaps more specifically a \"sausage\nlattice\"\nLine 499: Insert \"e.g.\" or similiar: (e.g. producirse)\nLine 500: misspelled \"syllable\"\nLine 500/503: I'd like some examples or further clarity on what palabras llanas\nand palabras estr\u00fajulas are and how you handle all three of these special\ncases.\nLine 570: \"and sentences longer than 50 words\"\nLine 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\"\nLine 574: are you sure this is an \"and\" and not an \"or\"?\nLine 596: \"trained for\" instead of \"trained on\"\nLine 597: corpus --> copora\nLine 604: size is --> sizes are\nLine 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable\ntraining times.\nLine 615: \"seven and five being the best values\"\nLine 617: Why 70? Increased from what to 70?\nTable 3: These are hyperparameters and not just ordinary parameters of the\nmodel\nLine 650: \"coverage exceeds 99%\"?\nLine 653: \"descending\"\nLine 666: \"quadratic\"\nLine 668: space before \\cites\nLine 676: \"by far\" or \"by a large margin\" instead of \"by large\"\nLine 716: below\nLine 729: \"The standard phrase-based ...\"\nzh-seg citation lists the year as 2016, but the tool actually was released in\n2009\nThe paper describes a method for improving two-step translation using deep\nlearning. Results are presented for Chinese->Spanish translation, but the\napproach seems to be largely language-independent.\n\nThe setting is fairly typical for two-step MT. The first step translates into a\nmorphologically underspecified version of the target language. The second step\nthen uses machine learning to fill in the missing morphological categories and\nproduces the final system output by inflecting the underspecified forms (using\na morphological generator). The main novelty of this work is the choice of deep\nNNs as classifiers in the second step. The authors also propose a rescoring\nstep which uses a LM to select the best variant.\n\nOverall, this is solid work with good empirical results: the classifier models\nreach a high accuracy (clearly outperforming baselines such as SVMs) and the\nimprovement is apparent even in the final translation quality.\n\nMy main problem with the paper is the lack of a comparison with some\nstraightforward deep-learning baselines. Specifically, you have a structured\nprediction problem and you address it with independent local decisions followed\nby a rescoring step. (Unless I misunderstood the approach.) But this is a\nsequence labeling task which RNNs are well suited for. How would e.g. a\nbidirectional LSTM network do when trained and used in the standard sequence\nlabeling setting? After reading the author response, I still think that\nbaselines (including the standard LSTM) are run in the same framework, i.e.\nindependently for each local label. If that's not the case, it should have been\nclarified better in the response. This is a problem because you're not using\nthe RNNs in the standard way and yet you don't justify why your way is better\nor compare the two approaches.\n\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best\nsentences? What features do you use? Or are you searching a weighted graph for\nthe single optimal path? This needs to be explained more clearly in the paper.\n(My current impression is that you produce a graph, then look for K best paths\nin it, generate the inflected sentences from these K paths and *then* use a LM\n-- and nothing else -- to select the best variant. But I'm not sure from\nreading the paper.) This was not addressed in the response.\n\nYou report that larger word embeddings lead to a longer training time. Do they\nalso influence the final results?\n\nCan you attempt to explain why adding information from the source sentence\nhurts? This seems a bit counter-intuitive -- does e.g. the number information\nnot get entirely lost sometimes because of this? I would appreciate a more\nthorough discussion on this in the final version, perhaps with a couple of\nconvincing examples.\n\nThe paper contains a number of typos and the general level of English may not\nbe sufficient for presentation at ACL.\n\nMinor corrections:\n\ncontext of the application of MT -> context of application for MT\n\nIn this cases, MT is faced in two-steps -> In this case, MT is divided into two\nsteps\n\nmarkov -> Markov\n\nCFR -> CRF\n\ntask was based on a direct translation -> task was based on direct translation\n\ntask provided corpus -> task provided corpora\n\nthe phrase-based system has dramatically -> the phrase-based approach...\n\ninvestigated different set of features -> ...sets of features\n\nwords as source of information -> words as the source...\n\ncorrespondant -> corresponding\n\nClasses for gender classifier -> Classes for the...\n\nfor number classifier -> for the...\n\nThis layer's input consists in -> ...consists of\n\nto extract most relevant -> ...the most...\n\nSigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer\nwould produce (-1, 1).\n\ninformation of a word consists in itself -> ...of itself\n\nthis $A$ set -> the set $A$\n\nempty sentences and longer than 50 words -> empty sentences and sentences\nlonger than...\n\nclassifier is trained on -> classifier is trained in\n\naproximately -> approximately\n\ncoverage raises the 99% -> coverage exceeds 99% (unless I misunderstand)\n\nin descendant order -> in descending order\n\ncuadratic -> quadratic (in multiple places)\n\nbut best results -> but the best results\n\nRescoring step improves -> The rescoring step...\n\nare not be comparable -> are not comparable\nThis paper presents a method for generating morphology, focusing on gender and\nnumber, using deep learning techniques. From a morphologically simplified\nSpanish text, the proposed approach uses a classifier to reassign the gender\nand number for each token, when necessary. The authors compared their approach\nwith other learning algorithms, and evaluated it in machine translation on the\nChinese-to-Spanish (Zh->Es) translation direction.\n\nRecently, the task of generating gender and number has been rarely tackled,\nmorphology generation methods usually target, and are evaluated on,\nmorphologically-rich languages like German or Finnish.\nHowever, calling the work presented in this paper \u201cmorphology\ngeneration\u201c\u00a0is a bit overselling as the proposed method clearly deals only\nwith\ngender and number. And given the fact that some rules are handcrafted for this\nspecific task, I do not think this method can be straightforwardly applied to\ndo more complex morphology generation for morphologically-rich languages.\n\nThis paper is relatively clear in the sections presenting the proposed method.\nA\nlot of work has been done to design the method and I think it can have some\ninteresting impact on various NLP tasks. However the evaluation part of\nthis work is barely understandable as many details of what is done, or why it\nis done, are missing. From this evaluation, we cannot know if the proposed\nmethod brings improvements over state-of-the-art methods while the experiments\ncannot be replicated. Furthermore, no analysis of the results obtained is\nprovided. Since half a page is still available, there was the possibility\nto provide more information to make more clear the evaluation. This work lacks\nof motivation. Why do you think deep learning can especially improve gender and\nnumber generation over state-of-the-art methods?\n\nIn your paper, the word \u201ccontribution\u201c should be used more wisely, as it is\nnow in the paper, it is not obvious what are the real contributions (more\ndetails below). \n\nabstract:\nwhat do you mean by unbalanced languages?\n\nsection 1:\nYou claim that your main contribution is the use of deep learning. Just the use\nof deep learning in some NLP task is not a contribution.\n\nsection 2:\nYou claim that neural machine translation (NMT), mentioned as \u201cneural\napproximations\u201c,  does not achieve state-of-the-art results for Zh->Es. I\nrecommend to remove this claim from the paper, or to discuss it more, since\nJunczys-Dowmunt et al. (2016), during the last IWSLT, presented some results\nfor Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10\nBLEU points.\n\nsection 5.1:\nYou wrote that using the Zh->Es language pair is one of your main\ncontributions. Just using a language pair is not a contribution. Nonetheless, I\nthink it is nice to see a paper on machine translation that does not focus of\nimproving machine translation for English.\nThe numbers provided in Table 2 were computed before or after preprocessing?\nWhy did you remove the sentences longer than 50 tokens?\nPrecise how did you obtain development and test sets, or provide them. Your\nexperiments are currently no replicable especially because of that.\n\nsection 5.2:\nYou wrote that you used Moses and its default parameters, but the default\nparameters of Moses are not the same depending on the version, so you should\nprovide the number of the version used.\n\nsection 5.3:\nWhat do you mean by \u201chardware cost\u201c?\nTable 3: more details should be provided regarding how did you obtain these\nvalues. You chose these values given the classifier accuracy, but how precisely\nand on what data did you train and test the classifiers? On the same data used\nin section 6?\nIf I understood the experiments properly, you used simplified Spanish. But I\ncannot find in the text how do you simplify Spanish. And how do you use it to\ntrain the classifier and the SMT system? \n\nsection 6:\nYour method is better than other classification\nalgorithms, but it says nothing about how it performs compared to the\nstate-of-the-art methods. You should at least precise why you chose these\nclassifications algorithms for comparison. Furthermore, how your rules impact\nthese results? And more generally, how do you explain such a high accuracy for\nyou method?\nDid you implement all these classification algorithms by yourselves? If not,\nyou must provide the URL or cite the framework you used.\nFor the SMT experiments, I guess you trained your phrase table on simplified\nSpanish. You must precise it.\nYou chose METEOR over other metrics like BLEU to evaluate your results. You\nmust provide some explanation for this choice. I particularly appreciate when I\nsee a MT paper that does not use BLEU for evaluation, but if you use METEOR,\nyou must mention which version you used. METEOR has largely changed since 2005.\nYou cited the paper of 2005, did you use the 2005 version? Or did you use the\nlast one with paraphrases? \nAre your METEOR scores statistically significant?\n\nsection 7:\nAs future work you mentioned \u201cfurther simplify morphology\u201c. In this paper,\nyou do not present any simplification of morphology, so I think that choosing\nthe word\n\u201cfurther\u201c is misleading.\n\nsome typos:\nfemenine\nensambling\ncuadratic\n\nstyle:\nplain text citations should be rewritten like this: \u201c(Toutanova et al, 2008)\nbuilt\u00a0\u201c should be \u201cToutanova et al. (2008) built \u201c\nplace the caption of your tables below the table and not above, and with more\nspace between the table and its caption.\nYou used the ACL 2016 template. You must use the new one prepared for ACL 2017.\nMore generally, I suggest that you read again the FAQ and the submission\ninstructions provided on the ACL 2017 website. It will greatly help you to\nimprove the paper. There are also important information regarding references:\nyou must provide DOI or URL of all ACL papers in your references.\n\n-----------------------\n\nAfter authors response:\n\nThank you for your response.\n\nYou wrote that rules are added just as post-processing, but does it mean that\nyou do not apply them to compute your classification results? Or if you do\napply them before computing these results, I'm still wondering about their\nimpact on these results.\n\nYou wrote that Spanish is simplified as shown in Table 1, but it does not\nanswer my question: how did you obtain these simplifications exactly? (rules?\nsoftware? etc.) The reader need to now that to reproduce your approach.\n\nThe classification algorithms presented in Table 5 are not state-of-the-art, or\nif they are you need to cite some paper. Furthermore, this table only tells\nthat deep learning gives the best results for classification, but it does not\ntell at all if your approach is better than state-of-the-art approach for\nmachine translation. You need to compare your approach with other\nstate-of-the-art morphology generation approaches (described in related work)\ndesigned for machine translation. If you do that your paper will be much more\nconvincing in my opinion.",
        "partial_review": "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\nSpanish) in a two-step process. First, a system translates into a simplified\nversion of the target language. Second, a system chooses morphological features\nfor each generated target word, and inflects the words based on those features.\n\nWhile I wish the authors would apply the work to more than one language pair, I\nbelieve the issue addressed by this work is one of the most important and\nunder-addressed problems with current MT systems. The approach taken by the\nauthors is very different than many modern approaches based on BPE and\ncharacter-level models, and instead harkens back to approaches such as\n\"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into\nMorphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013),\nboth of which are unfortunately uncited.\n\nI am also rather suspicious of the fact that the authors present only METEOR\nresults and no BLEU or qualitative improvements. If BLEU scores do not rise,\nperhaps the authors could argue why they believe their approach is still a net\nplus, and back the claim up with METEOR and example sentences.\n\nFurthermore, the authors repeatedly talk about gender and number as the two\nlinguistic features they seek to correctly handle, but seem to completely\noverlook person. Perhaps this is because first and second person pronouns and\nverbs rarely occur in news, but certainly this point at least merits brief\ndiscussion. I would also like to see some discussion of why rescoring hurts\nwith gender. If the accuracy is very good, shouldn the reranker learn to just\nkeep the 1-best?\n\nFinally, while the content of this paper is good overall, it has a huge amount\nof spelling, grammar, word choice, and style errors that render it unfit for\npublication in its current form. Below is dump of some errors that I found.\n\nOverall, I would like to this work in a future conference, hopefully with more\nthan one language pair, more evaluation metrics, and after further\nproofreading.\n\nGeneral error dump:\nLine 062: Zhand --> Zhang\nLine 122: CFR --> CRF\nWhole related work section: consistent use of \\cite when \\newcite is\nappropriate\nIt feels like there's a lot of filler: \"it is important to mention that\", \"it\nis worth mentioning that\", etc\nLine 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in\ngeneral?\nLine 191: \"a software\"\nLine 196: \"academic and commercial level\" -- this should definitely be\npluralized, but are these even levels?\nLine 210: \"a morphology-based simplified target\" makes it sound like this\nsimplified target uses morphology. Perhaps the authors mean \"a morphologically\nsimplified target\"?\nLine 217: \"decide on the morphological simplifications\"?\nTable 1: extra space in \"cuesti\u00f3n\" on the first line and \"titulado\" in the\nlast line.\nTable 1: Perhaps highlight differences between lines in this table somehow?\nHow is the simplification carried out? Is this simplifier hand written by the\nauthors, or does it use an existing tool?\nLine 290: i.e. --> e.g.\nLine 294: \"train on\" or \"train for\"\nLine 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires\nour architecture\"\nLine 324: drop this comma\nLine 338: This equation makes it look like all words share the same word vector\nW\nLine 422: This could also be \"casas blancas\", right? How does the system choose\nbetween the sg. and pl. forms? Remind the reader of the source side\nconditioning here.\nLine 445: This graph is just a lattice, or perhaps more specifically a \"sausage\nlattice\"\nLine 499: Insert \"e.g.\" or similiar: (e.g. producirse)\nLine 500: misspelled \"syllable\"\nLine 500/503: I'd like some examples or further clarity on what palabras llanas\nand palabras estr\u00fajulas are and how you handle all three of these special\ncases.\nLine 570: \"and sentences longer than 50 words\"\nLine 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\"\nLine 574: are you sure this is an \"and\" and not an \"or\"?\nLine 596: \"trained for\" instead of \"trained on\"\nLine 597: corpus --> copora\nLine 604: size is --> sizes are\nLine 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable\ntraining times.\nLine 615: \"seven and five being the best values\"\nLine 617: Why 70? Increased from what to 70?\nTable 3: These are hyperparameters and not just ordinary parameters of the\nmodel\nLine 650: \"coverage exceeds 99%\"?\nLine 653: \"descending\"\nLine 666: \"quadratic\"\nLine 668: space before \\cites\nLine 676: \"by far\" or \"by a large margin\" instead of \"by large\"\nLine 716: below\nLine 729: \"The standard phrase-based ...\"\nzh-seg citation lists the year as 2016, but the tool actually was released in\n2009\nThe paper describes a method for improving two-step translation using deep\nlearning. Results are presented for Chinese->Spanish translation, but the\napproach seems to be largely language-independent.\n\nThe setting is fairly typical for two-step MT. The first step translates into a\nmorphologically underspecified version of the target language. The second step\nthen uses machine learning to fill in the missing morphological categories and\nproduces the final system output by inflecting the underspecified forms (using\na morphological generator). The main novelty of this work is the choice of deep\nNNs as classifiers in the second step. The authors also propose a rescoring\nstep which uses a LM to select the best variant.\n\nOverall, this is solid work with good empirical results: the classifier models\nreach a high accuracy (clearly outperforming baselines such as SVMs) and the\nimprovement is apparent even in the final translation quality.\n\nMy main problem with the paper is the lack of a comparison with some\nstraightforward deep-learning baselines. Specifically, you have a structured\nprediction problem and you address it with independent local decisions followed\nby a rescoring step. (Unless I misunderstood the approach.) But this is a\nsequence labeling task which RNNs are well suited for. How would e.g. a\nbidirectional LSTM network do when trained and used in the standard sequence\nlabeling setting? After reading the author response, I still think that\nbaselines (including the standard LSTM) are run in the same framework, i.e.\nindependently for each local label. If that's not the case, it should have been\nclarified better in the response. This is a problem because you're not using\nthe RNNs in the standard way and yet you don't justify why your way is better\nor compare the two approaches.\n\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best\nsentences? What features do you use? Or are you searching a weighted graph for\nthe single optimal path? This needs to be explained more clearly in the paper.\n(My current impression is that you produce a graph, then look for K best paths\nin it, generate the inflected sentences from these K paths and *then* use a LM\n-- and nothing else -- to select the best variant. But I'm not sure from\nreading the paper.) This was not addressed in the response.\n\nYou report that larger word embeddings lead to a longer training time. Do they\nalso influence the final results?\n\nCan you attempt to explain why adding information from the source sentence\nhurts? This seems a bit counter-intuitive -- does e.g. the number information\nnot get entirely lost sometimes because of this? I would appreciate a more\nthorough discussion on this in the final version, perhaps with a couple of\nconvincing examples.\n\nThe paper contains a number of typos and the general level of English may not\nbe sufficient for presentation at ACL.\n\nMinor corrections:\n\ncontext of the application of MT -> context of application for MT\n\nIn this cases, MT is faced in two-steps -> In this case, MT is divided into two\nsteps\n\nmarkov -> Markov\n\nCFR -> CRF\n\ntask was based on a direct translation -> task was based on direct translation\n\ntask provided corpus -> task provided corpora\n\nthe phrase-based system has dramatically -> the phrase-based approach...\n\ninvestigated different set of features -> ...sets of features\n\nwords as source of information -> words as the source...\n\ncorrespondant -> corresponding",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Morphology Generation for Statistical Machine Translation using Deep Learning Techniques \n\nHere is the abstract of the paper: Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR. \n\nHere is the partial review: This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\nSpanish) in a two-step process. First, a system translates into a simplified\nversion of the target language. Second, a system chooses morphological features\nfor each generated target word, and inflects the words based on those features.\n\nWhile I wish the authors would apply the work to more than one language pair, I\nbelieve the issue addressed by this work is one of the most important and\nunder-addressed problems with current MT systems. The approach taken by the\nauthors is very different than many modern approaches based on BPE and\ncharacter-level models, and instead harkens back to approaches such as\n\"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into\nMorphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013),\nboth of which are unfortunately uncited.\n\nI am also rather suspicious of the fact that the authors present only METEOR\nresults and no BLEU or qualitative improvements. If BLEU scores do not rise,\nperhaps the authors could argue why they believe their approach is still a net\nplus, and back the claim up with METEOR and example sentences.\n\nFurthermore, the authors repeatedly talk about gender and number as the two\nlinguistic features they seek to correctly handle, but seem to completely\noverlook person. Perhaps this is because first and second person pronouns and\nverbs rarely occur in news, but certainly this point at least merits brief\ndiscussion. I would also like to see some discussion of why rescoring hurts\nwith gender. If the accuracy is very good, shouldn the reranker learn to just\nkeep the 1-best?\n\nFinally, while the content of this paper is good overall, it has a huge amount\nof spelling, grammar, word choice, and style errors that render it unfit for\npublication in its current form. Below is dump of some errors that I found.\n\nOverall, I would like to this work in a future conference, hopefully with more\nthan one language pair, more evaluation metrics, and after further\nproofreading.\n\nGeneral error dump:\nLine 062: Zhand --> Zhang\nLine 122: CFR --> CRF\nWhole related work section: consistent use of \\cite when \\newcite is\nappropriate\nIt feels like there's a lot of filler: \"it is important to mention that\", \"it\nis worth mentioning that\", etc\nLine 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in\ngeneral?\nLine 191: \"a software\"\nLine 196: \"academic and commercial level\" -- this should definitely be\npluralized, but are these even levels?\nLine 210: \"a morphology-based simplified target\" makes it sound like this\nsimplified target uses morphology. Perhaps the authors mean \"a morphologically\nsimplified target\"?\nLine 217: \"decide on the morphological simplifications\"?\nTable 1: extra space in \"cuesti\u00f3n\" on the first line and \"titulado\" in the\nlast line.\nTable 1: Perhaps highlight differences between lines in this table somehow?\nHow is the simplification carried out? Is this simplifier hand written by the\nauthors, or does it use an existing tool?\nLine 290: i.e. --> e.g.\nLine 294: \"train on\" or \"train for\"\nLine 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires\nour architecture\"\nLine 324: drop this comma\nLine 338: This equation makes it look like all words share the same word vector\nW\nLine 422: This could also be \"casas blancas\", right? How does the system choose\nbetween the sg. and pl. forms? Remind the reader of the source side\nconditioning here.\nLine 445: This graph is just a lattice, or perhaps more specifically a \"sausage\nlattice\"\nLine 499: Insert \"e.g.\" or similiar: (e.g. producirse)\nLine 500: misspelled \"syllable\"\nLine 500/503: I'd like some examples or further clarity on what palabras llanas\nand palabras estr\u00fajulas are and how you handle all three of these special\ncases.\nLine 570: \"and sentences longer than 50 words\"\nLine 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\"\nLine 574: are you sure this is an \"and\" and not an \"or\"?\nLine 596: \"trained for\" instead of \"trained on\"\nLine 597: corpus --> copora\nLine 604: size is --> sizes are\nLine 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable\ntraining times.\nLine 615: \"seven and five being the best values\"\nLine 617: Why 70? Increased from what to 70?\nTable 3: These are hyperparameters and not just ordinary parameters of the\nmodel\nLine 650: \"coverage exceeds 99%\"?\nLine 653: \"descending\"\nLine 666: \"quadratic\"\nLine 668: space before \\cites\nLine 676: \"by far\" or \"by a large margin\" instead of \"by large\"\nLine 716: below\nLine 729: \"The standard phrase-based ...\"\nzh-seg citation lists the year as 2016, but the tool actually was released in\n2009\nThe paper describes a method for improving two-step translation using deep\nlearning. Results are presented for Chinese->Spanish translation, but the\napproach seems to be largely language-independent.\n\nThe setting is fairly typical for two-step MT. The first step translates into a\nmorphologically underspecified version of the target language. The second step\nthen uses machine learning to fill in the missing morphological categories and\nproduces the final system output by inflecting the underspecified forms (using\na morphological generator). The main novelty of this work is the choice of deep\nNNs as classifiers in the second step. The authors also propose a rescoring\nstep which uses a LM to select the best variant.\n\nOverall, this is solid work with good empirical results: the classifier models\nreach a high accuracy (clearly outperforming baselines such as SVMs) and the\nimprovement is apparent even in the final translation quality.\n\nMy main problem with the paper is the lack of a comparison with some\nstraightforward deep-learning baselines. Specifically, you have a structured\nprediction problem and you address it with independent local decisions followed\nby a rescoring step. (Unless I misunderstood the approach.) But this is a\nsequence labeling task which RNNs are well suited for. How would e.g. a\nbidirectional LSTM network do when trained and used in the standard sequence\nlabeling setting? After reading the author response, I still think that\nbaselines (including the standard LSTM) are run in the same framework, i.e.\nindependently for each local label. If that's not the case, it should have been\nclarified better in the response. This is a problem because you're not using\nthe RNNs in the standard way and yet you don't justify why your way is better\nor compare the two approaches.\n\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best\nsentences? What features do you use? Or are you searching a weighted graph for\nthe single optimal path? This needs to be explained more clearly in the paper.\n(My current impression is that you produce a graph, then look for K best paths\nin it, generate the inflected sentences from these K paths and *then* use a LM\n-- and nothing else -- to select the best variant. But I'm not sure from\nreading the paper.) This was not addressed in the response.\n\nYou report that larger word embeddings lead to a longer training time. Do they\nalso influence the final results?\n\nCan you attempt to explain why adding information from the source sentence\nhurts? This seems a bit counter-intuitive -- does e.g. the number information\nnot get entirely lost sometimes because of this? I would appreciate a more\nthorough discussion on this in the final version, perhaps with a couple of\nconvincing examples.\n\nThe paper contains a number of typos and the general level of English may not\nbe sufficient for presentation at ACL.\n\nMinor corrections:\n\ncontext of the application of MT -> context of application for MT\n\nIn this cases, MT is faced in two-steps -> In this case, MT is divided into two\nsteps\n\nmarkov -> Markov\n\nCFR -> CRF\n\ntask was based on a direct translation -> task was based on direct translation\n\ntask provided corpus -> task provided corpora\n\nthe phrase-based system has dramatically -> the phrase-based approach...\n\ninvestigated different set of features -> ...sets of features\n\nwords as source of information -> words as the source...\n\ncorrespondant -> corresponding \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper details a method of achieving translation from morphologically impoverished languages (e.g. Chinese) to morphologically rich ones (e.g. Spanish) in a two-step process. First, a system translates into a simplified version of the target language. Second, a system chooses morphological features for each generated target word and inflects the words based on those features. While I wish the authors would apply the work to more than one language pair, I believe the issue addressed by this work is one of the most important and under-addressed problems with current MT systems. The approach taken by the authors is very different than many modern approaches based on BPE and character-level models, and instead harkens back to approaches such as 'Factored Translation Models' (Koehn and Hoang, 2007) and 'Translating into Morphologically Rich Languages with Synthetic Phrases' (Chahuneau et a. 2013), both of which are unfortunately uncited.\\n\\nI am also rather suspicious of the fact that the authors present only METEOR results and no BLEU or qualitative improvements. If BLEU scores do not rise, perhaps the authors could argue why they believe their approach is still a net plus, and back the claim up with METEOR and example sentences.\\n\\nFurthermore, the authors repeatedly talk about gender and number as the two linguistic features they seek to correctly handle but seem to completely overlook person. Perhaps this is because first and second person pronouns and verbs rarely occur in news, but certainly this point at least merits brief discussion. I would also like to see some discussion of why rescoring hurts with gender. If the accuracy is very good, shouldn't the reranker learn to just keep the 1-best?\\n\\nFinally, while the content of this paper is good overall, it has a huge amount of spelling, grammar, word choice, and style errors that render it unfit for publication in its current form. Below is dump of some errors that I found.\\n\\nOverall, I would like to see this work in a future conference, hopefully with more than one language pair, more evaluation metrics, and after further proofreading.\\n\\nGeneral error dump:\\nLine 062: Zhand --> Zhang\\nLine 122: CFR --> CRF\\nWhole related work section: consistent use of \\\\cite when \\\\newcite is appropriate\\nIt feels like there's a lot of filler: 'it is important to mention that', 'it is worth mentioning that', etc\\nLine 182, 184: 'The popular phrase-based MT system' = moses? or PBMT systems in general?\\nLine 191: 'a software'\\nLine 196: 'academic and commercial level' -- this should definitely be pluralized, but are these even levels?\\nLine 210: 'a morphology-based simplified target' makes it sound like this simplified target uses morphology. Perhaps the authors mean 'a morphologically simplified target'?\\nLine 217: 'decide on the morphological simplifications'?\\nTable 1: extra space in 'cuesti\u00f3n' on the first line and 'titulado' in the last line.\\nTable 1: Perhaps highlight differences between lines in this table somehow?\\nHow is the simplification carried out? Is this simplifier hand written by the authors or does it use an existing tool?\\nLine 290: i.e. --> e.g.\\nLine 294: 'train on' or 'train for'\\nLine 320: 'our architecture is inspired by' or 'Collobert's proposal inspires our architecture'\\nLine 324: drop this comma\\nLine 338: This equation makes it look like all words share the same word vector W\\nLine 422: This could also be 'casas blancas', right? How does the system choose between the sg. and pl. forms? Remind the reader of the source side conditioning here.\\nLine 445: This graph is just a lattice, or perhaps more specifically a 'sausage lattice'\\nLine 499: Insert 'e.g.' or similar: (e.g. producirse)\\nLine 500: misspelled 'syllable'\\nLine 500/503: I'd like some examples or further clarity on what palabras llanas and palabras estr\u00fajulas are and how you handle all three of these special cases.\\nLine 570: 'and sentences longer than 50 words'\\nLine 571: 'by means of zh-seg' (no determiner) or 'by means of the zh-seg tool'\\nLine 574: are you sure this is an 'and' and not an 'or'?\\nLine 596: 'trained for' instead of 'trained on'\\nLine 597: corpus --> corpora\\nLine 604: size is --> sizes are\\nLine 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable training times.\\nLine 615: 'seven and five being the best values'\\nLine 617: Why 70? Increased from what to 70?\\nTable 3: These are hyperparameters and not just ordinary parameters of the model\\nLine 650: 'coverage exceeds 99%'?\\nLine 653: 'descending'\\nLine 666: 'quadratic'\\nLine 668: space before \\\\cites\\nLine 676: 'by far' or 'by a large margin' instead of 'by large'\\nLine 716: below\\nLine 729: 'The standard phrase-based ...'\\nzh-seg citation lists the year as 2016, but the tool was actually released in 2009\\n\\nThe paper describes a method for improving two-step translation using deep learning. Results are presented for Chinese->Spanish translation, but the approach seems to be largely language-independent.\\n\\nThe setting is fairly typical for two-step MT. The first step translates into a morphologically underspecified version of the target language. The second step then uses machine learning to fill in the missing morphological categories and produces the final system output by inflecting the underspecified forms (using a morphological generator). The main novelty of this work is the choice of deep NNs as classifiers in the second step. The authors also propose a rescoring step which uses a LM to select the best variant.\\n\\nOverall, this is solid work with good empirical results: the classifier models reach a high accuracy (clearly outperforming baselines such as SVMs) and the improvement is apparent even in the final translation quality.\\n\\nMy main problem with the paper is the lack of a comparison with some straightforward deep-learning baselines. Specifically, you have a structured prediction problem and you address it with independent local decisions followed by a rescoring step. (Unless I misunderstood the approach.) But this is a sequence labeling task which RNNs are well-suited for. How would e.g. a bidirectional LSTM network do when trained and used in the standard sequence labeling setting? After reading the author response, I still think that baselines (including the standard LSTM) are run in the same framework, i.e. independently for each local label. If that's not the case, it should have been clarified better in the response. This is a problem because you're not using the RNNs in the standard way and yet you don't justify why your way is better or compare the two approaches.\\n\\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best sentences? What features do you use? Or are you searching a weighted graph for the single optimal path? This needs to be explained more clearly in the paper. (My current impression is that you produce a graph, then look for K best paths in it, generate the inflected sentences from these K paths and *then* use an LM -- and nothing else -- to select the best variant. But I'm not sure from reading the paper.) This was not addressed in the response.\\n\\nYou report that larger word embeddings lead to a longer training time. Do they also influence the final results?\\n\\nCan you attempt to explain why adding information from the source sentence hurts? This seems a bit counter-intuitive -- does e.g. the number information not get entirely lost sometimes because of this? I would appreciate a more thorough discussion on this in the final version, perhaps with a couple of convincing examples.\\n\\nThe paper contains a number of typos, and the general level of English may not be sufficient for presentation at ACL.\\n\\nMinor corrections:\\ncontext of the application of MT -> context of application for MT\\n\\nIn this cases, MT is faced in two-steps -> In this case, MT is divided into two steps\\n\\nmarkov -> Markov\\n\\nCFR -> CRF\\n\\ntask was based on a direct translation -> task was based on direct translation\\n\\ntask provided corpus -> task provided corpora\\n\\nthe phrase-based system has dramatically -> the phrase-based approach...\\n\\ninvestigated different set of features -> ...sets of features\\n\\nwords as source of information -> words as the source...\\n\\ncorrespondant -> corresponding\\n\\nOutput in the following json format:\\n{\\n    \\\"complete_review\\\": \\\"\\\"\\n}\\n\"\n}",
        "human_end_index": 8205
    },
    {
        "id": "375",
        "source": "PeerRead/acl_2017/",
        "title": "CANE: Context-Aware Network Embedding for Relation Modeling",
        "human_review": "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090\u20131098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based network embeddings.\n\nGiven that the aim of this paper is to model differing representations of\nsocial network actors' different roles, it should really cite and compare to\nthe mixed membership stochastic blockmodel (MMSB):\n\nAiroldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed\nmembership stochastic blockmodels. Journal of Machine Learning Research.\n\nThe MMSB allows each node to randomly select a different \"role\" when deciding\nwhether to form each edge.\n\n- General Discussion:\n\nThe aforementioned statistical models do not leverage text, and they do not use\nscalable neural network implementations based on negative sampling, but they\nare based on well-principled generative models instead of heuristic neural\nnetwork objective functions and algorithms.  There are more recent extensions\nof these models and inference algorithms which are more scalable, and which do\nleverage text.\n\nIs the difference in performance between CENE and CANE in Figure 3\nstatistically insignificant? (A related question: were the experiments repeated\nmore than once with random train/test splits?)\n\nWere the grid searches for hyperparameter values, mentioned in Section 5.3,\nperformed with evaluation on the test set (which would be problematic), or on a\nvalidation set, or on the training set?",
        "partial_review": "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090\u20131098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based ne",
        "task3_prompt": "\nComplete a partially written peer review about the paper: CANE: Context-Aware Network Embedding for Relation Modeling \n\nHere is the abstract of the paper: Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from \\url{https://github.com/thunlp/CANE}. \n\nHere is the partial review: This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090\u20131098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based ne \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper addresses the network embedding problem by introducing a neural network model which uses both the network structure and associated text on the nodes, with an attention model to vary the textual representation based on the text of the neighboring nodes. The model leverages both the network and the text to construct the latent representations, and the mutual attention approach seems sensible. A relatively thorough evaluation is provided, with multiple datasets, baselines, and evaluation tasks. However, one weakness of this paper is that it does not cite or compare to previous work on the latent space model of Peter Hoff et al., which has been a significant contribution to the statistical and probabilistic machine learning literature. It would be beneficial for the authors to acknowledge and discuss this previous work in order to provide a more comprehensive review of the related literature in network embedding.\"",
        "human_end_index": 1376
    },
    {
        "id": 376,
        "source": "PeerRead/acl_2017/",
        "title": "Event-based, Recursive Neural Networks for the Extraction and Aggregation of International Alliance Relations",
        "human_review": "- Strengths: Useful modeling contribution, and potentially useful annotated\ndata, for an important problem -- event extraction for the relationships\nbetween countries as expressed in news text.\n\n- Weaknesses: Many points are not explained well in the paper. \n\n- General Discussion:\n\nThis work tackles an important and interesting event extraction problem --\nidentifying positive and negative interactions between pairs of countries in\nthe world (or rather, between actors affiliated with countries).  The primary\ncontribution is an application of supervised, structured neural network models\nfor sentence-level event/relation extraction.  While previous work has examined\ntasks in the overall area, to my knowledge there has not been any publicly\navailble sentence-level annotated data for the problem -- the authors here make\na contribution as well by annotating some data included with the submission; if\nit is released, it could be useful for future researchers in this area.\n\nThe proposed models -- which seem to be an application of various\ntree-structured recursive neural network models -- demonstrate a nice\nperformance increase compared to a fairly convincing, broad set of baselines\n(if we are able to trust them; see below).  The paper also presents a manual\nevaluation of the inferred time series from a news corpus which is nice to see.\n\nI'm torn about this paper.  The problem is a terrific one and the application\nof the recursive models seems like a contribution to this problem. \nUnfortunately, many aspects of the models, experimentation, and evaluation are\nnot explained very well.  The same work, with a more carefully written paper,\ncould be really great.\n\nSome notes:\n\n- Baselines need more explanation.  For example, the sentiment lexicon is not\nexplained for the SVM.                    The LSTM classifier is left highly\nunspecified\n(L407-409) -- there are multiple different architectures to use an LSTM for\nclassification.  How was it trained?  Is there a reference for the approach? \nAre the authors using off-the-shelf code (in which case, please refer and cite,\nwhich would also make it easier for the reader to understand and replicate if\nnecessary)?  It would be impossible to replicate based on the two-line\nexplanation here.  \n\n- (The supplied code does not seem to include the baselines, just the recursive\nNN models.  It's great the authors supplied code for part of the system so I\ndon't want to penalize them for missing it -- but this is relevant since the\npaper itself has so few details on the baselines that they could not really be\nreplicated based on the explanation in the paper.)\n\n- How were the recursive NN models trained?\n\n- The visualization section is only a minor contribution; there isn't really\nany innovation or findings about what works or doesn't work here.\n\nLine by line:\n\nL97-99: Unclear. Why is this problem difficult?  Compared to what? (also the\nsentence is somewhat ungrammatical...)\n\nL231 - the trees are binarized, but how?\n\nFootnote 2 -- \"the tensor version\" - needs citation to explain what's being\nreferred to.\n\nL314: How are non-state verbs defined?                    Does the definition of\n\"event\nword\"s\nhere come from any particular previous work that motivates it?                   \nPlease\nrefer to\nsomething appropriate or related.\n\nFootnote 4: of course the collapsed form doesn't work, because the authors\naren't using dependency labels -- the point of stanford collapsed form is to\nremove prepositions from the dependeny path and instead incorporate them into\nthe labels.\n\nL414: How are the CAMEO/TABARI categories mapped to positive and negative\nentries?  Is performance sensitive to this mapping?  It seems like a hard task\n(there are hundreds of those CAMEO categories....) Did the authors consider\nusing the Goldstein scaling, which has been used in political science, as well\nas the cited work by O'Connor et al.?  Or is it bad to use for some reason?\n\nL400-401: what is the sentiment lexicon and why is it appropriate for the task?\n\nL439-440: Not clear.  \"We failed at finding an alpha meeting the requirements\nfor the FT model.\"  What does that mean? What are the requirements? What did\nthe authors do in their attempt to find it?\n\nL447,L470: \"precision and recall values are based on NEG and POS classes\". \nWhat does this mean?  So there's a 3x3 contingency table of gold and predicted\n(POS, NEU, NEG) classes, but this sentence leaves ambiguous how precision and\nrecall are calculated from this information.\n\n5.1 aggregations: this seems fine though fairly ad-hoc.  Is this temporal\nsmoothing function a standard one?  There's not much justification for it,\nespecially given something simpler like a fixed window average could have been\nused.\n\n5.2 visualizations: this seems pretty ad-hoc without much justification for the\nchoices.  The graph visualization shown does not seem to illustrate much. \nShould also discuss related work in 2d spatial visualization of country-country\nrelationships by Peter Hoff and Michael Ward.\n\n5.3\nL638-639: \"unions of countries\" isn't a well defined concept.  mMybe the\nauthors mean \"international organizations\"?\n\nL646-648: how were these 5 strong and 5 weak peaks selected?  In particular,\nhow were they chosen if there were more than 5 such peaks?\n\nL680-683: This needs more examples or explanation of what it means to judge the\npolarity of a peak.  What does it look like if the algorithm is wrong?               \n   \nHow\nhard was this to assess?  What was agreement rate if that can be judged?\n\nL738-740: The authors claim Gerrish and O'Connor et al. have a different\n\"purpose and outputs\" than the authors' work.  That's not right.  Both those\nworks try to do both (1) extract time series or other statistical information\nabout the polarity of the relationships between countries, and *also* (2)\nextract topical keywords to explain aspects of the relationships.  The paper\nhere is only concerned with #1 and less concerned with #2, but certainly the\nprevious work addresses #1.  It's fine to not address #2 but this last sentence\nseems like a pretty odd statement.\n\nThat raises the question -- Gerrish and O'Connor both conduct evaluations with\nan external database of country relations developed in political science\n(\"MID\", military interstate disputes).              Why don't the authors of this\nwork do\nthis evaluation as well?  There are various weaknesses of the MID data, but the\nevaluation approach needs to be discussed or justified.",
        "partial_review": "- Strengths: Useful modeling contribution, and potentially useful annotated\ndata, for an important problem -- event extraction for the relationships\nbetween countries as expressed in news text.\n\n- Weaknesses: Many points are not explained well in the paper. \n\n- General Discussion:\n\nThis work tackles an important and interesting event extraction problem --\nidentifying positive and negative interactions between pairs of countries in\nthe world (or rather, between actors affiliated with countries).  The primary\ncontribution is an application of supervised, structured neural network models\nfor sentence-level event/relation extraction.  While previous work has examined\ntasks in the overall area, to my knowledge there has not been any publicly\navailble sentence-level annotated data for the problem -- the authors here make\na contribution as well by annotating some data included with the submission; if\nit is released, it could be useful for future researchers in this area.\n\nThe proposed models -- which seem to be an application of various\ntree-structured recursive neural network models -- demonstrate a nice\nperformance increase compared to a fairly convincing, broad set of baselines\n(if we are able to trust them; see below).  The paper also presents a manual\nevaluation of the inferred time series from a news corpus which is nice to see.\n\nI'm torn about this paper.  The problem is a terrific one and the application\nof the recursive models seems like a contribution to this problem. \nUnfortunately, many aspects of the models, experimentation, and evaluation are\nnot explained very well.  The same work, with a more carefully written paper,\ncould be really great.\n\nSome notes:\n\n- Baselines need more explanation.  For example, the sentiment lexicon is not\nexplained for the SVM.                    The LSTM classifier is left highly\nunspecified\n(L407-409) -- there are multiple different architectures to use an LSTM for\nclassification.  How was it trained?  Is there a reference for the approach? \nAre the authors using off-the-shelf code (in which case, please refer and cite,\nwhich would also make it easier for the reader to understand and replicate if\nnecessary)?  It would be impossible to replicate based on the two-line\nexplanation here.  \n\n- (The supplied code does not seem to include the baselines, just the recursive\nNN models.  It's great the authors supplied code for part of the system so I\ndon't want to penalize them for missing it -- but this is relevant since the\npaper itself has so few details on the baselines that they could not really be\nreplicated based on the explanation in the paper.)\n\n- How were the recursive NN models trained?\n\n- The visualization section is only a minor contribution; there isn't really\nany innovation or findings about what works or doesn't work here.\n\nLine by line:\n\nL97-99: Unclear. Why is this problem difficult?  Compared to what? (also the\nsentence is somewhat ungrammatical...)\n\nL231 - the trees are binarized, but how?\n\nFootnote 2 -- \"the tensor version\" - needs citation to explain what's being\nreferred to.\n\nL314: How are non-state verbs defined?                    Does the definition of\n\"event\nword\"s\nhere come from any",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Event-based, Recursive Neural Networks for the Extraction and Aggregation of International Alliance Relations \n\nHere is the abstract of the paper: In this article, we explore how text mining can help producing reliable, high-level numerical data for multi-document relation extraction applications. We combine neural network techniques, information aggregation and visualization methods to extract event-based relations with a good tolerance to noise. We describe a search engine based on these methods, identifying the evolution of alliance and opposition relations between countries. \n\nHere is the partial review: - Strengths: Useful modeling contribution, and potentially useful annotated\ndata, for an important problem -- event extraction for the relationships\nbetween countries as expressed in news text.\n\n- Weaknesses: Many points are not explained well in the paper. \n\n- General Discussion:\n\nThis work tackles an important and interesting event extraction problem --\nidentifying positive and negative interactions between pairs of countries in\nthe world (or rather, between actors affiliated with countries).  The primary\ncontribution is an application of supervised, structured neural network models\nfor sentence-level event/relation extraction.  While previous work has examined\ntasks in the overall area, to my knowledge there has not been any publicly\navailble sentence-level annotated data for the problem -- the authors here make\na contribution as well by annotating some data included with the submission; if\nit is released, it could be useful for future researchers in this area.\n\nThe proposed models -- which seem to be an application of various\ntree-structured recursive neural network models -- demonstrate a nice\nperformance increase compared to a fairly convincing, broad set of baselines\n(if we are able to trust them; see below).  The paper also presents a manual\nevaluation of the inferred time series from a news corpus which is nice to see.\n\nI'm torn about this paper.  The problem is a terrific one and the application\nof the recursive models seems like a contribution to this problem. \nUnfortunately, many aspects of the models, experimentation, and evaluation are\nnot explained very well.  The same work, with a more carefully written paper,\ncould be really great.\n\nSome notes:\n\n- Baselines need more explanation.  For example, the sentiment lexicon is not\nexplained for the SVM.                    The LSTM classifier is left highly\nunspecified\n(L407-409) -- there are multiple different architectures to use an LSTM for\nclassification.  How was it trained?  Is there a reference for the approach? \nAre the authors using off-the-shelf code (in which case, please refer and cite,\nwhich would also make it easier for the reader to understand and replicate if\nnecessary)?  It would be impossible to replicate based on the two-line\nexplanation here.  \n\n- (The supplied code does not seem to include the baselines, just the recursive\nNN models.  It's great the authors supplied code for part of the system so I\ndon't want to penalize them for missing it -- but this is relevant since the\npaper itself has so few details on the baselines that they could not really be\nreplicated based on the explanation in the paper.)\n\n- How were the recursive NN models trained?\n\n- The visualization section is only a minor contribution; there isn't really\nany innovation or findings about what works or doesn't work here.\n\nLine by line:\n\nL97-99: Unclear. Why is this problem difficult?  Compared to what? (also the\nsentence is somewhat ungrammatical...)\n\nL231 - the trees are binarized, but how?\n\nFootnote 2 -- \"the tensor version\" - needs citation to explain what's being\nreferred to.\n\nL314: How are non-state verbs defined?                    Does the definition of\n\"event\nword\"s\nhere come from any \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "Output in the following json format:\n{\n    \"complete_review\": \"I would recommend revising and providing more details on the baselines, as well as the models, experimentation, and evaluation methods used in the paper. Additionally, the problem could be better explained, along with how the models were trained. The visualization section could also be improved to provide more insights. Overall, with more clarity and detail, this paper has the potential to be a great contribution to the field.\"\n}",
        "human_end_index": 3193
    },
    {
        "id": "382",
        "source": "PeerRead/acl_2017/",
        "title": "Creating Training Corpora for NLG Micro-Planners",
        "human_review": "- Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, Fran\u00e7ois Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413\u2013456.\n\nAmy Isard, 2016. \u201cThe Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts\u201d , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portoro\u017e, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline on the two corpora could disappear if Wen et al.'s\nsubstantially higher-scoring method were employed.  The assumption that this\nrelative difference would remain even with fancier methods should be made\nexplicit, e.g. by acknowledging the issue in a footnote.  Even with this\nlimitation, the comparison does still strike this reviewer as a useful\ncomponent of the overall comparison between the datasets.\n\nRe whether a paper about dataset creation should be able to get into ACL\nwithout system results:  though this indeed not unprecedented, the key issue is\nperhaps how novel and important the dataset is likely to be, and here this\nreviewer acknowledges the importance of the dataset in comparison to existing\nones (even if the key advance is in the already published content selection\nwork).\n\nFinally, this reviewer concurs with Reviewer 1 about the need to clarify the\nrole of domain dependence and what it means to be \"wide coverage\" in the final\nversion of the paper, if accepted.\n- Strengths:\n* Potentially valuable resource\n* Paper makes some good points\n\n- Weaknesses:\n* Awareness of related work (see below)\n* Is what the authors are trying to do (domain-independent microplanning) even\npossible (see below)\n* Are the crowdsourced texts appropriate (see below)\n\n- General Discussion:\nThis is an interesting paper which presents a potentially valuable resource,\nand I in many ways I am sympathetic to it.  However, I have some high-level\nconcerns, which are not addressed in the paper.  Perhaps the authors can\naddress these in their response.\n\n(1) I was a bit surprised by the constant reference and comparison to Wen 2016,\nwhich is a fairly obscure paper I have not previously heard of.  It would be\nbetter if the authors justified their work by comparison to well-known corpora,\nsuch as the ones they list in Section 2. Also, there are many other NLG\nprojects that looked at microplanning issue when verbalising DBPedia, indeed\nthere was a workshop in 2016 with many papers on NLG and DBPedia\n(https://webnlg2016.sciencesconf.org/  and\nhttp://aclweb.org/anthology/W/W16/#3500); see also previous work by Duboue and\nKutlak.  I would like to see less of a fixation on Wen (2016), and more\nawareness of other work on NLG and DBPedia.\n\n(2) Microplanning tends to be very domain/genre dependent.  For example,\npronouns are used much more often in novels than in aircraft maintenance\nmanuals.   This is why so much work has focused on domain-dependent resources. \n  So there are some real questions about whether it is possible even in theory\nto train a \"wide-coverage microplanner\".  The authors do not discuss this at\nall; they need to show they are aware of this concern.\n\n(3) I would be concerned about the quality of the texts obtained from\ncrowdsourcing.              A lot of people dont write very well, so it is not at all\nclear\nto me that gathering example texts from random crowdsourcers is going to\nproduce a good corpus for training microplanners.  Remember that the ultimate\ngoal of microplanning is to produce texts that are easy to *read*.  Imitating\nhuman writers (which is what this paper does, along with most learning\napproaches to microplanning) makes sense if we are confident that the human\nwriters have produced well-written easy-to-read texts.              Which is a\nreasonable\nassumption if the writers are professional journalists (for example), but a\nvery dubious one if the writers are random crowdsourcers.\n\nFrom a presentational perspective, the authors should ensure that all text in\ntheir paper meets the ACL font size criteria.  Some of the text in Fig 1 and\n(especially) Fig 2 is tiny and very difficult to read; this text should be the\nsame font size as the text in the body of the paper.\n\nI will initially rate this paper as borderline.  I look forward to seeing the\nauthor's response, and will adjust my rating accordingly.",
        "partial_review": "- Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, Fran\u00e7ois Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413\u2013456.\n\nAmy Isard, 2016. \u201cThe Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts\u201d , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portoro\u017e, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Creating Training Corpora for NLG Micro-Planners \n\nHere is the abstract of the paper: In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers.  We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016's. We show that while Wen et al.'s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task. \n\nHere is the partial review: - Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, Fran\u00e7ois Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413\u2013456.\n\nAmy Isard, 2016. \u201cThe Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts\u201d , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portoro\u017e, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "The paper presents a step towards developing more challenging corpora for training sentence planners in data-to-text NLG, which is a timely and important direction. However, there are some weaknesses in the paper that need to be addressed. \n\nFirstly, it is unclear whether the work reported in this paper represents a substantial advance over Perez-Beltrachini et al.'s (2016) method for selecting content. The authors do not directly compare the present paper to that one. It appears that the main novelty of this paper is the additional analysis, which is however rather superficial.\n\nSecondly, while it is good that the authors report a comparison of how an NNLG baseline fares on this corpus in comparison to that of Wen et al. (2016), the BLEU scores in Wen et al.'s paper appear to be much higher. This suggests that this NNLG baseline is not sufficient for an informative comparison, and therefore questions the validity of the comparisons made in this paper.\n\nIn the general discussion, the authors need to more clearly articulate why this paper should be considered a substantial advance over what has been published already by Perez-Beltrachini et al. Furthermore, it is not common for ACL to publish a main session paper on a corpus development methodology without presenting new results of a system using the corpus. The paper could be strengthened by including an analysis of the syntactic constructions in the two corpora, as well as providing more details about how the number of different path shapes are determined and how they are associated with the syntactic constructions.\n\nLastly, the authors should note the limitation that their method does not include richer discourse relations, such as Contrast, Consequence, or Background, which have long been central to NLG. It would be beneficial to discuss the corpora described by Walker et al. and Isard in comparison to the method described in this paper.\n\nOverall, the paper needs to address the weaknesses mentioned above and provide stronger justifications for the claims made. Additionally, including more detailed analyses and comparisons would significantly improve the paper's contribution to the field.",
        "human_end_index": 3860
    },
    {
        "id": "384",
        "source": "PeerRead/acl_2017/",
        "title": "Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition",
        "human_review": "- Strengths:\n - the model if theoretically solid and motivated by formal semantics. \n\n- Weaknesses:\n\n - The paper is about is-a relation extraction but the majority of literature\nabout taxonomization is not referenced in the paper, inter alia:\n\nFlati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.\n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\n\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens \u00a8\nLehmann, Richard Cyganiak, and Zachary Ive.\n2007. DBpedia: A nucleus for a web of open data.\n\nGerard de Melo and Gerhard Weikum. 2010. MENTA:\nInducing Multilingual Taxonomies from Wikipedia.\n\nZornitsa Kozareva and Eduard H. Hovy. 2010. A\nSemi-Supervised Method to Learn and Construct\nTaxonomies Using the Web. \n\nVivi Nastase, Michael Strube, Benjamin Boerschinger,\nCaecilia Zirn, and Anas Elghafari. 2010. WikiNet:\nA Very Large Scale Multi-Lingual Concept Network.\n\nSimone Paolo Ponzetto and Michael Strube. 2007.\nDeriving a large scale taxonomy from Wikipedia.\n\nSimone Paolo Ponzetto and Michael Strube. 2011.\nTaxonomy induction based on a collaboratively built\nknowledge repository. \n\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2008. YAGO: A large ontology from\nWikipedia and WordNet. \n\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\n2013. OntoLearn Reloaded: A graph-based algorithm\nfor taxonomy induction. \n\n - Experiments are poor, they only compare against \"Hearst patterns\" without\ntaking into account the works previously cited.\n\n- General Discussion:\n The paper is easy to follow and the supplementary material is also well\nwritten and useful, however the paper lack of references of is a relation\nextraction and taxonomization literature. The same apply for the experiments.\nIn fact no meaningful comparison is performed and the authors not even take\ninto account the existence of other systems (more recent than hearst patterns).\n\nI read authors answers but still i'm not convinced that they couldn't perform\nmore evaluations. I understand that they have a solid theoretical motivation\nbut still, i think that comparison are very important to asses if the\ntheoretical intuitions of the authors are confirmed also in practice. While\nit's true that all the works i suggested as comparison build taxonomies, is\nalso true that a comparison is possible considering the edges of a taxonomy.\n\nAnyway, considering the detailed author answer and the discussion with the\nother reviewer i can rise my score to 3 even if i still think that this paper\nis poor of experiments and does not frame correctly in the is-a relation\nextraction / taxonomy building literature.\n- Strengths:\n\nThis paper presents an approach for fine-grained IsA extraction by learning\nmodifier interpretations. The motivation of the paper is easy to understand and\nthis is an interesting task. In addition, the approach seems solid in general\nand the experimental results show that the approach increases in the number of\nfine-grained classes that can be populated.\n\n- Weaknesses:\n\nSome parts of the paper are hard to follow. It is unclear to me why D((e, p,\no)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is\nexplained as the product of how often e has been observed with some property\nand the weight of that property for the class MH. In addition, it also seems\nunclear how effective introducing compositional models itself is in increasing\nthe coverage. I think one of the major factors of the increase of the coverage\nis the modifier expansion, which seems to also be applicable to the baseline\n'Hearst'. It would be interesting to see the scores 'Hearst' with modifier\nexpansion.\n\n- General Discussion:\n\nOverall, the task is interesting and the approach is generally solid. However,\nsince this paper has weaknesses described above, I'm ambivalent about this\npaper.\n\n- Minor comment:\n\nI'm confused with some notations. For example, it is unclear for me what 'H'\nstands for. It seems that 'H' sometimes represents a class such as in (e, H)\n(- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is\nmy\nunderstanding correct?\n\nIn Paragraph \"Precision-Recall Analysis\", why the authors use area under the\nROC curve instead of area under the Precision-Recall curve, despite the\nparagraph title \"Precision-Recall Analysis\"?\n\n- After reading the response:\n\nThank you for the response. I'm not fully satisfied with the response as to the\nmodifier expansion. I do not think the modifier expansion can be applied to\nHearst as to the proposed method. However, I'm wondering whether there is no\nway to take into account the similar modifiers to improve the coverage of\nHearst. I'm actually between 3 and 4, but since it seems still unclear how\neffective introducing compositional models itself is, I keep my recommendation\nas it is.\n- strengths\nThis is a novel approach to modeling the compositional structure of complex\ncategories that maintains a set theoretic interpretation of common nouns and\nmodifiers, while also permitting a distributional interpretation of head\nmodification. The approach is well motivated and clearly defined and the\nexperiments show that show that this decomposed representation can improve upon\nthe Hearst-pattern derived IsA relations upon which it is trained in terms of\ncoverage.\n\n- weaknesses\nThe experiments are encouraging. However, it would be nice to see ROC curves\nfor the new approach alone, not in an ensemble with Hearst patterns. Table 5\ntells us that Mods_I increases coverage at the cost of precision and Figure 2\ntells us that Mods_I matches Hearst pattern precision for the high precision\nregion of the data. However, neither of these tell us whether the model can\ndistinguish between the high and low precision regions, and the ROC curves\n(which would tell us this) are only available for ensembled models.\n\nI believe that Eqn. 7 has an unnecessary $w$ since it is already the case that\n$w=D(\\rangle e, p, o \\langle)$.\n\n- discussion\nOverall, this is a nice idea that is well described and evaluated. I think this\npaper would be a good addition to ACL.",
        "partial_review": "- Strengths:\n - the model if theoretically solid and motivated by formal semantics. \n\n- Weaknesses:\n\n - The paper is about is-a relation extraction but the majority of literature\nabout taxonomization is not referenced in the paper, inter alia:\n\nFlati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.\n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\n\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens \u00a8\nLehmann, Richard Cyganiak, and Zachary Ive.\n2007. DBpedia: A nucleus for a web of open data.\n\nGerard de Melo and Gerhard Weikum. 2010. MENTA:\nInducing Multilingual Taxonomies from Wikipedia.\n\nZornitsa Kozareva and Eduard H. Hovy. 2010. A\nSemi-Supervised Method to Learn and Construct\nTaxonomies Using the Web. \n\nVivi Nastase, Michael Strube, Benjamin Boerschinger,\nCaecilia Zirn, and Anas Elghafari. 2010. WikiNet:\nA Very Large Scale Multi-Lingual Concept Network.\n\nSimone Paolo Ponzetto and Michael Strube. 2007.\nDeriving a large scale taxonomy from Wikipedia.\n\nSimone Paolo Ponzetto and Michael Strube. 2011.\nTaxonomy induction based on a collaboratively built\nknowledge repository. \n\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2008. YAGO: A large ontology from\nWikipedia and WordNet. \n\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\n2013. OntoLearn Reloaded: A graph-based algorithm\nfor taxonomy induction. \n\n - Experiments are poor, they only compare against \"Hearst patterns\" without\ntaking into account the works previously cited.\n\n- General Discussion:\n The paper is easy to follow and the supplementary material is also well\nwritten and useful, however the paper lack of references of is a relation\nextraction and taxonomization literature. The same apply for the experiments.\nIn fact no meaningful comparison is performed and the authors not even take\ninto account the existence of other systems (more recent than hearst patterns).\n\nI read authors answers but still i'm not convinced that they couldn't perform\nmore evaluations. I understand that they have a solid theoretical motivation\nbut still, i think that comparison are very important to asses if the\ntheoretical intuitions of the authors are confirmed also in practice. While\nit's true that all the works i suggested as comparison build taxonomies, is\nalso true that a comparison is possible considering the edges of a taxonomy.\n\nAnyway, considering the detailed author answer and the discussion with the\nother reviewer i can rise my score to 3 even if i still think that this paper\nis poor of experiments and does not frame correctly in the is-a relation\nextraction / taxonomy building literature.\n- Strengths:\n\nThis paper presents an approach for fine-grained IsA extraction by learning\nmodifier interpretations. The motivation of the paper is easy to understand and\nthis is an interesting task. In addition, the approach seems solid in general\nand the experimental results show that the approach increases in the number of\nfine-grained classes that can be populated.\n",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition \n\nHere is the abstract of the paper: We present a method for populating fine-grained classes (e.g., \u201c1950s American jazz musicians\u201d) with instances (e.g., Charles Mingus ). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns. \n\nHere is the partial review: - Strengths:\n - the model if theoretically solid and motivated by formal semantics. \n\n- Weaknesses:\n\n - The paper is about is-a relation extraction but the majority of literature\nabout taxonomization is not referenced in the paper, inter alia:\n\nFlati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.\n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\n\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens \u00a8\nLehmann, Richard Cyganiak, and Zachary Ive.\n2007. DBpedia: A nucleus for a web of open data.\n\nGerard de Melo and Gerhard Weikum. 2010. MENTA:\nInducing Multilingual Taxonomies from Wikipedia.\n\nZornitsa Kozareva and Eduard H. Hovy. 2010. A\nSemi-Supervised Method to Learn and Construct\nTaxonomies Using the Web. \n\nVivi Nastase, Michael Strube, Benjamin Boerschinger,\nCaecilia Zirn, and Anas Elghafari. 2010. WikiNet:\nA Very Large Scale Multi-Lingual Concept Network.\n\nSimone Paolo Ponzetto and Michael Strube. 2007.\nDeriving a large scale taxonomy from Wikipedia.\n\nSimone Paolo Ponzetto and Michael Strube. 2011.\nTaxonomy induction based on a collaboratively built\nknowledge repository. \n\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2008. YAGO: A large ontology from\nWikipedia and WordNet. \n\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\n2013. OntoLearn Reloaded: A graph-based algorithm\nfor taxonomy induction. \n\n - Experiments are poor, they only compare against \"Hearst patterns\" without\ntaking into account the works previously cited.\n\n- General Discussion:\n The paper is easy to follow and the supplementary material is also well\nwritten and useful, however the paper lack of references of is a relation\nextraction and taxonomization literature. The same apply for the experiments.\nIn fact no meaningful comparison is performed and the authors not even take\ninto account the existence of other systems (more recent than hearst patterns).\n\nI read authors answers but still i'm not convinced that they couldn't perform\nmore evaluations. I understand that they have a solid theoretical motivation\nbut still, i think that comparison are very important to asses if the\ntheoretical intuitions of the authors are confirmed also in practice. While\nit's true that all the works i suggested as comparison build taxonomies, is\nalso true that a comparison is possible considering the edges of a taxonomy.\n\nAnyway, considering the detailed author answer and the discussion with the\nother reviewer i can rise my score to 3 even if i still think that this paper\nis poor of experiments and does not frame correctly in the is-a relation\nextraction / taxonomy building literature.\n- Strengths:\n\nThis paper presents an approach for fine-grained IsA extraction by learning\nmodifier interpretations. The motivation of the paper is easy to understand and\nthis is an interesting task. In addition, the approach seems solid in general\nand the experimental results show that the approach increases in the number of\nfine-grained classes that can be populated.\n \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Strengths:\\n- the model if theoretically solid and motivated by formal semantics.\\n\\nWeaknesses:\\n- The paper is about is-a relation extraction but the majority of literature\\nabout taxonomization is not referenced in the paper, inter alia:\\n\\nFlati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.\\n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\\n\\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens \u00a8\\nLehmann, Richard Cyganiak, and Zachary Ive.\\n2007. DBpedia: A nucleus for a web of open data.\\n\\nGerard de Melo and Gerhard Weikum. 2010. MENTA:\\nInducing Multilingual Taxonomies from Wikipedia.\\n\\nZornitsa Kozareva and Eduard H. Hovy. 2010. A\\nSemi-Supervised Method to Learn and Construct\\nTaxonomies Using the Web.\\n\\nVivi Nastase, Michael Strube, Benjamin Boerschinger,\\nCaecilia Zirn, and Anas Elghafari. 2010. WikiNet:\\nA Very Large Scale Multi-Lingual Concept Network.\\n\\nSimone Paolo Ponzetto and Michael Strube. 2007.\\nDeriving a large scale taxonomy from Wikipedia.\\n\\nSimone Paolo Ponzetto and Michael Strube. 2011.\\nTaxonomy induction based on a collaboratively built\\nknowledge repository.\\n\\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\\nWeikum. 2008. YAGO: A large ontology from\\nWikipedia and WordNet.\\n\\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\\n2013. OntoLearn Reloaded: A graph-based algorithm\\nfor taxonomy induction.\\n\\n- Experiments are poor, they only compare against \\\"Hearst patterns\\\" without\\ntaking into account the works previously cited.\\n\\n- General Discussion:\\n The paper is easy to follow and the supplementary material is also well\\nwritten and useful, however the paper lack of references of is a relation\\nextraction and taxonomization literature. The same apply for the experiments.\\nIn fact no meaningful comparison is performed and the authors not even take\\ninto account the existence of other systems (more recent than hearst patterns).\\n\\nI read authors answers but still i'm not convinced that they couldn't perform\\nmore evaluations. I understand that they have a solid theoretical motivation\\nbut still, i think that comparison are very important to asses if the\\ntheoretical intuitions of the authors are confirmed also in practice. While\\nit's true that all the works i suggested as comparison build taxonomies, is\\nalso true that a comparison is possible considering the edges of a taxonomy.\\n\\nAnyway, considering the detailed author answer and the discussion with the\\nother reviewer i can rise my score to 3 even if i still think that this paper\\nis poor of experiments and does not frame correctly in the is-a relation\\nextraction / taxonomy building literature.\\n\\nStrengths:\\n\\nThis paper presents an approach for fine-grained IsA extraction by learning\\nmodifier interpretations. The motivation of the paper is easy to understand and\\nthis is an interesting task. In addition, the approach seems solid in general\\nand the experimental results show that the approach increases in the number of\\nfine-grained classes that can be populated.\"\n}",
        "human_end_index": 2987
    },
    {
        "id": "387",
        "source": "PeerRead/acl_2017/",
        "title": "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
        "human_review": "- Strengths:\n\nThis paper tackles an interesting problem and provides a (to my knowledge)\nnovel and reasonable way of learning and combining cognitive features with\ntextual features for sentiment analysis and irony detection. The paper is \nclearly written and organized, and the authors provided a lot of useful detail\nand informative example and plots. Most of the results are convincing, and the\nauthors did a good job comparing their approach and results with previous work.\n\n- Weaknesses:\n\n1. Just from the reading abstract, I expected that the authors' approach would\nsignificantly outperform previous methods, and that using both the eye-gaze and\ntextual features consistently yields the best results. Upon reading the actual\nresults section, however, it seems like the findings were more mixed. I think\nit would be helpful to update the abstract and introduction to reflect this. \n2. When evaluating the model on dataset 1 for sentiment analysis, were the\nsarcastic utterances included? Did the model do better on classifying the\nnon-sarcastic utterances than the sarcastic ones?\n3. I understand why the eye-movement data would be useful for sarcasm\ndetection, but it wasn't as obvious to me why it would be helpful for\n(non-sarcastic) sentiment classification beyond the textual features. \n\n- General Discussion:\n\nThis paper contains a lot of interesting content, and the approach seems solid\nand novel to me. The results were a little weaker than I had anticipated from\nthe abstract, but I believe would still be interesting to the larger community\nand merits publication.\n- Strengths:\n\n(1) A deep CNN framework is proposed to extract and combine cognitive features\nwith textual features for sentiment analysis and sarcasm detection. \n\n(2) The ideas is interesting and novelty.\n\n- Weaknesses:\n\n(1) Replicability would be an important concern. Researchers cannot replicate\nthe system/method for improvement due to lack of data for feature extraction. \n\n- General Discussion:\n\nOverall, this paper is well written and organized. The experiments are\nconducted carefully for comparison with previous work and the analysis is\nreasonable. I offer some comments as follows.\n\n(1)           Does this model be suitable on sarcastic/non-sarcastic utterances?\nThe\nauthors should provide more details for further analysis. \n\n(2)           Why the eye-movement data would be useful for\nsarcastic/non-sarcastic\nsentiment classification beyond the textual features? The authors should\nprovide more explanations.",
        "partial_review": "- Strengths:\n\nThis paper tackles an interesting problem and provides a (to my knowledge)\nnovel and reasonable way of learning and combining cognitive features with\ntextual features for sentiment analysis and irony detection. The paper is \nclearly written and organized, and the authors provided a lot of useful detail\nand informative example and plots. Most of the results are convincing, and the\nauthors did a good job comparing their approach and results with previous work.\n\n- Weaknesses:\n\n1. Just from the reading abstract, I expected that the authors' approach would\nsignificantly outperform previous methods, and that using both the eye-gaze and\ntextual features consistently yields the best results. Upon reading the actual\nresults section, however, it seems like the findings were more mixed. I think\nit would be helpful to update the abstract and introduction to reflect this. \n2. When evaluating the model on dataset 1 for sentiment analysis, were the\nsarcastic utterances included? Did the model do better on classifying the\nnon-sarcastic utterances than the sarcastic ones?\n3. I understand why the eye-movement data would be useful for sarcasm\ndetection, but it wasn't as obvious to me why it would be helpful for\n(non-sarcastic) sentiment classification beyond the text",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network \n\nHere is the abstract of the paper: Cognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system.  We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i)  CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features. \n\nHere is the partial review: - Strengths:\n\nThis paper tackles an interesting problem and provides a (to my knowledge)\nnovel and reasonable way of learning and combining cognitive features with\ntextual features for sentiment analysis and irony detection. The paper is \nclearly written and organized, and the authors provided a lot of useful detail\nand informative example and plots. Most of the results are convincing, and the\nauthors did a good job comparing their approach and results with previous work.\n\n- Weaknesses:\n\n1. Just from the reading abstract, I expected that the authors' approach would\nsignificantly outperform previous methods, and that using both the eye-gaze and\ntextual features consistently yields the best results. Upon reading the actual\nresults section, however, it seems like the findings were more mixed. I think\nit would be helpful to update the abstract and introduction to reflect this. \n2. When evaluating the model on dataset 1 for sentiment analysis, were the\nsarcastic utterances included? Did the model do better on classifying the\nnon-sarcastic utterances than the sarcastic ones?\n3. I understand why the eye-movement data would be useful for sarcasm\ndetection, but it wasn't as obvious to me why it would be helpful for\n(non-sarcastic) sentiment classification beyond the text \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper provides a novel approach to learning and combining cognitive features with textual features for sentiment analysis and sarcasm detection. The paper is well-written and organized, with informative examples and plots. However, there are a few weaknesses that could be addressed.\\n\\n- In the abstract and introduction, it would be helpful to clearly state that the findings were mixed, rather than presenting the approach as consistently outperforming previous methods. This would align the expectations of the readers with the actual results presented.\\n\\n- In the evaluation of the sentiment analysis model on dataset 1, it is unclear whether the sarcastic utterances were included. It would be useful to clarify this and also mention if the model performed better on classifying non-sarcastic utterances compared to sarcastic ones.\\n\\n- While it is evident why eye-movement data would be beneficial for sarcasm detection, it is less clear why it would improve non-sarcastic sentiment classification beyond the textual features. It would be helpful to provide some insights or explanations in this regard.\\n\\nOverall, the paper presents an interesting approach and provides convincing results. Addressing the mentioned weaknesses would further strengthen the paper's contribution.\"\n}",
        "human_end_index": 1282
    },
    {
        "id": 388,
        "source": "PeerRead/acl_2017/",
        "title": "Universal Semantic Parsing",
        "human_review": "This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependency grammar structures into [what the paper calls] semantic\nlogical form representations.  In essence, each UD construct is assigned a\ntarget construction in logical form, and a procedure is defined to effect the\nconversion, working \u2018inside-out\u2019 using an intermediate form to ensure\nproper nesting of substructures into encapsulating ones.  Two evaluations are\ncarried out: comparing the results to gold-standard lambda structures and\nmeasuring the effectiveness of the resulting lambda expressions in actually\ndelivering the answers to questions from two QA sets.  \n\nIt is impossible to describe all this adequately in the space provided.  The\nauthors have taken some care to cover all principal parts, but there are still\nmany missing details.  I would love to see a longer version of the paper! \nParticularly the QA results are short-changed; it would have been nice to learn\nwhich types of question are not handled, and which are not answered correctly,\nand why not.  This information would have been useful to gaining better insight\ninto the limitations of the logical form representations.  \n\nThat leads to my main concern/objection.  This logical form representation is\nnot in fact a \u2018real\u2019 semantic one.                          It is, essentially, a\nrather\nclose\nrewrite of the dependency structure of the input, with some (good) steps toward\n\u2018semanticization\u2019, including the insertion of lambda operators, the\nexplicit inclusion of dropped arguments (via the enhancement operation), and\nthe introduction of appropriate types/units for such constructions as eventive\nadjectives and nouns like \u201crunning horse\u201d and \u201cpresident in 2009\u201d.  But\nmany (even simple) aspects of semantic are either not present (at least, not in\nthe paper) and/or simply wrong.  Missing: quantification (as in \u201cevery\u201d or\n\u201call\u201d); numbers (as in \u201c20\u201d or \u201cjust over 1000\u201d); various forms of\nreference (as in \u201che\u201d, \u201cthat man\u201d, \u201cwhat I said before\u201d); negation\nand modals, which change the semantics in interesting ways; inter-event\nrelationships (as in the subevent relationship between the events in \u201cthe\nvacation was nice, but traveling was a pain\u201d; etc. etc.  To add them one can\neasily cheat, by treating these items as if they were just unusual words and\ndefining obvious and simple lambda formulas for them.  But they in fact require\nspecific treatment; for example, a number requires the creation of a separate\nset object in the representation, with its own canonical variable (allowing\nlater text to refer to \u201cone of them\u201d and bind the variable properly).  For\nanother example, Person A\u2019s model of an event may differ from Person B\u2019s,\nso one needs two representation symbols for the event, plus a coupling and\nmapping between them.  For another example, one has to be able to handle time,\neven if simply by temporally indexing events and states.  None of this is here,\nand it is not immediately obvious how this would be added.  In some cases, as\nDRT shows, quantifier and referential scoping is not trivial.  \n\nIt is easy to point to missing things, and unfair to the paper in some sense;\nyou can\u2019t be expected to do it all.  But you cannot be allowed to make\nobvious errors.  Very disturbing is the assignment of event relations strictly\nin parallel with the verb\u2019s (or noun\u2019s) syntactic roles.  No-one can claim\nseriously that \u201che broke the window\u201d and \u201cthe window broke\u201d has\n\u201che\u201d and \u201cthe window\u201d filling the same semantic role for \u201cbreak\u201d. \nThat\u2019s simply not correct, and one cannot dismiss the problem, as the paper\ndoes, to some nebulous subsequent semantic processing.                          This\nreally\nneeds\nadequate treatment, even in this paper.  This is to my mind the principal\nshortcoming of this work; for me this is the make-or-break point as to whether\nI would fight to have the paper accepted in the conference.  (I would have been\nfar happier if the authors had simply acknowledged that this aspect is wrong\nand will be worked on in future, with a sketch saying how: perhaps by reference\nto FrameNet and semantic filler requirements.)                          \n\nIndependent of the representation, the notation conversion procedure is\nreasonably clear.  I like the facts that it is rather cleaner and simpler than\nits predecessor (based on Stanford dependencies), and also that the authors\nhave the courage of submitting non-neural work to the ACL in these days of\nunbridled and giddy enthusiasm for anything neural.",
        "partial_review": "This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependency grammar structures into [what the paper calls] semantic\nlogical form representations.  In essence, each UD construct is assigned a\ntarget construction in logical form, and a procedure is defined to effect the\nconversion, working \u2018inside-out\u2019 using an intermediate form to ensure\nproper nesting of substructures into encapsulating ones.  Two evaluations are\ncarried out: comparing the results to gold-standard lambda structures and\nmeasuring the effectiveness of the resulting lambda expressions in actually\ndelivering the answers to questions from two QA sets.  \n\nIt is impossible to describe all this adequately in the space provided.  The\nauthors have taken some care to cover all principal parts, but there are still\nmany missing details.  I would love to see a longer version of the paper! \nParticularly the QA results are short-changed; it would have been nice to learn\nwhich types of question are not handled, and which are not answered correctly,\nand why not.  This information would have been useful to gaining better insight\ninto the limitations of the logical form representations.  \n\nThat leads to my main concern/objection.  This logical form representation is\nnot in fact a \u2018real\u2019 semantic one.                          It is, essentially, a\nrather\nclose\nrewrite of the dependency structure of the input, with some (good) steps toward\n\u2018semanticization\u2019, including the insertion of lambda operators, the\nexplicit inclusion of dropped arguments (via the enhancement operation), and\nthe introduction of appropriate types/units for such constructions as eventive\nadjectives and nouns like \u201crunning horse\u201d and \u201cpresident in 2009\u201d.  But\nmany (even simple) aspects of semantic are either not present (at least, not in\nthe paper) and/or simply wrong.  Missing: quantification (as in \u201cevery\u201d or\n\u201call\u201d); numbers (as in \u201c20\u201d or \u201cjust over 1000\u201d); various forms of\nreference (as in \u201che\u201d, \u201cthat man\u201d, \u201cwhat I said before\u201d); negation\nand modals, which change the semantics in interesting ways; inter-event\nrelationships (as in the subevent relationship between the events in \u201cthe\nvacation was nice, but traveling was a pain\u201d; etc. etc.  To add them one can\neasily cheat, by treating these items as if they were just unusual words an",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Universal Semantic Parsing \n\nHere is the abstract of the paper: Universal Dependencies (UD) provides a cross-linguistically uniform syntactic representation, with the aim of advancing multilingual applications of parsing and natural language understanding. Reddy et al. (2016) recently developed a semantic interface for (English) Stanford Dependencies, based on the lambda calculus.  In this work, we introduce UDepLambda, a similar semantic interface for UD, which allows mapping natural language to logical forms in an almost language-independent framework. We evaluate our approach on semantic parsing for the task of question answering against Freebase.  To facilitate multilingual evaluation, we provide German and Spanish translations of the WebQuestions and GraphQuestions datasets. Results show that UDepLambda outperforms strong baselines across languages and datasets.  For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions. \n\nHere is the partial review: This paper describes interesting and ambitious work: the automated conversion\nof Universal Dependency grammar structures into [what the paper calls] semantic\nlogical form representations.  In essence, each UD construct is assigned a\ntarget construction in logical form, and a procedure is defined to effect the\nconversion, working \u2018inside-out\u2019 using an intermediate form to ensure\nproper nesting of substructures into encapsulating ones.  Two evaluations are\ncarried out: comparing the results to gold-standard lambda structures and\nmeasuring the effectiveness of the resulting lambda expressions in actually\ndelivering the answers to questions from two QA sets.  \n\nIt is impossible to describe all this adequately in the space provided.  The\nauthors have taken some care to cover all principal parts, but there are still\nmany missing details.  I would love to see a longer version of the paper! \nParticularly the QA results are short-changed; it would have been nice to learn\nwhich types of question are not handled, and which are not answered correctly,\nand why not.  This information would have been useful to gaining better insight\ninto the limitations of the logical form representations.  \n\nThat leads to my main concern/objection.  This logical form representation is\nnot in fact a \u2018real\u2019 semantic one.                          It is, essentially, a\nrather\nclose\nrewrite of the dependency structure of the input, with some (good) steps toward\n\u2018semanticization\u2019, including the insertion of lambda operators, the\nexplicit inclusion of dropped arguments (via the enhancement operation), and\nthe introduction of appropriate types/units for such constructions as eventive\nadjectives and nouns like \u201crunning horse\u201d and \u201cpresident in 2009\u201d.  But\nmany (even simple) aspects of semantic are either not present (at least, not in\nthe paper) and/or simply wrong.  Missing: quantification (as in \u201cevery\u201d or\n\u201call\u201d); numbers (as in \u201c20\u201d or \u201cjust over 1000\u201d); various forms of\nreference (as in \u201che\u201d, \u201cthat man\u201d, \u201cwhat I said before\u201d); negation\nand modals, which change the semantics in interesting ways; inter-event\nrelationships (as in the subevent relationship between the events in \u201cthe\nvacation was nice, but traveling was a pain\u201d; etc. etc.  To add them one can\neasily cheat, by treating these items as if they were just unusual words an \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper describes interesting and ambitious work: the automated conversion of Universal Dependency grammar structures into semantic logical form representations. The authors have taken some care to cover all principal parts, but there are still many missing details. It would be beneficial to see a longer version of the paper to provide a more comprehensive understanding. The QA results are lacking in detail, and it would have been helpful to know which types of questions are not handled and which are not answered correctly, as well as the limitations of the logical form representations. My main concern is that the logical form representation in the paper is not a true semantic representation. It is more of a modified rewrite of the dependency structure, with some improvements towards semanticization, but it lacks important semantic aspects such as quantification, numbers, reference, negation, modals, and inter-event relationships. These missing elements should be addressed to enhance the semantic accuracy of the proposed approach.\"\n}",
        "human_end_index": 2333
    },
    {
        "id": 395,
        "source": "PeerRead/acl_2017/",
        "title": "DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations",
        "human_review": "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular sense selection and representation process. The learning is\nachieved by a message passing scheme between the two modules that is cast as a\nreinforcement learning problem by the authors.\n\n- Strengths:\n\nThe paper is generally well written, presents most of its ideas clearly and\nmakes apt comparisons to related work where required. The experiments are well\nstructured and the results are overall good, though not outstanding. However,\nthere are several problems with the paper that prevent me from endorsing it\ncompletely.\n\n- Weaknesses:\n\nMy main concern with the paper is the magnification of its central claims,\nbeyond their actual worth.\n\n1) The authors use the term \"deep\" in their title and then several times in the\npaper. But they use a skip-gram architecture (which is not deep). This is\nmisrepresentation.\n\n2) Also reinforcement learning is one of the central claims of this paper.\nHowever, to the best of my understanding, the motivation and implementation\nlacks clarity. Section 3.2 tries to cast the task as a reinforcement learning\nproblem but goes on to say that there are 2 major drawbacks, due to which a\nQ-learning algorithm is used. This algorithm does not relate to the originally\nclaimed policy.\n\nFurthermore, it remains unclear how novel their modular approach is. Their work\nseems to be very similar to EM learning approaches, where an optimal sense is\nselected in the E step and an objective is optimized in the M step to yield\nbetter sense representations. The authors do not properly distinguish their\napproach, nor motivative why RL should be preferred over EM in the first place.\n\n3) The authors make use of the term pure-sense representations multiple times,\nand claim this as a central contribution of their paper. I am not sure what\nthis means, or why it is beneficial.\n\n4) They claim linear-time sense selection in their model. Again, it is not\nclear to me how this is the case. A highlighting of this fact in the relevant\npart of the paper would be helpful. \n\n5) Finally, the authors claim state-of-the-art results. However, this is only\non a single MaxSimC metric. Other work has achieved overall better results\nusing the AvgSimC metric. So, while state-of-the-art isn't everything about a\npaper, the claim that this paper achieves it - in the abstract and intro - is\nat least a little misleading.\nThis paper describes a novel approach for learning multi-sense word\nrepresentations using reinforcement learning. A CBOW-like architecture is used\nfor sense selection, computing a score for each sense based on the dot product\nbetween the sum of word embeddings in the current context and the corresponding\nsense vector. A second module based on the skip-gram model is used to train\nsense representations, given results from the sense selection module. In order\nto train these two modules, the authors apply Q-Learning, where the Q-value is\nprovided by the CBOW-based sense selection module. The reward is given by the\nskip-gram negative sampling likelihood. Additionally, the authors propose an\napproach for determining the number of senses for each word non-parametrically,\nby creating new senses when the Q-values for existing scores have a score under\n0.5.\n\nThe resulting approach achieves good results under the \"MaxSimC\" metric, and\nresults comparable to previous approaches under \"AvgSimC\". The authors suggest\nthat their approach could be used to improve the performance for downstream\ntasks by replacing word embeddings with their most probable sense embedding. It\nwould have been nice to see this claim explored, perhaps in a sequential\nlabeling task such as POS-tagging or NER, especially in light of previous work\nquestioning the usefulness of multi-sense representations in downstream tasks.\nI found it somewhat misleading to suggest that relying on MaxSimC could reduce\noverhead in a real world application, as the sense disambiguation step (with\nassociated parameters) would still be required, in addition to the sense\nembeddings. A clustering-based approach using a weighted average of sense\nrepresentations would have similar overhead. The claims about improving over\nword2vec using 1/100 of the data are also not particularly surprising on SCWS.\nThese are misleading contributions, as they do not advance/differ much from\nprevious work.\n\nThe modular quality of their approach results in a flexibility that I think\ncould have been explored further. The sense disambiguation module uses a vector\naveraging (CBOW) approach. A positive aspect of their model is that they should\nbe able to substitute other context composition approaches (using alternative\nneural architecture composition techniques) relatively easily.\n\nThe paper applies an interesting approach to a problem that has been explored\nnow in many ways. The results on standard benchmarks are comparable to previous\nwork, but not particularly surprising/interesting. However, the approach goes\nbeyond a simple extension of the skip-gram model for multi-sense representation\nlearning by providing a modular framework based on reinforcement learning.\nIdeally, this aspect would be explored further. But overall, the approach\nitself may be interesting enough on its own to be considered for acceptance, as\nit could help move research in this area forward.\n\n* There are a number of typos that should be addressed (line\n190--representations*, 331--selects*, 492--3/4th*).\n\nNOTE: Thank you to the authors for their response.\nTMP\nStrength: The paper propose DRL-Sense model that shows a marginal improvement\non SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.\n\nWeakness:\nThe technical aspects of the paper raise several concerns:\nCould the authors clarify two drawbacks in 3.2? The first drawback states that\noptimizing equation (2) leads to the underestimation of the probability of\nsense. As I understand, eq(2) is the expected reward of sense selection, z_{ik}\nand z_{jl} are independent actions and there are only two actions to optimize.\nThis should be relatively easy. In NLP setting, optimizing the expected rewards\nover a sequence of actions for episodic-task has been proven doable (Sequence\nLevel Training with Recurrent Neural Networks, Ranzato 2015) even in a more\nchallenging setting of machine translation where the number of actions ~30,000\nand the average sequence length ~30 words. The DRL-Sense model has maximum 3\nactions and it does not have sequential nature of RL. This makes it hard to\naccept the claim about the first drawback.\n\nThe second drawback, accompanied with the detail math in Appendix A, states\nthat the update formula is to minimize the likelihood due to the log-likelihood\nis negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, \u2026)\nminimize a function f, however, a common practice when we want to maximize f we\njust minimize -f. Since the reward defined in the paper is negative, any\nstandard optimizer can be use on the expected of the negative reward, which is\nalways greater than 0. This is often done in many modeling tasks such as\nlanguage model, we minimize negative log-likelihood instead of maximizing the\nlikelihood. The authors also claim that when \u201cthe log-likelihood reaches 0,\nit also indicates that the likelihood reaches infinity and computational flow\non U and V\u201d (line 1046-1049). Why likelihood\u2192infinity? Should it be\nlikelihood\u21921?\n\nCould the authors also explain how DRL -Sense is based on Q-learning? The\nhorizon in the model is length of 1. There is no transition between\nstate-actions and there is not Markov-property as I see it (k, and l are draw\nindependently). I am having trouble to see the relation between Q-learning and\nDRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment\nwhereas in the paper, the rewards is computed by the model. What\u2019s the reward\nin DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy\nin eq(4)?  \n\nCross entropy is defined as H(p, q) = -\\sum_{x} q(x)\\log q(x), which variable\ndo the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar\n(computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total\nnumber of senses eq(1). These two categorial variables do not have the same\ndimension, how is cross-entropy H in eq(4) is computed then?\n\nCould the authors justify the dropout exploration? Why not epsilon-greedy\nexploration? Dropout is often used for model regularization, preventing\noverfitting. How do the authors know the gain in using dropout is because of\nexploration but regularization?\n\nThe authors states that Q-value is a probabilistic estimation (line 419), can\nyou elaborate what is the set of variables the distribution is defined? When\nyou sum over that set of variable, do you get 1? I interpret that Q is a\ndistribution over senses per word, however  definition of q in eq(3) does not\ncontain a normalizing constant, so I do not see q is a valid distribution. This\nalso related to the value 0.5 in section 3.4 as a threshold for exploration.\nWhy 0.5 is chosen here where q is just an arbitrary number between (0, 1) and\nthe constrain \\sum_z q(z) = 1 does not held? Does the authors allow the\ncreation of a new sense in the very beginning or after a few training epochs? I\nwould image that at the beginning of training, the model is unstable and\ncreating new senses might introduce noises to the model.  Could the authors\ncomment on that?\n\nGeneral discussion\nWhat\u2019s the justification for omitting negative samples in line 517? Negative\nsampling has been use successfully in word2vec due to the nature of the task:\nlearning representation. Negative sampling, however does not work well when the\nmain interest is modeling a distribution p() over senses/words. Noise\ncontrastive estimation is often preferred when it comes to modeling a\ndistribution. The DRL-Sense, uses collocation likelihood to compute the reward,\nI wonder how the approximation presented in the paper affects the learning of\nthe embeddings.\n\nWould the authors consider task-specific evaluation for sense embeddings as\nsuggested in recent research [1,2]\n\n[1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor\nLabutov, David Mimno and Thorsten Joachims.\n\n[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks .\nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer\n\n---\nI have read the response.",
        "partial_review": "This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular sense selection and representation process. The learning is\nachieved by a message passing scheme between the two modules that is cast as a\nreinforcement learning problem by the authors.\n\n- Strengths:\n\nThe paper is generally well written, presents most of its ideas clearly and\nmakes apt comparisons to related work where required. The experiments are well\nstructured and the results are overall good, though not outstanding. However,\nthere are several problems with the paper that prevent me from endorsing it\ncompletely.\n\n- Weaknesses:\n\nMy main concern with the paper is the magnification of its central claims,\nbeyond their actual worth.\n\n1) The authors use the term \"deep\" in their title and then several times in the\npaper. But they use a skip-gram architecture (which is not deep). This is\nmisrepresentation.\n\n2) Also reinforcement learning is one of the central claims of this paper.\nHowever, to the best of my understanding, the motivation and implementation\nlacks clarity. Section 3.2 tries to cast the task as a reinforcement learning\nproblem but goes on to say that there are 2 major drawbacks, due to which a\nQ-learning algorithm is used. This algorithm does not relate to the originally\nclaimed policy.\n\nFurthermore, it remains unclear how novel their modular approach is. Their work\nseems to be very similar to EM learning approaches, where an optimal sense is\nselected in the E step and an objective is optimized in the M step to yield\nbetter sense representations. The authors do not properly distinguish their\napproach, nor motivative why RL should be preferred over EM in the first place.\n\n3) The authors make use of the term pure-sense representations multiple times,\nand claim this as a central contribution of their paper. I am not sure what\nthis means, or why it is beneficial.\n\n4) They claim linear-time sense selection in their model. Again, it is not\nclear to me how this is the case. A highlighting of this fact in the relevant\npart of the paper would be helpful. \n\n5) Finally, the authors claim state-of-the-art results. However, this is only\non a single MaxSimC metric. Other work has achieved overall better results\nusing the AvgSimC metric. So, while state-of-the-art isn't everything about a\npaper, the claim that this paper achieves it - in the abstract and intro - is\nat least a little misleading.\nThis paper describes a novel approach for learning multi-sense word\nrepresentations using reinforcement learning. A CBOW-like architecture is used\nfor sense selection, computing a score for each sense based on the dot product\nbetween the sum of word embeddings in the current context and the corresponding\nsense vector. A second module based on the skip-gram model is used to train\nsense representations, given results from the sense selection module. In order\nto train these two modules, the authors apply Q-Learning, where the Q-value is\nprovided by the CBOW-based sense selection module. The reward is given by the\nskip-gram negative sampling likelihood. Additionally, the authors propose an\napproach for determining the number of senses for each word non-parametrically,\nby creating new senses when the Q-values for existing scores have a score under\n0.5.\n\nThe resulting approach achieves good results under the \"MaxSimC\" metric, and\nresults comparable to previous approaches under \"AvgSimC\". The authors suggest\nthat their approach could be used to improve the performance for downstream\ntasks by replacing word embeddings with their most probable sense embedding. It\nwould have been nice to see this claim explored, perhaps in a sequential\nlabeling task such as POS-tagging or NER, especially in light of previous work\nquestioning the usefulness of multi-sense representations in downstream tasks.\nI found it somewhat misleading to suggest that relying on MaxSimC could reduce\noverhead in a real world application, as the sense disambiguation step (with\nassociated parameters) would still be required, in addition to the sense\nembeddings. A clustering-based approach using a weighted average of sense\nrepresentations would have similar overhead. The claims about improving over\nword2vec using 1/100 of the data are also not particularly surprising on SCWS.\nThese are misleading contributions, as they do not advance/differ much from\nprevious work.\n\nThe modular quality of their approach results in a flexibility that I think\ncould have been explored further. The sense disambiguation module uses a vector\naveraging (CBOW) approach. A positive aspect of their model is that they should\nbe able to substitute other context composition approaches (using alternative\nneural architecture composition techniques) relatively easily.\n\nThe paper applies an interesting approach to a problem that has been explored\nnow in many ways. The results on standard benchmarks are comparable to previous\nwork, but not particularly surprising/interesting. However, the approach goes\nbeyond a simple extension of the skip-gram model for multi-sense representation\nlearning by providing a modular framework based on reinforcement l",
        "task3_prompt": "\nComplete a partially written peer review about the paper: DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations \n\nHere is the abstract of the paper: This paper proposes DRL-Sense--a multi-sense word representation learning model, to address the word sense ambiguity issue, where a sense selection module and a sense representation module are jointly learned in a reinforcement learning fashion. A novel reward passing procedure is proposed to enable joint training on the selection and representation modules. The modular design implements pure sense-level representation learning with linear time sense selection (decoding). We further develop a non-parametric learning algorithm and a sense exploration mechanism for better flexibility and robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on contextual word similarities and comparable performance with Google's word2vec while using much less training data. \n\nHere is the partial review: This paper outlines a method to learn sense embeddings from unannotated corpora\nusing a modular sense selection and representation process. The learning is\nachieved by a message passing scheme between the two modules that is cast as a\nreinforcement learning problem by the authors.\n\n- Strengths:\n\nThe paper is generally well written, presents most of its ideas clearly and\nmakes apt comparisons to related work where required. The experiments are well\nstructured and the results are overall good, though not outstanding. However,\nthere are several problems with the paper that prevent me from endorsing it\ncompletely.\n\n- Weaknesses:\n\nMy main concern with the paper is the magnification of its central claims,\nbeyond their actual worth.\n\n1) The authors use the term \"deep\" in their title and then several times in the\npaper. But they use a skip-gram architecture (which is not deep). This is\nmisrepresentation.\n\n2) Also reinforcement learning is one of the central claims of this paper.\nHowever, to the best of my understanding, the motivation and implementation\nlacks clarity. Section 3.2 tries to cast the task as a reinforcement learning\nproblem but goes on to say that there are 2 major drawbacks, due to which a\nQ-learning algorithm is used. This algorithm does not relate to the originally\nclaimed policy.\n\nFurthermore, it remains unclear how novel their modular approach is. Their work\nseems to be very similar to EM learning approaches, where an optimal sense is\nselected in the E step and an objective is optimized in the M step to yield\nbetter sense representations. The authors do not properly distinguish their\napproach, nor motivative why RL should be preferred over EM in the first place.\n\n3) The authors make use of the term pure-sense representations multiple times,\nand claim this as a central contribution of their paper. I am not sure what\nthis means, or why it is beneficial.\n\n4) They claim linear-time sense selection in their model. Again, it is not\nclear to me how this is the case. A highlighting of this fact in the relevant\npart of the paper would be helpful. \n\n5) Finally, the authors claim state-of-the-art results. However, this is only\non a single MaxSimC metric. Other work has achieved overall better results\nusing the AvgSimC metric. So, while state-of-the-art isn't everything about a\npaper, the claim that this paper achieves it - in the abstract and intro - is\nat least a little misleading.\nThis paper describes a novel approach for learning multi-sense word\nrepresentations using reinforcement learning. A CBOW-like architecture is used\nfor sense selection, computing a score for each sense based on the dot product\nbetween the sum of word embeddings in the current context and the corresponding\nsense vector. A second module based on the skip-gram model is used to train\nsense representations, given results from the sense selection module. In order\nto train these two modules, the authors apply Q-Learning, where the Q-value is\nprovided by the CBOW-based sense selection module. The reward is given by the\nskip-gram negative sampling likelihood. Additionally, the authors propose an\napproach for determining the number of senses for each word non-parametrically,\nby creating new senses when the Q-values for existing scores have a score under\n0.5.\n\nThe resulting approach achieves good results under the \"MaxSimC\" metric, and\nresults comparable to previous approaches under \"AvgSimC\". The authors suggest\nthat their approach could be used to improve the performance for downstream\ntasks by replacing word embeddings with their most probable sense embedding. It\nwould have been nice to see this claim explored, perhaps in a sequential\nlabeling task such as POS-tagging or NER, especially in light of previous work\nquestioning the usefulness of multi-sense representations in downstream tasks.\nI found it somewhat misleading to suggest that relying on MaxSimC could reduce\noverhead in a real world application, as the sense disambiguation step (with\nassociated parameters) would still be required, in addition to the sense\nembeddings. A clustering-based approach using a weighted average of sense\nrepresentations would have similar overhead. The claims about improving over\nword2vec using 1/100 of the data are also not particularly surprising on SCWS.\nThese are misleading contributions, as they do not advance/differ much from\nprevious work.\n\nThe modular quality of their approach results in a flexibility that I think\ncould have been explored further. The sense disambiguation module uses a vector\naveraging (CBOW) approach. A positive aspect of their model is that they should\nbe able to substitute other context composition approaches (using alternative\nneural architecture composition techniques) relatively easily.\n\nThe paper applies an interesting approach to a problem that has been explored\nnow in many ways. The results on standard benchmarks are comparable to previous\nwork, but not particularly surprising/interesting. However, the approach goes\nbeyond a simple extension of the skip-gram model for multi-sense representation\nlearning by providing a modular framework based on reinforcement l \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"earning. This modular design allows for flexibility to substitute alternative context composition approaches. However, the paper falls short in several aspects. Firstly, the authors claim to use deep reinforcement learning, but they employ a skip-gram architecture that is not deep. Secondly, the implementation of reinforcement learning lacks clarity and the authors do not sufficiently distinguish their approach from EM learning techniques. Additionally, the concept of 'pure-sense representations' is not well-defined. The claim of linear-time sense selection is unclear and should be further explained. Lastly, while the paper achieves state-of-the-art results on the MaxSimC metric, it is important to note that other metrics have yielded better results in previous work. Overall, the approach is interesting, but the paper needs improvement in terms of clarity, novelty, and claims made.\"\n}",
        "human_end_index": 5141
    },
    {
        "id": 419,
        "source": "PeerRead/acl_2017/",
        "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion",
        "human_review": "The paper introduces a simple and effective method for morphological paradigm\ncompletion in low-resource settings. The method uses a character-based seq2seq\nmodel trained on a mix of examples in two languages: a resource-poor language\nand a closely-related resource-rich language; each training example is\nannotated with a paradigm properties and a language ID. Thus, the model enables\ntransfer learning across languages when the two languages share common\ncharacters and common paradigms. While the proposed multi-lingual solution is\nnot novel (similar architectures have been explored in syntax, language\nmodeling, and MT), the novelty of this paper is to apply the approach to\nmorphology. Experimental results show substantial improvements over monolingual\nbaselines, and include a very thorough analysis of the impact of language\nsimilarities on the quality of results. The paper is interesting, very clearly\nwritten, I think it\u2019ll be a nice contribution to the conference program. \n\nDetailed comments: \n\n\u2014 My main question is why the proposed general multilingual methodology was\nlimited to pairs of languages, rather than to sets of similar languages? For\nexample, all Romance languages could be included in the training to improve\nSpanish paradigm completion, and all Slavic languages with Cyrillic script\ncould be mixed to improve Ukrainian. It would be interesting to see the\nextension of the models from bi-lingual to multilingual settings. \n\n\u2014 I think Arabic is not a fair (and fairly meaningless) baseline, given how\ndifferent is its script and morphology from the target languages. A more\ninteresting baseline would be, e.g., a language with a partially shared\nalphabet but a different typology. For example, a Slavic language with Latin\nscript could be used as a baseline language for Romance languages. If Arabic is\nexcluded, and if we consider a most distant language in the same the same\nfamily as a baseline, experimental results are still strong. \n\n\u2014 A half-page discussion of contribution of Arabic as a regularizer also adds\nlittle to the paper; I\u2019d just remove Arabic from all the experiments and\nwould add a regularizer (which, according to footnote 5, works even better than\nadding Arabic as a transfer language).              \n\n\u2014 Related work is missing a line of work on \u201clanguage-universal\u201d RNN\nmodels that use basically the same approach: they learn shared parameters for\ninputs in multiple languages, and add a language tag to the input to mediate\nbetween languages. Related studies include a multilingual parser (Ammar et al.,\n2016), language models (Tsvetkov et al., 2016), and machine translation\n(Johnson et al., 2016 )\n\nMinor: \n\u2014 I don\u2019t think that the claim is correct in line 144 that POS tags are\neasy to transfer across languages. Transfer of POS annotations is also a\nchallenging task.  \n\nReferences: \n\nWaleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah\nA.\nSmith. \"Many languages, one parser.\u201d TACL 2016. \n\nYulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick\nLittell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. \"Polyglot\nneural language models: A case study in cross-lingual phonetic representation\nlearning.\u201d NAACL 2016.\n\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng\nChen, Nikhil Thorat et al. \"Google's Multilingual Neural Machine Translation\nSystem: Enabling Zero-Shot Translation.\" arXiv preprint arXiv:1611.04558 2016.\n\n-- Response to author response: \n\nThanks for your response & I'm looking forward to reading the final version!",
        "partial_review": "The paper introduces a simple and effective method for morphological paradigm\ncompletion in low-resource settings. The method uses a character-based seq2seq\nmodel trained on a mix of examples in two languages: a resource-poor language\nand a closely-related resource-rich language; each training example is\nannotated with a paradigm properties and a language ID. Thus, the model enables\ntransfer learning across languages when the two languages share common\ncharacters and common paradigms. While the proposed multi-lingual solution is\nnot novel (similar architectures have been explored in syntax, language\nmodeling, and MT), the novelty of this paper is to apply the approach to\nmorphology. Experimental results show substantial improvements over monolingual\nbaselines, and include a very thorough analysis of the impact of language\nsimilarities on the quality of results. The paper is interesting, very clearly\nwritten, I think it\u2019ll be a nice contribution to the conference program. \n\nDetailed comments: \n\n\u2014 My main question is why the proposed general multilingual methodology was\nlimited to pairs of languages, rather than to sets of similar languages? For\nexample, all Romance languages could be included in the training to improve\nSpanish paradigm completion, and all Slavic languages with Cyrillic script\ncould be mixed to improve Ukrainian. It would be interesting to see the\nextension of the models from bi-lingual to multilingual settings. \n\n\u2014 I think Arabic is not a fair (and fairly meaningless) baseline, given how\ndifferent is its script and morphology from the target languages. A more\ninteresting baseline would be, e.g., a language with a partially shared\nalphabet but a different typology. For example, a Slavic language with Latin\nscript could be used as a baseline language for Rom",
        "task3_prompt": "\nComplete a partially written peer review about the paper: One-Shot Neural Cross-Lingual Transfer for Paradigm Completion \n\nHere is the abstract of the paper: We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge. \n\nHere is the partial review: The paper introduces a simple and effective method for morphological paradigm\ncompletion in low-resource settings. The method uses a character-based seq2seq\nmodel trained on a mix of examples in two languages: a resource-poor language\nand a closely-related resource-rich language; each training example is\nannotated with a paradigm properties and a language ID. Thus, the model enables\ntransfer learning across languages when the two languages share common\ncharacters and common paradigms. While the proposed multi-lingual solution is\nnot novel (similar architectures have been explored in syntax, language\nmodeling, and MT), the novelty of this paper is to apply the approach to\nmorphology. Experimental results show substantial improvements over monolingual\nbaselines, and include a very thorough analysis of the impact of language\nsimilarities on the quality of results. The paper is interesting, very clearly\nwritten, I think it\u2019ll be a nice contribution to the conference program. \n\nDetailed comments: \n\n\u2014 My main question is why the proposed general multilingual methodology was\nlimited to pairs of languages, rather than to sets of similar languages? For\nexample, all Romance languages could be included in the training to improve\nSpanish paradigm completion, and all Slavic languages with Cyrillic script\ncould be mixed to improve Ukrainian. It would be interesting to see the\nextension of the models from bi-lingual to multilingual settings. \n\n\u2014 I think Arabic is not a fair (and fairly meaningless) baseline, given how\ndifferent is its script and morphology from the target languages. A more\ninteresting baseline would be, e.g., a language with a partially shared\nalphabet but a different typology. For example, a Slavic language with Latin\nscript could be used as a baseline language for Rom \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper introduces a simple and effective method for morphological paradigm completion in low-resource settings. The method uses a character-based seq2seq model trained on a mix of examples in two languages: a resource-poor language and a closely-related resource-rich language; each training example is annotated with a paradigm properties and a language ID. Thus, the model enables transfer learning across languages when the two languages share common characters and common paradigms. While the proposed multi-lingual solution is not novel (similar architectures have been explored in syntax, language modeling, and MT), the novelty of this paper is to apply the approach to morphology. Experimental results show substantial improvements over monolingual baselines, and include a very thorough analysis of the impact of language similarities on the quality of results. The paper is interesting, very clearly written, and I think it will be a nice contribution to the conference program.\\n\\nDetailed comments:\\n\\n- My main question is why the proposed general multilingual methodology was limited to pairs of languages, rather than to sets of similar languages? For example, all Romance languages could be included in the training to improve Spanish paradigm completion, and all Slavic languages with Cyrillic script could be mixed to improve Ukrainian. It would be interesting to see the extension of the models from bi-lingual to multilingual settings.\\n\\n- I think Arabic is not a fair (and fairly meaningless) baseline, given how different its script and morphology are from the target languages. A more interesting baseline would be, for example, a language with a partially shared alphabet but a different typology. For example, a Slavic language with Latin script could be used as a baseline language for Romance languages.\\n\\n\"\n}",
        "human_end_index": 1802
    },
    {
        "id": "433",
        "source": "PeerRead/acl_2017/",
        "title": "Universal Dependencies Parsing for Colloquial Singaporean English",
        "human_review": "The paper describes a deep-learning-based model for parsing the creole\nSingaporean English to Universal Dependencies. They implement a parser based on\nthe model by Dozat and Manning (2016) and add neural stacking (Chen et al.,\n2016) to it. They train an English model and then use some of the hidden\nrepresentations of the English model as input to their Singlish parser. This\nallows them to make use of the much larger English training set along with a\nsmall Singlish treebank, which they annotate. They show that their approach\n(LAS 76.57) works better than just using an English parser (LAS 65.6) or\ntraining a parser on their small Singlish data set (LAS 64.01). They also\nanalyze for which\ncommon constructions, their approach improves parsing quality. \n\nThey also describe and evaluate a stacked POS model based on Chen et al.\n(2016), they discuss how common constructions should be analyzed in the UD\nframework, and they provide an annotated treebank of 1,200 sentences. 100 of\nthem were annotated by two people and their inter-annotator agreement was 85.3\nUAS and 75.7 LAS.\n\n- Strengths:\n\n - They obtain good results and their experimental setup appears to be solid.\n\n - They perform many careful analyses and explore the influence on many\nparameters of their model.\n\n - They provide a small Singlish treebank annotated according to the Universal\nDependencies v1.4 guidelines.\n\n - They propose very sound guidelines on how to analyze common Singlish\nconstructions in UD.\n\n - Their method is linguistically informed and they nicely exploit similarity\nbetween standard English and the creole Singaporean English.\n\n - The paper presents methods for a low-resource language.\n\n - They are not just applying an existing English method to another language\nbut instead present a method that can be potentially used for other closely\nrelated language pairs.\n\n - They use a well-motivated method for selecting the sentences to include in\ntheir treebank.\n\n - The paper is very well written and easy to read.\n\n- Weaknesses:\n\n - The annotation quality seems to be rather poor. They performed double\nannotation of 100 sentences and their inter-annotator agreement is just 75.72%\nin terms of LAS. This makes it hard to assess how reliable the estimate of the\nLAS of their model is, and the LAS of their model is in fact slightly higher\nthan the inter-annotator agreement. \n\nUPDATE: Their rebuttal convincingly argued that the second annotator who just\nannotated the 100 examples to compute the IAA didn't follow the annotation\nguidelines for several common constructions. Once the second annotator fixed\nthese issues, the IAA was reasonable, so I no longer consider this a real\nissue.\n\n- General Discussion:\n\nI am a bit concerned about the apparently rather poor annotation quality of the\ndata and how this might influence the results, but overall, I liked the paper\na lot and I think this would be a good contribution to the conference.\n\n- Questions for the authors:\n\n - Who annotated the sentences? You just mention that 100 sentences were\nannotated by one of the authors to compute inter=annotator agreement but you\ndon't mention who annotated all the sentences.\n\n - Why was the inter-annotator agreement so low? In which cases was there\ndisagreement? Did you subsequently discuss and fix the sentences for which\nthere was disagreement?\n\n - Table A2: There seem to be a lot of discourse relations (almost as many as\ndobj relations) in your treebank. Is this just an artifact of the colloquial\nlanguage or did you use \"discourse\" for things that are not considered\n\"discourse\" in other languages in UD?\n\n - Table A3: Are all of these discourse particles or discourse + imported\nvocab? If the latter, perhaps put them in separate tables, and glosses would be\nhelpful.\n\n- Low-level comments:\n\n - It would have been interesting if you had compared your approach to the one\nby Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you\nshould mention this paper in the reference section.\n\n - You use the word \"grammar\" in a slightly strange way. I think replacing\n\"grammar\" with syntactic constructions would make it clearer what you try to\nconvey. (e.g., line 90)\n\n - Line 291: I don't think this can be regarded as a variant of\nit-extraposition. But I agree with the analysis in Figure 2, so perhaps just\nget rid of this sentence.\n\n - Line 152: I think the model by Dozat and Manning (2016) is no longer\nstate-of-the art, so perhaps just replace it with \"very high performing model\"\nor something like that.\n\n - It would be helpful if you provided glosses in Figure 2.\n- Strengths:\nNice results, nice data set. Not so much work on Creole-like languages,\nespecially English.  \n\n- Weaknesses:\nA global feeling of \"Deja-vu\", a lot of similar techniques have been applied to\nother domains, other ressource-low languages. Replace word embeddings by\nclusters and neural models by whatever was in fashion 5 years ago and we can\nfind more or less the same applied to Urdu or out-of-domain parsing. I liked\nthis paper though, but I would have appreciated the authors to highlight more\ntheir contributions and position their work better within the literature.\n\n- General Discussion:\n\nThis paper presents a set of experiments designed a) to show the effectiveness\nof a neural parser  in a scarce resource scenario and b) to introduce a new\ndata set of Creole English (from Singapour, called Singlish). While this data\nset is relatively small (1200 annotated sentences, used with 80k unlabeled\nsentences for word embeddings induction), the authors manage to present\nrespectable results via interesting approach even though using features from\nrelatively close languages are not unknown from the parsing community (see all\nthe line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based\nparsers, and so on).\nAssuming we can see Singlish as an extreme of Out-of-domain English and given\nall the set of experiments, I wonder why the authors didn\u2019t try the classical\ntechnique on domain-adaptation, namely training with UD_EN+90% of the Singlish\nwithin a 10 cross fold experiment ? just so we can have another interesting\nbaseline (with and without word embeddings, with bi-lingual embeddings if\nenough parallel data is available).\nI think that paper is interesting but I really would have appreciated more\npositioning regarding all previous work in parsing low-ressources languages and\nextreme domain adaptation. A table presenting some results for Irish and other\nvery small treebanks would be nice.\nAlso how come the IAA is so low regarding the labeled relations?\n\n*****************************************\nNote after reading the authors' answer\n*****************************************\n\nThanks for your clarifications (especially for redoing the IAA evaluation). I\nraised my recommendation to 4, I hope it'll get accepted.\nThe authors construct a new dataset of 1200 Singaporean English (Singlish)\nsentences annotated with Universal Dependencies. They show that they can\nimprove the performance of a POS tagger and a dependency parser on the Singlish\ncorpus by integrating English syntactic knowledge via a neural stacking model.\n\n- Strengths:\nSinglish is a low-resource language. The NLP community needs more data for low\nresource languages, and the dataset accompanying this paper is a useful\ncontribution. There is also relatively little NLP research on creoles, and the\npotential of using transfer-learning to analyze creoles, and this paper makes a\nnice contribution in that area.\n\nThe experimental setup used by the authors is clear. They provide convincing\nevidence that incorporating knowledge from an English-trained parser into a\nSinglish parser outperforms both an English-only parser and a Singlish-only\nparser on the Singlish data. They also provide a good overview of the relevant\ndifferences between English and Singlish for the purposes of syntactic parser\nand a useful analysis of how different parsing models handle these\nSinglish-specific constructions.\n\n- Weaknesses:\n\nThere are three main issues I see with this paper:\n*  There is insufficient comparison to the UD annotation of non-English\nlanguages. Many of the constructions they bring up as specific to Singlish are\nalso present in other UD languages, and the annotations should ideally be\nconsistent between Singlish and these languages.\n*  I'd like to see an analysis on the impact of training data size. A central\nclaim of this paper is that using English data can improve performance on a\nlow-resource language like Singlish. How much more Singlish data would be\nneeded before the English data became unnecessary?\n*  What happens if you train a single POS/dep parsing model on the concatenated\nUD Web and Singlish datasets? This is much simpler than incorporating neural\nstacking. The case for neural stacking is stronger if it can outperform this\nbaseline.\n\n- General Discussion:\nLine 073: \u201cPOS taggers and dependency parsers perform poorly on such Singlish\ntexts based on our observations\u201d - be more clear that you will quantify this\nlater. As such, it seems a bit hand-wavy.\n\nLine 169: Comparison to neural network models for multi-lingual parsing. As far\nas I can tell, you don't directly try the approach of mapping Singlish and\nEnglish word embeddings into the same embedding space.\n\nLine 212: Introduction of UD Eng. At this point, it is appropriate to point out\nthat the Singlish data is also web data, so the domain matches UD Eng.\n\nLine 245: \u201cAll borrowed words are annotated according to their original\nmeanings\u201d. Does this mean they have the same POS as in  the language from\nwhich they were borrowed? Or the POS of their usage in Singlish?\n\nFigure 2: Standard English glosses would be very useful in understanding the\nconstructions and checking the correctness of the UD relations used.\n\nLine 280: Topic prominence: You should compare with the \u201cdislocated\u201d label\nin UD. From the UD paper: \u201cThe dislocated relation captures preposed (topics)\nand postposed elements\u201d. The syntax you are describing sounds similar to a\ntopic-comment-style syntax; if it is different, then you should make it clear\nhow.\n\nLine 294: \u201cSecond, noun phrases used to modify the predicate with the\npresence of a preposition is regarded as a \u201cnsubj\u201d (nominal subject).\u201d\nHere, I need a gloss to determine if this analysis makes sense. If the phrase\nis really being used to modify the predicate, then this should not be nsubj. UD\nmakes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If\nthis is a case of modification, then you should use one of the modification\nrelations, not a core argument relation. Should clarify the language here.\n\nLine 308: \u201cIn UD-Eng standards, predicative \u201cbe\u201d is the only verb used as\na copula, which often depends on its complement to avoid copular head.\u201d This\nis an explicit decision made in UD, to increase parallelism with non-copular\nlanguages (e.g., Singlish). You should call this out. I think the rest of the\ndiscussion of copula handling is not necessary.\n\nLine 322: \u201cNP deletion: Noun-phrase (NP) deletion often results in null\nsubjects or objects.\u201d This is common in other languages (zero-anaphora in\ne.g. Spanish, Italian, Russian, Japanese\u2026 )Would be good to point this out,\nand also point to how this is dealt with in UD in those languages (I believe\nthe same way you handle it).\n\nLing 330: Subj/verb inversion is common in interrogatives in other languages\n(\u201cFue Marta al supermercado/Did Marta go to the supermarket?\u201d). Tag\nquestions are present in English (though perhaps are not as frequent). You\nshould make sure that your analysis is consistent with these languages.\n\nSec 3.3 Data Selection and Annotation:\nThe way you chose the Singlish sentences, of course an English parser will do\npoorly (they are chosen to be disimilar to sentences an English parser has seen\nbefore). But do you have a sense of how a standard English parser does overall\non Singlish, if it is not filtered this way? How common are sentences with\nout-of-vocabulary terms or the constructions you discussed in 3.2?\n\nA language will not necessarily capture unusual sentence structure,\nparticularly around long-distance dependencies. Did you investigate whether\nthis method did a good job of capturing sentences with the grammatical\ndifferences to English you discussed in Section 3.2?\n\nLine 415: \u201cthe inter-annotator agreement has an unlabeled attachment score\n(UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.\u201d\n*  What\u2019s the agreement on POS tags? Is this integrated with LAS?\n*  Note that in Silveira et al 2014, which produced UD-Eng, they measured 94%\ninter-annotator agreement on a per-token basis. Why the discrepancy?\n\nPOS tagging and dep parsing sections:\nFor both POS-tagging and dep parsing, I\u2019d like to see some analysis on the\neffect of training set size. E.g., how much more Singlish data would be needed\nto train a POS tagger/dep parser entirely on Singlish and get the same accuracy\nas the stacked model?\n\nWhat happens if you just concatenate the datasets? E.g., train a model on a\nhybrid dataset of EN and Singlish, and see what the result is?\n\nLine 681: typo: \u201cpre-rained\u201d should be \u201cpre-trained\u201d\n\n742 \u201cThe neural stacking model leads to the biggest improvement over nearly\nall categories except for a slightly lower yet competitive performance on \u201cNP\nDeletion\u201d cases\u201d --- seems that the English data strongly biases the parser\nto expect an explicit subj/obj. you could try deleting subj/obj from some\nEnglish sentences to improve performance on this construction.",
        "partial_review": "The paper describes a deep-learning-based model for parsing the creole\nSingaporean English to Universal Dependencies. They implement a parser based on\nthe model by Dozat and Manning (2016) and add neural stacking (Chen et al.,\n2016) to it. They train an English model and then use some of the hidden\nrepresentations of the English model as input to their Singlish parser. This\nallows them to make use of the much larger English training set along with a\nsmall Singlish treebank, which they annotate. They show that their approach\n(LAS 76.57) works better than just using an English parser (LAS 65.6) or\ntraining a parser on their small Singlish data set (LAS 64.01). They also\nanalyze for which\ncommon constructions, their approach improves parsing quality. \n\nThey also describe and evaluate a stacked POS model based on Chen et al.\n(2016), they discuss how common constructions should be analyzed in the UD\nframework, and they provide an annotated treebank of 1,200 sentences. 100 of\nthem were annotated by two people and their inter-annotator agreement was 85.3\nUAS and 75.7 LAS.\n\n- Strengths:\n\n - They obtain good results and their experimental setup appears to be solid.\n\n - They perform many careful analyses and explore the influence on many\nparameters of their model.\n\n - They provide a small Singlish treebank annotated according to the Universal\nDependencies v1.4 guidelines.\n\n - They propose very sound guidelines on how to analyze common Singlish\nconstructions in UD.\n\n - Their method is linguistically informed and they nicely exploit similarity\nbetween standard English and the creole Singaporean English.\n\n - The paper presents methods for a low-resource language.\n\n - They are not just applying an existing English method to another language\nbut instead present a method that can be potentially used for other closely\nrelated language pairs.\n\n - They use a well-motivated method for selecting the sentences to include in\ntheir treebank.\n\n - The paper is very well written and easy to read.\n\n- Weaknesses:\n\n - The annotation quality seems to be rather poor. They performed double\nannotation of 100 sentences and their inter-annotator agreement is just 75.72%\nin terms of LAS. This makes it hard to assess how reliable the estimate of the\nLAS of their model is, and the LAS of their model is in fact slightly higher\nthan the inter-annotator agreement. \n\nUPDATE: Their rebuttal convincingly argued that the second annotator who just\nannotated the 100 examples to compute the IAA didn't follow the annotation\nguidelines for several common constructions. Once the second annotator fixed\nthese issues, the IAA was reasonable, so I no longer consider this a real\nissue.\n\n- General Discussion:\n\nI am a bit concerned about the apparently rather poor annotation quality of the\ndata and how this might influence the results, but overall, I liked the paper\na lot and I think this would be a good contribution to the conference.\n\n- Questions for the authors:\n\n - Who annotated the sentences? You just mention that 100 sentences were\nannotated by one of the authors to compute inter=annotator agreement but you\ndon't mention who annotated all the sentences.\n\n - Why was the inter-annotator agreement so low? In which cases was there\ndisagreement? Did you subsequently discuss and fix the sentences for which\nthere was disagreement?\n\n - Table A2: There seem to be a lot of discourse relations (almost as many as\ndobj relations) in your treebank. Is this just an artifact of the colloquial\nlanguage or did you use \"discourse\" for things that are not considered\n\"discourse\" in other languages in UD?\n\n - Table A3: Are all of these discourse particles or discourse + imported\nvocab? If the latter, perhaps put them in separate tables, and glosses would be\nhelpful.\n\n- Low-level comments:\n\n - It would have been interesting if you had compared your approach to the one\nby Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you\nshould mention this paper in the reference section.\n\n - You use the word \"grammar\" in a slightly strange way. I think replacing\n\"grammar\" with syntactic constructions would make it clearer what you try to\nconvey. (e.g., line 90)\n\n - Line 291: I don't think this can be regarded as a variant of\nit-extraposition. But I agree with the analysis in Figure 2, so perhaps just\nget rid of this sentence.\n\n - Line 152: I think the model by Dozat and Manning (2016) is no longer\nstate-of-the art, so perhaps just replace it with \"very high performing model\"\nor something like that.\n\n - It would be helpful if you provided glosses in Figure 2.\n- Strengths:\nNice results, nice data set. Not so much work on Creole-like languages,\nespecially English.  \n\n- Weaknesses:\nA global feeling of \"Deja-vu\", a lot of similar techniques have been applied to\nother domains, other ressource-low languages. Replace word embeddings by\nclusters and neural models by whatever was in fashion 5 years ago and we can\nfind more or less the same applied to Urdu or out-of-domain parsing. I liked\nthis paper though, but I would have appreciated the authors to highlight more\ntheir contributions and position their work better within the literature.\n\n- General Discussion:\n\nThis paper presents a set of experiments designed a) to show the effectiveness\nof a neural parser  in a scarce resource scenario and b) to introduce a new\ndata set of Creole English (from Singapour, called Singlish). While this data\nset is relatively small (1200 annotated sentences, used with 80k unlabeled\nsentences for word embeddings induction), the authors manage to present\nrespectable results via interesting approach even though using features from\nrelatively close languages are not unknown from the parsing community (see all\nthe line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based\nparsers, and so on).\nAssuming we can see Singlish as an extreme of Out-of-domain English and given\nall the set of experiments, I wonder why the authors didn\u2019t try the classical\ntechnique on domain-adaptation, namely training with UD_EN+90% of the Singlish\nwithin a 10 cross fold experiment ? just so we can have another interesting\nbaseline (with and without word embeddings, with bi-lingual embeddings if\nenough parallel data is available).\nI think that paper is interesting but I really would have appreciated more\npositioning regarding all previous work in parsing low-ressources languages and\nextreme domain adaptation. A table presenting some results for Irish and other\nvery small treebanks would be nice.\nAlso how come the IAA is so low regarding the labeled relations?\n\n*****************************************\nNote after reading the authors' answer\n*****************************************\n\nThanks for your clarifications (especi",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Universal Dependencies Parsing for Colloquial Singaporean English \n\nHere is the abstract of the paper: Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research. \n\nHere is the partial review: The paper describes a deep-learning-based model for parsing the creole\nSingaporean English to Universal Dependencies. They implement a parser based on\nthe model by Dozat and Manning (2016) and add neural stacking (Chen et al.,\n2016) to it. They train an English model and then use some of the hidden\nrepresentations of the English model as input to their Singlish parser. This\nallows them to make use of the much larger English training set along with a\nsmall Singlish treebank, which they annotate. They show that their approach\n(LAS 76.57) works better than just using an English parser (LAS 65.6) or\ntraining a parser on their small Singlish data set (LAS 64.01). They also\nanalyze for which\ncommon constructions, their approach improves parsing quality. \n\nThey also describe and evaluate a stacked POS model based on Chen et al.\n(2016), they discuss how common constructions should be analyzed in the UD\nframework, and they provide an annotated treebank of 1,200 sentences. 100 of\nthem were annotated by two people and their inter-annotator agreement was 85.3\nUAS and 75.7 LAS.\n\n- Strengths:\n\n - They obtain good results and their experimental setup appears to be solid.\n\n - They perform many careful analyses and explore the influence on many\nparameters of their model.\n\n - They provide a small Singlish treebank annotated according to the Universal\nDependencies v1.4 guidelines.\n\n - They propose very sound guidelines on how to analyze common Singlish\nconstructions in UD.\n\n - Their method is linguistically informed and they nicely exploit similarity\nbetween standard English and the creole Singaporean English.\n\n - The paper presents methods for a low-resource language.\n\n - They are not just applying an existing English method to another language\nbut instead present a method that can be potentially used for other closely\nrelated language pairs.\n\n - They use a well-motivated method for selecting the sentences to include in\ntheir treebank.\n\n - The paper is very well written and easy to read.\n\n- Weaknesses:\n\n - The annotation quality seems to be rather poor. They performed double\nannotation of 100 sentences and their inter-annotator agreement is just 75.72%\nin terms of LAS. This makes it hard to assess how reliable the estimate of the\nLAS of their model is, and the LAS of their model is in fact slightly higher\nthan the inter-annotator agreement. \n\nUPDATE: Their rebuttal convincingly argued that the second annotator who just\nannotated the 100 examples to compute the IAA didn't follow the annotation\nguidelines for several common constructions. Once the second annotator fixed\nthese issues, the IAA was reasonable, so I no longer consider this a real\nissue.\n\n- General Discussion:\n\nI am a bit concerned about the apparently rather poor annotation quality of the\ndata and how this might influence the results, but overall, I liked the paper\na lot and I think this would be a good contribution to the conference.\n\n- Questions for the authors:\n\n - Who annotated the sentences? You just mention that 100 sentences were\nannotated by one of the authors to compute inter=annotator agreement but you\ndon't mention who annotated all the sentences.\n\n - Why was the inter-annotator agreement so low? In which cases was there\ndisagreement? Did you subsequently discuss and fix the sentences for which\nthere was disagreement?\n\n - Table A2: There seem to be a lot of discourse relations (almost as many as\ndobj relations) in your treebank. Is this just an artifact of the colloquial\nlanguage or did you use \"discourse\" for things that are not considered\n\"discourse\" in other languages in UD?\n\n - Table A3: Are all of these discourse particles or discourse + imported\nvocab? If the latter, perhaps put them in separate tables, and glosses would be\nhelpful.\n\n- Low-level comments:\n\n - It would have been interesting if you had compared your approach to the one\nby Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you\nshould mention this paper in the reference section.\n\n - You use the word \"grammar\" in a slightly strange way. I think replacing\n\"grammar\" with syntactic constructions would make it clearer what you try to\nconvey. (e.g., line 90)\n\n - Line 291: I don't think this can be regarded as a variant of\nit-extraposition. But I agree with the analysis in Figure 2, so perhaps just\nget rid of this sentence.\n\n - Line 152: I think the model by Dozat and Manning (2016) is no longer\nstate-of-the art, so perhaps just replace it with \"very high performing model\"\nor something like that.\n\n - It would be helpful if you provided glosses in Figure 2.\n- Strengths:\nNice results, nice data set. Not so much work on Creole-like languages,\nespecially English.  \n\n- Weaknesses:\nA global feeling of \"Deja-vu\", a lot of similar techniques have been applied to\nother domains, other ressource-low languages. Replace word embeddings by\nclusters and neural models by whatever was in fashion 5 years ago and we can\nfind more or less the same applied to Urdu or out-of-domain parsing. I liked\nthis paper though, but I would have appreciated the authors to highlight more\ntheir contributions and position their work better within the literature.\n\n- General Discussion:\n\nThis paper presents a set of experiments designed a) to show the effectiveness\nof a neural parser  in a scarce resource scenario and b) to introduce a new\ndata set of Creole English (from Singapour, called Singlish). While this data\nset is relatively small (1200 annotated sentences, used with 80k unlabeled\nsentences for word embeddings induction), the authors manage to present\nrespectable results via interesting approach even though using features from\nrelatively close languages are not unknown from the parsing community (see all\nthe line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based\nparsers, and so on).\nAssuming we can see Singlish as an extreme of Out-of-domain English and given\nall the set of experiments, I wonder why the authors didn\u2019t try the classical\ntechnique on domain-adaptation, namely training with UD_EN+90% of the Singlish\nwithin a 10 cross fold experiment ? just so we can have another interesting\nbaseline (with and without word embeddings, with bi-lingual embeddings if\nenough parallel data is available).\nI think that paper is interesting but I really would have appreciated more\npositioning regarding all previous work in parsing low-ressources languages and\nextreme domain adaptation. A table presenting some results for Irish and other\nvery small treebanks would be nice.\nAlso how come the IAA is so low regarding the labeled relations?\n\n*****************************************\nNote after reading the authors' answer\n*****************************************\n\nThanks for your clarifications (especi \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper describes a deep-learning-based model for parsing the creole Singaporean English to Universal Dependencies. They implement a parser based on the model by Dozat and Manning (2016) and add neural stacking (Chen et al., 2016) to it. They train an English model and then use some of the hidden representations of the English model as input to their Singlish parser. This allows them to make use of the much larger English training set along with a small Singlish treebank, which they annotate. They show that their approach (LAS 76.57) works better than just using an English parser (LAS 65.6) or training a parser on their small Singlish data set (LAS 64.01). They also analyze for which common constructions, their approach improves parsing quality.\\n\\nThey also describe and evaluate a stacked POS model based on Chen et al. (2016), they discuss how common constructions should be analyzed in the UD framework, and they provide an annotated treebank of 1,200 sentences. 100 of them were annotated by two people and their inter-annotator agreement was 85.3 UAS and 75.7 LAS.\\n\\n- Strengths:\\n\\n - They obtain good results and their experimental setup appears to be solid.\\n\\n - They perform many careful analyses and explore the influence on many parameters of their model.\\n\\n - They provide a small Singlish treebank annotated according to the Universal Dependencies v1.4 guidelines.\\n\\n - They propose very sound guidelines on how to analyze common Singlish constructions in UD.\\n\\n - Their method is linguistically informed and they nicely exploit similarity between standard English and the creole Singaporean English.\\n\\n - The paper presents methods for a low-resource language.\\n\\n - They are not just applying an existing English method to another language but instead present a method that can be potentially used for other closely related language pairs.\\n\\n - They use a well-motivated method for selecting the sentences to include in their treebank.\\n\\n - The paper is very well written and easy to read.\\n\\n- Weaknesses:\\n\\n - The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement.\\n\\nUPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.\\n\\n- General Discussion:\\n\\nI am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.\\n\\n- Questions for the authors:\\n\\n - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.\\n\\n - Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?\\n\\n - Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use \\\"discourse\\\" for things that are not considered \\\"discourse\\\" in other languages in UD?\\n\\n - Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.\\n\\n- Low-level comments:\\n\\n - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.\\n\\n - You use the word \\\"grammar\\\" in a slightly strange way. I think replacing \\\"grammar\\\" with syntactic constructions would make it clearer what you try to convey. (e.g., line 90)\\n\\n - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.\\n\\n - Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with \\\"very high performing model\\\" or something like that.\\n\\n - It would be helpful if you provided glosses in Figure 2.\\n- Strengths:\\nNice results, nice data set. Not so much work on Creole-like languages, especially English.  \\n\\n- Weaknesses:\\nA global feeling of \\\"Deja-vu\\\", a lot of similar techniques have been applied to other domains, other ressource-low languages. Replace word embeddings by clusters and neural models by whatever was in fashion 5 years ago and we can find more or less the same applied to Urdu or out-of-domain parsing. I liked this paper though, but I would have appreciated the authors to highlight more their contributions and position their work better within the literature.\\n\\n- General Discussion:\\n\\nThis paper presents a set of experiments designed a) to show the effectiveness of a neural parser in a scarce resource scenario and b) to introduce a new data set of Creole English (from Singapour, called Singlish). While this data set is relatively small (1200 annotated sentences, used with 80k unlabeled sentences for word embeddings induction), the authors manage to present respectable results via interesting approach even though using features from relatively close languages are not unknown from the parsing community (see all the line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based parsers, and so on).\\nAssuming we can see Singlish as an extreme of Out-of-domain English and given all the set of experiments, I wonder why the authors didn\u2019t try the classical technique on domain-adaptation, namely training with UD_EN+90% of the Singlish within a 10 cross fold experiment ? just so we can have another interesting baseline (with and without word embeddings, with bi-lingual embeddings if enough parallel data is available).\\nI think that paper is interesting but I really would have appreciated more positioning regarding all previous work in parsing low-ressources languages and extreme domain adaptation. A table presenting some results for Irish and other very small treebanks would be nice.\\nAlso how come the IAA is so low regarding the labeled relations?\\n*****************************************\\nNote after reading the authors' answer\\n*****************************************\\n\\nThanks for your clarifications (especi\"\n}",
        "human_end_index": 6731
    },
    {
        "id": 440,
        "source": "PeerRead/acl_2017/",
        "title": "A* CCG Parsing with a Supertag and Dependency Factored Model",
        "human_review": "- Strengths:\nThis paper presents an extension to A* CCG parsing to include dependency\ninformation.  Achieving this while maintaining speed and tractability is a very\nimpressive feature of this approach.  The ability to precompute attachments is\na nice trick.                  I also really appreciated the evaluation of the\neffect of\nthe\nhead-rules on normal-form violations and would love to see more details on the\nremaining cases.\n\n- Weaknesses:\nI'd like to see more analysis of certain dependency structures.  I'm\nparticularly interested in how coordination and relative clauses are handled\nwhen the predicate argument structure of CCG is at odds with the dependency\nstructures normally used by other dependency parsers.\n\n- General Discussion:\nI'm very happy with this work and feel it's a very nice contribution to the\nliterature.  The only thing missing for me is a more in-depth analysis of the\ntypes of constructions which saw the most improvement (English and Japanese)\nand a discussion (mentioned above) reconciling Pred-Arg dependencies with those\nof other parsers.\nThis paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient A* decoding algorithm.\nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more\nexpressive global parsing model, presumably because this factorization makes\nlearning easier. It's great that they also report results on another language,\nshowing large improvements over existing work on Japanese CCG parsing. One\nsurprising original result is that modeling the first word of a constituent as\nthe head substantially outperforms linguistically motivated head rules. \n\nOverall this is a good paper that makes a nice contribution. I only have a few\nsuggestions:\n- I liked the way that the dependency and supertagging models interact, but it\nwould be good to include baseline results for simpler variations (e.g. not\nconditioning the tag on the head dependency).\n- The paper achieves new state-of-the-art results on Japanese by a large\nmargin. However, there has been a lot less work on this data - would it also be\npossible to train the Lee et al. parser on this data for comparison?\n- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging\nmodels for CCG and SRL, and may be worth citing.",
        "partial_review": "- Strengths:\nThis paper presents an extension to A* CCG parsing to include dependency\ninformation.  Achieving this while maintaining speed and tractability is a very\nimpressive feature of this approach.  The ability to precompute attachments is\na nice trick.                  I also really appreciated the evaluation of the\neffect of\nthe\nhead-rules on normal-form violations and would love to see more details on the\nremaining cases.\n\n- Weaknesses:\nI'd like to see more analysis of certain dependency structures.  I'm\nparticularly interested in how coordination and relative clauses are handled\nwhen the predicate argument structure of CCG is at odds with the dependency\nstructures normally used by other dependency parsers.\n\n- General Discussion:\nI'm very happy with this work and feel it's a very nice contribution to the\nliterature.  The only thing missing for me is a more in-depth analysis of the\ntypes of constructions which saw the most improvement (English and Japanese)\nand a discussion (mentioned above) reconciling Pred-Arg dependencies with those\nof other parsers.\nThis paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an e",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A* CCG Parsing with a Supertag and Dependency Factored Model \n\nHere is the abstract of the paper: We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing. \n\nHere is the partial review: - Strengths:\nThis paper presents an extension to A* CCG parsing to include dependency\ninformation.  Achieving this while maintaining speed and tractability is a very\nimpressive feature of this approach.  The ability to precompute attachments is\na nice trick.                  I also really appreciated the evaluation of the\neffect of\nthe\nhead-rules on normal-form violations and would love to see more details on the\nremaining cases.\n\n- Weaknesses:\nI'd like to see more analysis of certain dependency structures.  I'm\nparticularly interested in how coordination and relative clauses are handled\nwhen the predicate argument structure of CCG is at odds with the dependency\nstructures normally used by other dependency parsers.\n\n- General Discussion:\nI'm very happy with this work and feel it's a very nice contribution to the\nliterature.  The only thing missing for me is a more in-depth analysis of the\ntypes of constructions which saw the most improvement (English and Japanese)\nand a discussion (mentioned above) reconciling Pred-Arg dependencies with those\nof other parsers.\nThis paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an e \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents an extension to A* CCG parsing to include dependency information, which is a very impressive feature of this approach. The ability to precompute attachments is also a nice trick for achieving speed and tractability. However, more analysis is needed on certain dependency structures, especially coordination and relative clauses, to understand how they are handled in comparison to other dependency parsers. Additionally, a more in-depth analysis of the types of constructions that saw the most improvement in English and Japanese parsing results would be valuable. Overall, this work is a very nice contribution to the literature, but a discussion reconciling Pred-Arg dependencies with those of other parsers is missing.\"\n}",
        "human_end_index": 1199
    },
    {
        "id": 444,
        "source": "PeerRead/acl_2017/",
        "title": "Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting",
        "human_review": "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics (fluency, co-herence and\nmatch) and an automatic metric for \u2018Similarity'. Whilst the metric for\nSimilarity is unique and interesting the paper does not give any evidence of\nthis as an effective automatic metric as correlations between this metric and\nthe others are low, (which they say that they should be used separately). The\nauthors claim it can be used to meaningfully analyse system performance but we\nhave to take their word for it as again there is no correlation with any\nhand-annotated performance metric.  Getting worse scores than a baseline system\nisn\u2019t evidence that the metric captures quality (e.g. you could have a very\nstrong baseline).\n\nSome missing references, e.g. recent work looking at automating co-herence,\ne.g. using mutual information density (e.g. Li et al. 2015). In addition, some\nreference to style matching from the NLG community are missing (e.g. Dethlefs\net al. 2014 and the style matching work by Pennebaker).\nThis paper studies how to properly evaluate systems that produce ghostwriting\nof rap lyrics.\nThe authors present manual evaluation along three key aspects: fluency,\ncoherence, and style matching.\nThey also introduce automatic metrics that consider uniqueness via maximum\ntraining similarity, and stylistic similarity via rhyme density.\n\nI can find some interesting analysis and discussion in the paper.\nThe way for manually evaluating style matching especially makes sense to me.\n\nThere also exist a few important concerns for me.\n\nI am not convinced about the appropriateness of only doing fluency/coherence\nratings at line level.\nThe authors mention that they are following Wu (2014), but I find that work\nactually studying a different setting of hip hop lyrical challenges and\nresponses, which should be treated at line level in nature.\nWhile in this work, a full verse consists of multiple lines that normally\nshould be topically and structurally coherent.\nCurrently I cannot see any reason why not to evaluate fluency/coherence for a\nverse as a whole.\n\nAlso, I do not reckon that one should count so much on automatic metrics, if\nthe main goal is to ``generate similar yet unique lyrics''.\nFor uniqueness evaluation, the calculations are performed on verse level.\nHowever, many rappers may only produce lyrics within only a few specific topics\nor themes.\nIf a system can only extract lines from different verses, presumably we might\nalso get a fluent, coherent verse with low verse level similarity score, but we\ncan hardly claim that the system ``generalizes'' well.\nFor stylistic similarity with the specified artist, I do not think rhyme\ndensity can say it all, as it is position independent and therefore may not be\nenough to reflect the full information of style of an artist.\n\nIt does not seem that the automatic metrics have been verified to be well\ncorrelated with corresponding real manual ratings on uniqueness or stylistic\nmatching.\nI also wonder if one needs to evaluate semantic information commonly expressed\nby a specified rapper as well, other than only caring about rhythm.\n\nMeanwhile, I understand the motivation for this study is the lack of *sound*\nevaluation methodology.\nHowever, I still find one statement particularly weird:\n``our methodology produces a continuous numeric score for the whole verse,\nenabling better comparison.''\nIs enabling comparisons really more important than making slightly vague but\nmore reliable, more convincing judgements?\n\nMinor issue:\nIncorrect quotation marks in Line 389\nThis paper proposes to present a more comprehensive evaluation methodology for\nthe assessment of automatically generated rap lyrics (as being similar to a\ntarget artist).  While the assessment of the generation of creative work is\nvery challenging and of great interest to the community, this effort falls\nshort of its claims of a comprehensive solution to this problem.\n\nAll assessment of this nature ultimately falls to a subjective measure -- can\nthe generated sample convince an expert that the generated sample was produced\nby the true artist rather than an automated preocess?  This is essentially a\nmore specific version of a Turing Test.   The effort to automate some parts of\nthe evaluation to aid in optimization and to understand how humans assess\nartistic similarity is valuable.  However, the specific findings reported in\nthis work do not encourage a belief that these have been reliably identified.\n\nSpecifically -- Consider the central question: Was a sample generated by a\ntarget artist?        The human annotators who were asked this were not able to\nconsistently respond to this question.        This means either 1) the annotators did\nnot have sufficient expertise to perform the task, or 2) the task was too\nchallenging, or some combination of the two.  \n\nThe proposed automatic measures also failed to show a reliable agreement to\nhuman raters performing the same task.        This dramatically limits their efficacy\nin providing a proxy for human assessment.   The low interannotator agreement\nmay be \"expected\" because the task is subjective, but the idea of decomposing\nthe evaluation into fluency and coherence components is meant to make it more\ntractable, and thereby improve the consistency of rater scores.  A low IAA for\nan evaluation metric is a cause for concern and limits its viability as a\ngeneral purpose tool.  \n\nSpecific questions/comments:\n\n* Why is a line-by-line level evaluation prefered to a verse level analysis. \nSpecifically for \"coherence\", a line by line analysis limits the scope of\ncoherence to consequtive lines.\n\n* Style matching -- This term assumes that these 13 artists each have a\ndistinct style, and always operate in that style. I would argue that some of\nthese artists (kanye west, eminem, jay z, drake, tupac and notorious big) have\nproduced work in multiple styles.  A more accurate term for this might be\n\"artist matching\".\n\n* In Section 4.2 The central automated component of the evaluation is low\ntf*idf with existing verses, and similar rhyme density.  Given the limitations\nof rhyme density -- how well does this work.  Even with the manual intervention\ndescribed?\n\n* In Section 6.2 -- This description should include how many judges were used\nin this study? In how many cases did the judges already know the verse they\nwere judging?  In this case the test will not assess how easy it is to match\nstyle, but rather, the judges recall and rap knowledge.",
        "partial_review": "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics (fluency, co-herence and\nmatch) and an automatic metric for \u2018Similarity'. Whilst the metric for\nSimilarity is unique and interesting the paper does not give any evidence of\nthis as an effective automatic metric as correlations between this metric and\nthe others are low, (which they say that they should be used separately). The\nauthors claim it can be used to meaningfully analyse system performance but we\nhave to take their word for it as again there is no correlation with any\nhand-annotated performance metric.  Getting worse scores than a baseline system\nisn\u2019t evidence that the metric captures quality (e.g. you could have a very\nstrong baseline).\n\nSome missing references, e.g. recent work looking at automating co-herence,\ne.g. using mutual information density (e.g. Li et al. 2015). In addition, some\nreference to style matching from the NLG community are missing (e.g. Dethlefs\net al. 2014 and the style matching work by Pennebaker).\nThis paper studies how to properly evaluate systems that produce ghostwriting\nof rap lyrics.\nThe authors present manual evaluation along three key aspects: fluency,\ncoherence, and style matching.\nThey also introduce automatic metrics that consider uniqueness via maximum\ntraining similarity, and stylistic similarity via rhyme density.\n\nI can find some interesting analysis and discussion in the paper.\nThe way for manually evaluating style matching especially makes sense to me.\n\nThere also exist a few important concerns for me.\n\nI am not convinced about the appropriateness of only doing fluency/coherence\nratings at line level.\nThe authors mention that they are following Wu (2014), but I find that work\nactually studying a different setting of hip hop lyrical challenges and\nresponses, which should be treated at line level in nature.\nWhile in this work, a full verse consists of multiple lines that normally\nshould be topically and structurally coherent.\nCurrently I cannot see any reason why not to evaluate fluency/coherence for a\nverse as a whole.\n\nAlso, I do not reckon that one should count so much on automatic metrics, if\nthe main goal is to ``generate similar yet unique lyrics''.\nFor uniqueness evaluation, the calculations are performed on verse level.\nHowever, many rappers may only produce lyrics within only a few specific topics\nor themes.\nIf a system can only extract lines from different verses, presumably we might\nalso get a fluent, coherent verse with low verse level similarity score, but we\ncan hardly claim that the system ``generalizes'' well.\nFor stylistic similarity with the specified artist, I do not think rhyme\ndensity can say it all, as it is position independent and therefore may not be\nenough to reflect the full information of style of an artist.\n\nIt does not seem that the automatic metrics have been verified to be well\ncorrelated with corresponding real manual ratings on uniqueness or stylistic\nmatching.\nI also wonder if one needs to evaluate semantic information commonly expressed\nby a specified rapper as w",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting \n\nHere is the abstract of the paper: Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider creativity, style, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluation methods for one such task, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions of this task. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel evaluation methodology that addresses several complementary aspects of this task, and illustrate how such evaluation can be used to meaningfully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse. \n\nHere is the partial review: This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics (fluency, co-herence and\nmatch) and an automatic metric for \u2018Similarity'. Whilst the metric for\nSimilarity is unique and interesting the paper does not give any evidence of\nthis as an effective automatic metric as correlations between this metric and\nthe others are low, (which they say that they should be used separately). The\nauthors claim it can be used to meaningfully analyse system performance but we\nhave to take their word for it as again there is no correlation with any\nhand-annotated performance metric.  Getting worse scores than a baseline system\nisn\u2019t evidence that the metric captures quality (e.g. you could have a very\nstrong baseline).\n\nSome missing references, e.g. recent work looking at automating co-herence,\ne.g. using mutual information density (e.g. Li et al. 2015). In addition, some\nreference to style matching from the NLG community are missing (e.g. Dethlefs\net al. 2014 and the style matching work by Pennebaker).\nThis paper studies how to properly evaluate systems that produce ghostwriting\nof rap lyrics.\nThe authors present manual evaluation along three key aspects: fluency,\ncoherence, and style matching.\nThey also introduce automatic metrics that consider uniqueness via maximum\ntraining similarity, and stylistic similarity via rhyme density.\n\nI can find some interesting analysis and discussion in the paper.\nThe way for manually evaluating style matching especially makes sense to me.\n\nThere also exist a few important concerns for me.\n\nI am not convinced about the appropriateness of only doing fluency/coherence\nratings at line level.\nThe authors mention that they are following Wu (2014), but I find that work\nactually studying a different setting of hip hop lyrical challenges and\nresponses, which should be treated at line level in nature.\nWhile in this work, a full verse consists of multiple lines that normally\nshould be topically and structurally coherent.\nCurrently I cannot see any reason why not to evaluate fluency/coherence for a\nverse as a whole.\n\nAlso, I do not reckon that one should count so much on automatic metrics, if\nthe main goal is to ``generate similar yet unique lyrics''.\nFor uniqueness evaluation, the calculations are performed on verse level.\nHowever, many rappers may only produce lyrics within only a few specific topics\nor themes.\nIf a system can only extract lines from different verses, presumably we might\nalso get a fluent, coherent verse with low verse level similarity score, but we\ncan hardly claim that the system ``generalizes'' well.\nFor stylistic similarity with the specified artist, I do not think rhyme\ndensity can say it all, as it is position independent and therefore may not be\nenough to reflect the full information of style of an artist.\n\nIt does not seem that the automatic metrics have been verified to be well\ncorrelated with corresponding real manual ratings on uniqueness or stylistic\nmatching.\nI also wonder if one needs to evaluate semantic information commonly expressed\nby a specified rapper as w \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents evaluation metrics for lyrics generation exploring the need for the lyrics to be original but in a similar style to an artist whilst being fluent and coherent. The motivation for the metrics is well explained and the paper is well written. However, the authors should provide evidence of the effectiveness of the automatic metric for 'Similarity' as correlations between this metric and the hand-annotated metrics are low. Additionally, some important references are missing, such as recent work on automating coherence and style matching in the NLG community. I find the manual evaluation of style matching to be sensible, but I have concerns about only evaluating fluency and coherence at the line level. The evaluation should also consider verse-level fluency and coherence. Furthermore, relying heavily on automatic metrics may not be appropriate if the goal is to generate similar yet unique lyrics. The automatic metrics should be verified to be well correlated with manual ratings on uniqueness and stylistic matching. Overall, the paper provides interesting analysis and discussion, but there are some important concerns that need to be addressed.\"\n}",
        "human_end_index": 3313
    },
    {
        "id": "447",
        "source": "PeerRead/acl_2017/",
        "title": "Neural Discourse Structure for Text Categorization",
        "human_review": "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. A RNN with attention\nmechanism is employed to compute a representation of text. The experiments on\nvarious of dataset shows the effectiveness of the proposed method. Below are my\ncomments:\n\n(1) From Table 2, it shows that \u201cUNLABELED\u201d model performs better on four\nout of five datasets than the \u201cFULL\u201d model. The authors should explain more\nabout this, because intuitively, incorporating additional relation labels\nshould bring some benefits. Is the performance of relation labelling so bad and\nit hurts the performance instead?\n\n(2) The paper also transforms the RST tree into a dependency structure as a\npre-process step. Instead of transforming, how about keep the original tree\nstructure and train a hierarchical model on that?\n\n(3) For the experimental datasets, instead of comparing with only one dataset\nwith each of the previous work, the authors may want to run experiments on more\ncommon datasets used by previous work.\n- Strengths:\n\nThe main strength of this paper is the incorporation of discourse structure in\nthe DNN's attention model, which allows the model to learn the weights given to\ndifferent EDUs.\n\nAlso the paper is very clear, and provides a good explanation of both RST and\nhow it is used in the model.\nFinally, the evaluation experiments are conducted thoroughly with strong,\nstate-of-the-art baselines.\n\n- Weaknesses:\n\nThe main weakness of the paper is that the results do not strongly support the\nmain claim that discourse structure can help text classification. Even the\nUNLABELED variant, which performs best and does outperform the state of the\nart, only provides minimal gains (and hurts in the legal/bills domain). The\napproach (particularly the FULL variant) seems to be too data greedy but no\nreal solution is provided to address this beyond the simpler UNLABELED and ROOT\nvariants.\n\n- General Discussion:\n\nIn general, this paper feels like a good first shot at incorporating discourse\nstructure into DNN-based classification, but does not fully convince that\nRST-style structure will significantly boost performance on most tasks (given\nthat it is also very costly to build a RST parser for a new domain, as would be\nneeded in the legal/bill domains described in this paper). I wish the authors\nhad explored or at least mentioned next steps in making this approach work, in\nparticular in the face of data sparsity. For example, how about defining\n(task-independent) discourse embeddings? Would it be possible to use a DNN for\ndiscourse parsing that could be incorporated in the main task DNN and optimized\njointly  end-to-end? Again, this is good work, I just wish the authors had\npushed it a little further given the mixed results.",
        "partial_review": "This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. A RNN with attention\nmechanism is employed to compute a representation of text. The experiments on\nvarious of dataset shows the effectiveness of the proposed method. Below are my\ncomments:\n\n(1) From Table 2, it shows that \u201cUNLABELED\u201d model performs better on four\nout of five datasets than the \u201cFULL\u201d model. The authors should explain more\nabout this, because intuitively, incorporating additional relation labels\nshould bring some benefits. Is the performance of relation labelling so bad and\nit hurts the performance instead?\n\n(2) The paper also transforms the RST tree into a dependency structure as a\npre-process step. Instead of transforming, how about keep the original tree\nstructure and train a hierarchical model on that?\n\n(3) For the experimental datasets, instead of comparing with only one dataset\nwith each of the previous work, the authors may want to run experiments on more\ncommon datasets used by previous work.\n- Strengths:\n\nThe main strength of this paper is the incorporation of discourse structure in\nthe DNN's attention model, which allows the model to learn the weights given to\ndifferent EDUs.\n\nAlso the paper is very clear, and provides a good explanation of both RST and\nhow it is used in the model.\nFinally, the evaluation experiments are conducted thoroughly with strong,\nstate-of-the-",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Neural Discourse Structure for Text Categorization \n\nHere is the abstract of the paper: We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization.  Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task.  Experiments consider variants of the approach and illustrate its strengths and weaknesses. \n\nHere is the partial review: This paper proposed to explore discourse structure, as defined by Rhetorical\nStructure Theory (RST) to improve text categorization. A RNN with attention\nmechanism is employed to compute a representation of text. The experiments on\nvarious of dataset shows the effectiveness of the proposed method. Below are my\ncomments:\n\n(1) From Table 2, it shows that \u201cUNLABELED\u201d model performs better on four\nout of five datasets than the \u201cFULL\u201d model. The authors should explain more\nabout this, because intuitively, incorporating additional relation labels\nshould bring some benefits. Is the performance of relation labelling so bad and\nit hurts the performance instead?\n\n(2) The paper also transforms the RST tree into a dependency structure as a\npre-process step. Instead of transforming, how about keep the original tree\nstructure and train a hierarchical model on that?\n\n(3) For the experimental datasets, instead of comparing with only one dataset\nwith each of the previous work, the authors may want to run experiments on more\ncommon datasets used by previous work.\n- Strengths:\n\nThe main strength of this paper is the incorporation of discourse structure in\nthe DNN's attention model, which allows the model to learn the weights given to\ndifferent EDUs.\n\nAlso the paper is very clear, and provides a good explanation of both RST and\nhow it is used in the model.\nFinally, the evaluation experiments are conducted thoroughly with strong,\nstate-of-the- \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper proposed to explore discourse structure, as defined by Rhetorical Structure Theory (RST) to improve text categorization. A RNN with attention mechanism is employed to compute a representation of text. The experiments on various of dataset shows the effectiveness of the proposed method. Below are my comments:\\n\\n(1) From Table 2, it shows that \\u201cUNLABELED\\u201d model performs better on four out of five datasets than the \\u201cFULL\\u201d model. The authors should explain more about this, because intuitively, incorporating additional relation labels should bring some benefits. Is the performance of relation labelling so bad and it hurts the performance instead?\\n\\n(2) The paper also transforms the RST tree into a dependency structure as a pre-process step. Instead of transforming, how about keep the original tree structure and train a hierarchical model on that?\\n\\n(3) For the experimental datasets, instead of comparing with only one dataset with each of the previous work, the authors may want to run experiments on more common datasets used by previous work.\\n- Strengths:\\n\\nThe main strength of this paper is the incorporation of discourse structure in the DNN's attention model, which allows the model to learn the weights given to different EDUs.\\n\\nAlso the paper is very clear, and provides a good explanation of both RST and how it is used in the model.\\nFinally, the evaluation experiments are conducted thoroughly with strong, state-of-the-art techniques and the results are well analyzed and discussed.\"\n}",
        "human_end_index": 1445
    },
    {
        "id": "462",
        "source": "PeerRead/acl_2017/",
        "title": "Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models",
        "human_review": "- Strengths:\nThe approach described in the manuscript outperformed the previous approaches\nand achieved the state-of-the-art result.\n\nRegarding data, the method used the combination of market and text data.\n\nThe approach used word embeddings to define the weight of each lexicon term by\nextending it to the similar terms in the document.\n\n- Weaknesses:\nDeep-learning based methods were known to be able to achieve relatively good\nperformances without much feature engineering in sentimental analysis. More\nliterature search is needed to compare with the related works would be better.\n\nThe approach generally improved performance by feature-based methods without\nmuch novelty in model or proposal of new features.\n\n- General Discussion:\nThe manuscript described an approach in sentimental analysis. The method used a\nrelatively new method of using word embeddings to define the weight of each\nlexicon term. However, the novelty is not significant enough.\n- Strengths:\n\n- Weaknesses:\n\n- General Discussion:\n\nThis paper investigates sentiment signals in  companies\u2019 annual 10-K filing\nreports to forecast volatility. \n\nThe authors evaluate information retrieval term weighting models which are\nseeded with a finance-oriented sentiment lexicon and expanded with word\nembeddings. PCA is used to reduce dimensionality before Support Vector\nRegression is applied for similarity estimation.\n\nIn addition to text-based features, the authors also use non-text-based market\nfeatures (e.g. sector information and volatility estimates).\n\nMultiple fusion methods to combine text features with market features are\nevaluated.\n\nCOMMENTS\n\nIt would be interesting to include two more experimental conditions, namely 1)\na simple trigram SVM which does not use any prior sentiment lexica, and 2)\nfeatures that reflect delta-IDFs scores for individual features.\nAs an additional baseline, it would be good to see binary features.\n\nThis paper could corroborate your references:\n\nhttps://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf",
        "partial_review": "- Strengths:\nThe approach described in the manuscript outperformed the previous approaches\nand achieved the state-of-the-art result.\n\nRegarding data, the method used the combination of market and text data.\n\nThe approach used word embeddings to define the weight of each lexicon term by\nextending it to the similar terms in the document.\n\n- Weaknesses:\nDeep-learning based methods were known to be able to achieve relatively good\nperformances without much feature engineering in sentimental analysis. More\nliterature search is needed to compare with the related works would be better.\n\nThe approach generally improved performance by feature-based methods without\nmuch novelty in model or proposal of new features.\n\n- General Discussion:\nThe manuscript described an approach in sentimental analysis. The method used a\nrelatively new method of using word embeddings to define the weight of each\nlexicon term. However, the novelty is not signif",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models \n\nHere is the abstract of the paper: Volatility prediction\u2014an essential concept in financial markets\u2014has recently been addressed using sentiment analysis methods. We investigate the sentiment of annual disclosures of companies in stock markets to forecast volatility. We specifically explore the use of recent Information Retrieval (IR) term weighting models that are effectively extended by related terms using word embeddings. In parallel to textual information, factual market data have been widely used as the mainstream approach to forecast market risk. We therefore study different fusion methods to combine text and market data resources. Our word embedding-based approach significantly outperforms state-of-the-art methods. In addition, we investigate the characteristics of the reports of the companies in different financial sectors. \n\nHere is the partial review: - Strengths:\nThe approach described in the manuscript outperformed the previous approaches\nand achieved the state-of-the-art result.\n\nRegarding data, the method used the combination of market and text data.\n\nThe approach used word embeddings to define the weight of each lexicon term by\nextending it to the similar terms in the document.\n\n- Weaknesses:\nDeep-learning based methods were known to be able to achieve relatively good\nperformances without much feature engineering in sentimental analysis. More\nliterature search is needed to compare with the related works would be better.\n\nThe approach generally improved performance by feature-based methods without\nmuch novelty in model or proposal of new features.\n\n- General Discussion:\nThe manuscript described an approach in sentimental analysis. The method used a\nrelatively new method of using word embeddings to define the weight of each\nlexicon term. However, the novelty is not signif \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The approach described in the manuscript outperformed the previous approaches and achieved the state-of-the-art result. The use of word embeddings to define the weight of each lexicon term is a relatively new and effective method. However, more literature search is needed to compare the approach with related works in sentiment analysis. While the approach improved performance compared to feature-based methods, there was not much novelty in the model or proposal of new features. Overall, the manuscript provides valuable insights into using sentiment analysis for volatility prediction using financial disclosures and explores the fusion of text and market data resources.\",\n}",
        "human_end_index": 941
    },
    {
        "id": "467",
        "source": "PeerRead/acl_2017/",
        "title": "Learning bilingual word embeddings with (almost) no bilingual data",
        "human_review": "- Strengths:\n\nThe paper presents an iterative method to induce bilingual word embeddings\nusing large monolingual corpora starting with very few (or automatically\nobtainable numeral) mappings between two languages. Compared to\nstate-of-the-art using larger bilingual dictionaries or parallel/comparable\ncorpora, the results obtained with the presented method that relies on very\nlittle or no manually prepared input are exciting and impressive.\n\n- Weaknesses:\n\nI would have liked to see a discussion on the errors of the method, and\npossibly a discussion on how the method could be adjusted to deal with them.\n\n- General Discussion:\n\nDoes the frequency of the seeds in the monolingual corpora matter?\n\nIt would be interesting to see the partial (in the sense of after n number of\niterations) evolution of the mapping between words in the two languages for a\nfew words. \n\nWhat happens with different translations of the same word (like different\nsenses)?\n\nOne big difference between German and English is the prevalence of compounds in\nGerman. What happens to these compounds? What are they mapped onto? Would a\npreprocessing step of splitting the compounds help? (using maybe only\ncorpus-internal unigram information)\n\nWhat would be the upper bound for such an approach? An analysis of errors --\ne.g. words very far from their counterpart in the other language -- would be\nvery interesting. It would also be interesting to see a discussion of where\nthese errors come from, and if they could be addressed with the presented\napproach.\nThis work proposes a self-learning bootstrapping approach to learning bilingual\nword embeddings, which achieves competitive results in tasks of bilingual\nlexicon induction and cross-lingual word similarity although it requires a\nminimal amount of bilingual supervision: the method leads to competitive\nperformance even when the seed dictionary is extremely small (25 dictionary\nitems!) or is constructed without any language pair specific information (e.g.,\nrelying on numerals shared between languages). \n\nThe paper is very well-written, admirably even so. I find this work 'eclectic'\nin a sense that its original contribution is not a breakthrough finding (it is\nmore a 'short paper idea' in my opinion), but it connects the dots from prior\nwork drawing inspiration and modelling components from a variety of previous\npapers on the subject, including the pre-embedding work on\nself-learning/bootstrapping (which is not fully recognized in the current\nversion of the paper). I liked the paper in general, but there are few other\nresearch questions that could/should have been pursued in this work. These,\nalong with only a partial recognition of related work and a lack of comparisons\nwith several other relevant baselines, are my main concern regarding this\npaper, and they should be fixed in the updated version(s).\n\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one\nof the first to tackle this very limited setup for learning cross-lingual\nembeddings (although not the first one, see Miceli Barone and more works\nbelow), this is the first truly bootstrapping/self-learning approach to\nlearning cross-lingual embeddings. However, this idea of bootstrapping\nbilingual vector spaces is not new at all (it is just reapplied to learning\nembeddings), and there is a body of work which used exactly the same idea with\ntraditional 'count-based' bilingual vector spaces. I suggest the authors to\ncheck the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP\n2013), and recognize the fact that their proposed bootstrapping approach is not\nso novel in this domain. There is also related work of Ellen Riloff's group on\nbootstrapping semantic lexicons in monolingual settings.\n\n*Relation to Artetxe et al.: I might be missing something here, but it seems\nthat the proposed bootstrapping algorithm is in fact only an iterative approach\nwhich repeatedly utilises the previously proposed model/formulation of Artetxe\net al. The only difference is the reparametrization (line 296-305). It is not\nclear to me whether the bootstrapping approach draws its performance from this\nreparametrization (and whether it would work with the previous\nparametrization), or the performance is a product of both the algorithm and\nthis new parametrization. Perhaps a more explicit statement in the text is\nneeded to fully understand what is going on here.\n\n*Comparison with prior work: Several very relevant papers have not been\nmentioned nor discussed in the current version of the paper. For instance, the\nrecent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word\nembeddings without bilingual corpora' seems very related to this work (as the\nbasic word overlap between the two titles reveals!), and should be at least\ndiscussed if not compared to. Another work which also relies on mappings with\nseed lexicons and also partially analyzes the setting with only a few hundred\nseed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of\nseed lexicons in learning bilingual word embeddings': these two papers might\nalso help the authors to provide more details for the future work section\n(e.g., the selection of reliable translation pairs might boost the performance\nfurther during the iterative process). Another very relevant work has appeared\nonly recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word\nvectors, orthogonal transformations and the inverted softmax'. This paper also\ndiscusses learning bilingual embeddings in very limited settings (e.g., by\nrelying only on shared words and cognates between two languages in a pair). As\na side note, it would be interesting to report results obtained using only\nshared words between the languages (such words definitely exist for all three\nlanguage pairs used in the experiments). This would also enable a direct\ncomparison with the work of Smith et al. (ICLR 2017) which rely on this setup.\n\n*Seed dictionary size and bilingual lexicon induction: It seems that the\nproposed algorithm (as discussed in Section 5) is almost invariant to the\nstarting seed lexicon, yielding very similar final BLI scores regardless of the\nstarting point. While a very intriguing finding per se, this also seems to\nsuggest an utter limitation of the current 'offline' approaches: they seem to\nhave hit the ceiling with the setup discussed in the paper; Vulic and Korhonen\n(ACL 2016) showed that we cannot really improve the results by simply\ncollecting more seed lexicon pairs, and this work suggests that any number of\nstarting pairs (from 25 to 5k) is good enough to reach this near-optimal\nperformance, which is also very similar to the numbers reported by Dinu et al.\n(arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more\ndiscussion on how to break this ceiling and further improve BLI results with\nsuch 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers\non the same dataset, so again it would be very interesting to link this work to\nthe work of Smith et al.\n\nIn other words, the authors state that in future work they plan to fine-tune\nthe method so that it can learn without any bilingual evidence. This is an\nadmirable 'philosophically-driven' feat, but from a more pragmatic point of\nview, it seems more pragmatic to detect how we can go over the plateau/ceiling\nwhich seems to be hit with these linear mapping approaches regardless of the\nnumber of used seed lexicon pairs (Figure 2).\n\n*Convergence criterion/training efficiency: The convergence criterion, although\ncrucial for the entire algorithm, both in terms of efficiency and efficacy, is\nmentioned only as a side note, and it is not entirely clear how the whole\nprocedure terminates. I suspect that the authors use the vanishing variation in\ncrosslingual word similarity performance as the criterion to stop the\nprocedure, but that makes the method applicable only to languages which have a\ncross-lingual word similarity dataset. I might be missing here given the\ncurrent description in the paper, but I do not fully understand how the\nprocedure stops for Finnish, given that there is no crosslingual word\nsimilarity dataset for English-Finnish.\n\n*Minor:\n- There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416):\nhttps://www.clarin.si/repository/xmlui/handle/11356/1074\n- Since the authors claim that the method could work with a seed dictionary\ncontaining only shared numerals, it would be very interesting to include an\nadditional language pair which does not share the alphabet (e.g.,\nEnglish-Russian, English-Bulgarian or even something more distant such as\nArabic and/or Hindi).\n\n*After the response: I would like to thank the authors for investing their time\ninto their response which helped me clarify some doubts and points raised in my\ninitial review. I hope that they would indeed clarify these points in the final\nversion, if given the opportunity.\nThe paper presents a self-learning framework for learning of bilingual word\nembeddings. The method uses two embeddings (in source and target languages) and\na seed lexicon. On each step of the mapping learning a new bilingual lexicon is\ninduced. Then the learning step is repeated using the new lexicon for learning\nof new mapping. The process stops when a convergence criterion is met.\n\nOne of the strengths is that the seed lexicon is directly encoded in the\nlearning process as a binary matrix. Then the self-learning framework solves a\nglobal optimization problem in which the seed lexicon is not explicitly\ninvolved. Its role is to establish the initial mapping between the two\nembeddings. This guarantees the convergence. The initial seed lexicon could be\nquite small (25 correspondences).\n\nThe small size of the seed lexicon is appealing for mappings between languages\nfor which there are not large bilingual lexicons.\n\nIt will be good to evaluate the framework with respect to the quality of the\ntwo word embeddings. If we have languages (or at least one of the languages)\nwith scarce language resources then the word embeddings for both languages\ncould differ in their structure and coverage. I think it could be simulated on\nthe basis of the available data via training the corresponding word embeddings\non different subcorpora for each language.",
        "partial_review": "- Strengths:\n\nThe paper presents an iterative method to induce bilingual word embeddings\nusing large monolingual corpora starting with very few (or automatically\nobtainable numeral) mappings between two languages. Compared to\nstate-of-the-art using larger bilingual dictionaries or parallel/comparable\ncorpora, the results obtained with the presented method that relies on very\nlittle or no manually prepared input are exciting and impressive.\n\n- Weaknesses:\n\nI would have liked to see a discussion on the errors of the method, and\npossibly a discussion on how the method could be adjusted to deal with them.\n\n- General Discussion:\n\nDoes the frequency of the seeds in the monolingual corpora matter?\n\nIt would be interesting to see the partial (in the sense of after n number of\niterations) evolution of the mapping between words in the two languages for a\nfew words. \n\nWhat happens with different translations of the same word (like different\nsenses)?\n\nOne big difference between German and English is the prevalence of compounds in\nGerman. What happens to these compounds? What are they mapped onto? Would a\npreprocessing step of splitting the compounds help? (using maybe only\ncorpus-internal unigram information)\n\nWhat would be the upper bound for such an approach? An analysis of errors --\ne.g. words very far from their counterpart in the other language -- would be\nvery interesting. It would also be interesting to see a discussion of where\nthese errors come from, and if they could be addressed with the presented\napproach.\nThis work proposes a self-learning bootstrapping approach to learning bilingual\nword embeddings, which achieves competitive results in tasks of bilingual\nlexicon induction and cross-lingual word similarity although it requires a\nminimal amount of bilingual supervision: the method leads to competitive\nperformance even when the seed dictionary is extremely small (25 dictionary\nitems!) or is constructed without any language pair specific information (e.g.,\nrelying on numerals shared between languages). \n\nThe paper is very well-written, admirably even so. I find this work 'eclectic'\nin a sense that its original contribution is not a breakthrough finding (it is\nmore a 'short paper idea' in my opinion), but it connects the dots from prior\nwork drawing inspiration and modelling components from a variety of previous\npapers on the subject, including the pre-embedding work on\nself-learning/bootstrapping (which is not fully recognized in the current\nversion of the paper). I liked the paper in general, but there are few other\nresearch questions that could/should have been pursued in this work. These,\nalong with only a partial recognition of related work and a lack of comparisons\nwith several other relevant baselines, are my main concern regarding this\npaper, and they should be fixed in the updated version(s).\n\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one\nof the first to tackle this very limited setup for learning cross-lingual\nembeddings (although not the first one, see Miceli Barone and more works\nbelow), this is the first truly bootstrapping/self-learning approach to\nlearning cross-lingual embeddings. However, this idea of bootstrapping\nbilingual vector spaces is not new at all (it is just reapplied to learning\nembeddings), and there is a body of work which used exactly the same idea with\ntraditional 'count-based' bilingual vector spaces. I suggest the authors to\ncheck the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP\n2013), and recognize the fact that their proposed bootstrapping approach is not\nso novel in this domain. There is also related work of Ellen Riloff's group on\nbootstrapping semantic lexicons in monolingual settings.\n\n*Relation to Artetxe et al.: I might be missing something here, but it seems\nthat the proposed bootstrapping algorithm is in fact only an iterative approach\nwhich repeatedly utilises the previously proposed model/formulation of Artetxe\net al. The only difference is the reparametrization (line 296-305). It is not\nclear to me whether the bootstrapping approach draws its performance from this\nreparametrization (and whether it would work with the previous\nparametrization), or the performance is a product of both the algorithm and\nthis new parametrization. Perhaps a more explicit statement in the text is\nneeded to fully understand what is going on here.\n\n*Comparison with prior work: Several very relevant papers have not been\nmentioned nor discussed in the current version of the paper. For instance, the\nrecent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word\nembeddings without bilingual corpora' seems very related to this work (as the\nbasic word overlap between the two titles reveals!), and should be at least\ndiscussed if not compared to. Another work which also relies on mappings with\nseed lexicons and also partially analyzes the setting with only a few hundred\nseed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of\nseed lexicons in learning bilingual word embeddings': these two papers might\nalso help the authors to provide more details for the future work section\n(e.g., the selection of reliable translation pairs might boost the performa",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Learning bilingual word embeddings with (almost) no bilingual data \n\nHere is the abstract of the paper: Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources. \n\nHere is the partial review: - Strengths:\n\nThe paper presents an iterative method to induce bilingual word embeddings\nusing large monolingual corpora starting with very few (or automatically\nobtainable numeral) mappings between two languages. Compared to\nstate-of-the-art using larger bilingual dictionaries or parallel/comparable\ncorpora, the results obtained with the presented method that relies on very\nlittle or no manually prepared input are exciting and impressive.\n\n- Weaknesses:\n\nI would have liked to see a discussion on the errors of the method, and\npossibly a discussion on how the method could be adjusted to deal with them.\n\n- General Discussion:\n\nDoes the frequency of the seeds in the monolingual corpora matter?\n\nIt would be interesting to see the partial (in the sense of after n number of\niterations) evolution of the mapping between words in the two languages for a\nfew words. \n\nWhat happens with different translations of the same word (like different\nsenses)?\n\nOne big difference between German and English is the prevalence of compounds in\nGerman. What happens to these compounds? What are they mapped onto? Would a\npreprocessing step of splitting the compounds help? (using maybe only\ncorpus-internal unigram information)\n\nWhat would be the upper bound for such an approach? An analysis of errors --\ne.g. words very far from their counterpart in the other language -- would be\nvery interesting. It would also be interesting to see a discussion of where\nthese errors come from, and if they could be addressed with the presented\napproach.\nThis work proposes a self-learning bootstrapping approach to learning bilingual\nword embeddings, which achieves competitive results in tasks of bilingual\nlexicon induction and cross-lingual word similarity although it requires a\nminimal amount of bilingual supervision: the method leads to competitive\nperformance even when the seed dictionary is extremely small (25 dictionary\nitems!) or is constructed without any language pair specific information (e.g.,\nrelying on numerals shared between languages). \n\nThe paper is very well-written, admirably even so. I find this work 'eclectic'\nin a sense that its original contribution is not a breakthrough finding (it is\nmore a 'short paper idea' in my opinion), but it connects the dots from prior\nwork drawing inspiration and modelling components from a variety of previous\npapers on the subject, including the pre-embedding work on\nself-learning/bootstrapping (which is not fully recognized in the current\nversion of the paper). I liked the paper in general, but there are few other\nresearch questions that could/should have been pursued in this work. These,\nalong with only a partial recognition of related work and a lack of comparisons\nwith several other relevant baselines, are my main concern regarding this\npaper, and they should be fixed in the updated version(s).\n\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one\nof the first to tackle this very limited setup for learning cross-lingual\nembeddings (although not the first one, see Miceli Barone and more works\nbelow), this is the first truly bootstrapping/self-learning approach to\nlearning cross-lingual embeddings. However, this idea of bootstrapping\nbilingual vector spaces is not new at all (it is just reapplied to learning\nembeddings), and there is a body of work which used exactly the same idea with\ntraditional 'count-based' bilingual vector spaces. I suggest the authors to\ncheck the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP\n2013), and recognize the fact that their proposed bootstrapping approach is not\nso novel in this domain. There is also related work of Ellen Riloff's group on\nbootstrapping semantic lexicons in monolingual settings.\n\n*Relation to Artetxe et al.: I might be missing something here, but it seems\nthat the proposed bootstrapping algorithm is in fact only an iterative approach\nwhich repeatedly utilises the previously proposed model/formulation of Artetxe\net al. The only difference is the reparametrization (line 296-305). It is not\nclear to me whether the bootstrapping approach draws its performance from this\nreparametrization (and whether it would work with the previous\nparametrization), or the performance is a product of both the algorithm and\nthis new parametrization. Perhaps a more explicit statement in the text is\nneeded to fully understand what is going on here.\n\n*Comparison with prior work: Several very relevant papers have not been\nmentioned nor discussed in the current version of the paper. For instance, the\nrecent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word\nembeddings without bilingual corpora' seems very related to this work (as the\nbasic word overlap between the two titles reveals!), and should be at least\ndiscussed if not compared to. Another work which also relies on mappings with\nseed lexicons and also partially analyzes the setting with only a few hundred\nseed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of\nseed lexicons in learning bilingual word embeddings': these two papers might\nalso help the authors to provide more details for the future work section\n(e.g., the selection of reliable translation pairs might boost the performa \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents an iterative method to induce bilingual word embeddings using large monolingual corpora starting with very few (or automatically obtainable numeral) mappings between two languages. Compared to state-of-the-art using larger bilingual dictionaries or parallel/comparable corpora, the results obtained with the presented method that relies on very little or no manually prepared input are exciting and impressive. I would have liked to see a discussion on the errors of the method, and possibly a discussion on how the method could be adjusted to deal with them. Does the frequency of the seeds in the monolingual corpora matter? It would be interesting to see the partial (in the sense of after n number of iterations) evolution of the mapping between words in the two languages for a few words. What happens with different translations of the same word (like different senses)? One big difference between German and English is the prevalence of compounds in German. What happens to these compounds? What are they mapped onto? Would a preprocessing step of splitting the compounds help? (using maybe only corpus-internal unigram information) What would be the upper bound for such an approach? An analysis of errors -- e.g. words very far from their counterpart in the other language -- would be very interesting. It would also be interesting to see a discussion of where these errors come from, and if they could be addressed with the presented approach. This work proposes a self-learning bootstrapping approach to learning bilingual word embeddings, which achieves competitive results in tasks of bilingual lexicon induction and cross-lingual word similarity although it requires a minimal amount of bilingual supervision: the method leads to competitive performance even when the seed dictionary is extremely small (25 dictionary items!) or is constructed without any language pair specific information (e.g., relying on numerals shared between languages). The paper is very well-written, admirably even so. I find this work 'eclectic' in a sense that its original contribution is not a breakthrough finding (it is more a 'short paper idea' in my opinion), but it connects the dots from prior work drawing inspiration and modelling components from a variety of previous papers on the subject, including the pre-embedding work on self-learning/bootstrapping (which is not fully recognized in the current version of the paper). I liked the paper in general, but there are few other research questions that could/should have been pursued in this work. These, along with only a partial recognition of related work and a lack of comparisons with several other relevant baselines, are my main concern regarding this paper, and they should be fixed in the updated version(s). Self-learning/bootstrapping of bilingual vector spaces: While this work is one of the first to tackle this very limited setup for learning cross-lingual embeddings (although not the first one, see Miceli Barone and more works below), this is the first truly bootstrapping/self-learning approach to learning cross-lingual embeddings. However, this idea of bootstrapping bilingual vector spaces is not new at all (it is just reapplied to learning embeddings), and there is a body of work which used exactly the same idea with traditional 'count-based' bilingual vector spaces. I suggest the authors to check the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP 2013), and recognize the fact that their proposed bootstrapping approach is not so novel in this domain. There is also related work of Ellen Riloff's group on bootstrapping semantic lexicons in monolingual settings. Relation to Artetxe et al.: I might be missing something here, but it seems that the proposed bootstrapping algorithm is in fact only an iterative approach which repeatedly utilises the previously proposed model/formulation of Artetxe et al. The only difference is the reparametrization (line 296-305). It is not clear to me whether the bootstrapping approach draws its performance from this reparametrization (and whether it would work with the previous parametrization), or the performance is a product of both the algorithm and this new parametrization. Perhaps a more explicit statement in the text is needed to fully understand what is going on here. Comparison with prior work: Several very relevant papers have not been mentioned nor discussed in the current version of the paper. For instance, the recent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word embeddings without bilingual corpora' seems very related to this work (as the basic word overlap between the two titles reveals!), and should be at least discussed if not compared to. Another work which also relies on mappings with seed lexicons and also partially analyzes the setting with only a few hundred seed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of seed lexicons in learning bilingual word embeddings': these two papers might also help the authors to provide more details for the future work section (e.g., the selection of reliable translation pairs might boost the performa\"\n}",
        "human_end_index": 5242
    },
    {
        "id": 477,
        "source": "PeerRead/acl_2017/",
        "title": "From Characters to Words to in Between: Do We Capture Morphology?",
        "human_review": "- Strengths:\ni. Motivation is well described.\nii. Provides detailed comparisons with various models across diverse languages\n\n- Weaknesses:\ni.          The conclusion is biased by the selected languages. \nii.           The experiments do not cover the claim of this paper completely.\n\n- General Discussion:\nThis paper issues a simple but fundamental question about word representation:\nwhat subunit of a word is suitable to represent morphologies and how to compose\nthe units. To answer this question, this paper applied word representations\nwith various subunits (characters, character-trigram, and morphs) and\ncomposition functions (LSTM, CNN, and a simple addition) to the language\nmodeling task to find the best combination. In addition, this paper evaluated\nthe task for more than 10 languages. This is because languages are\ntypologically diverse and the results can be different according to the word\nrepresentation and composition function. From their experimental results, this\npaper concluded that character-level representations are more effective, but\nthey are still imperfective in comparing them with a model with explicit\nknowledge of morphology. Another conclusion is that character-trigrams show\nreliable perplexity in the majority of the languages. \n\nHowever, this paper leaves some issues behind.\n-         First of all, there could be some selection bias of the experimental\nlanguages. This paper chose ten languages in four categories (up to three\nlanguages per a category). But, one basic question with the languages is \u201chow\ncan it be claimed that the languages are representatives of each category?\u201d\nAll the languages in the same category have the same tendency of word\nrepresentation and composition function? How can it be proved? For instance,\neven in this paper, two languages belonging to the same typology\n(agglutinative) show different results. Therefore, at least to me, it seems to\nbe better to focus on the languages tested in this paper instead of drawing a\ngeneral conclusions about all languages. \n-         There is some gap between the claim and the experiments. Is the\nlanguage modeling the best task to prove the claim of this paper? Isn\u2019t there\nany chance that the claim of this paper breaks in other tasks? Further\nexplanation on this issue is needed.\n-         In Section 5.2, this paper evaluated the proposed method only for\nArabic. Is there any reason why the experiment is performed only for Arabic?\nThere are plenty of languages with automatic morphological analyzers such as\nJapanese and Turkish.\n-         This paper considers only character-trigram among various n-grams. Is\nthere any good reason to choose only character-trigram? Is it always better\nthan character-bigram or character-fourgram? In general, language modeling with\nn-grams is affected by corpus size and some other factors. \n\nMinor typos: \n- There is a missing reference in Introduction. (88 line in Page 1)\n- root-and-patter -> root-and-pattern (524 line in Page 6)\ntldr: The authors compare a wide variety of approaches towards sub-word\nmodelling in language modelling, and show that modelling morphology gives the\nbest results over modelling pure characters. Further, the authors do some\nprecision experiments to show that the biggest benefit towards sub-word\nmodelling is gained after words typically exhibiting rich morphology (nouns and\nverbs). The paper is comprehensive and the experiments justify the core claims\nof the paper. \n\n- Strengths:\n\n1) A comprehensive overview of different approaches and architectures towards\nsub-word level modelling, with numerous experiments designed to support the\ncore claim that the best results come from modelling morphemes.\n\n2) The authors introduce a novel form of sub-word modelling based on character\ntri-grams and show it outperforms traditional approaches on a wide variety of\nlanguages.\n\n3) Splitting the languages examined by typology and examining the effects of\nthe models on various typologies is a welcome introduction of linguistics into\nthe world of language modelling.\n\n4) The analysis of perplexity reduction after various classes of words in\nRussian and Czech is particularly illuminating, showing how character-level and\nmorpheme-level models handle rare words much more gracefully. In light of these\nresults, could the authors say something about how much language modelling\nrequires understanding of semantics, and how much it requires just knowing\nvarious morphosyntactic effects?\n\n- Weaknesses:\n\n1) The character tri-gram LSTM seems a little unmotivated. Did the authors try\nother character n-grams as well? As a reviewer, I can guess that character\ntri-grams roughly correspond to morphemes, especially in Semitic languages, but\nwhat made the authors report results for 3-grams as opposed to 2- or 4-? In\naddition, there are roughly 26^3=17576 possible distinct trigrams in the Latin\nlower-case alphabet, which is enough to almost constitute a word embedding\ntable. Did the authors only consider observed trigrams? How many distinct\nobserved trigrams were there?\n\n2) I don't think you can meaningfully claim to be examining the effectiveness\nof character-level models on root-and-pattern morphology if your dataset is\nunvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I\nappreciate that finding transcribed Arabic and Hebrew with vowels may be\nchallenging, but it's half of the typology.\n\n3) Reduplication seems to be a different kind of phenomenon to the other three,\nwhich are more strictly morphological typologies. Indonesian and Malay also\nexhibit various word affixes, which can be used on top of reduplication, which\nis a more lexical process. I'm not sure splitting it out from the other\nlinguistic typologies is justified.\n\n- General Discussion:\n\n1) The paper was structured very clearly and was very easy to read.\n\n2) I'm a bit puzzled about why the authors chose to use 200 dimensional\ncharacter embeddings. Once the dimensionality of the embedding is greater than\nthe size of the vocabulary (here the number of characters in the alphabet),\nsurely you're not getting anything extra?\n\n-------------------------------\n\nHaving read the author response, my opinions have altered little. I still think\nthe same strengths and weakness that I have already discussed hold.",
        "partial_review": "- Strengths:\ni. Motivation is well described.\nii. Provides detailed comparisons with various models across diverse languages\n\n- Weaknesses:\ni.          The conclusion is biased by the selected languages. \nii.           The experiments do not cover the claim of this paper completely.\n\n- General Discussion:\nThis paper issues a simple but fundamental question about word representation:\nwhat subunit of a word is suitable to represent morphologies and how to compose\nthe units. To answer this question, this paper applied word representations\nwith various subunits (characters, character-trigram, and morphs) and\ncomposition functions (LSTM, CNN, and a simple addition) to the language\nmodeling task to find the best combination. In addition, this paper evaluated\nthe task for more than 10 languages. This is because languages are\ntypologically diverse and the results can be different according to the word\nrepresentation and composition function. From their experimental results, this\npaper concluded that character-level representations are more effective, but\nthey are still imperfective in comparing them with a model with explicit\nknowledge of morphology. Another conclusion is that character-trigrams show\nreliable perplexity in the majority of the languages. \n\nHowever, this paper leaves some issues behind.\n-         First of all, there could be some selection bias of the experimental\nlanguages. This paper chose ten languages in four categories (up to three\nlanguages per a category). But, one basic question with the languages is \u201chow\ncan it be claimed that the languages are representatives of each category?\u201d\nAll the languages in the same category have the same tendency of word\nrepresentation and composition function? How can it be proved? For instance,\neven in this paper, two languages belonging to the same typology\n(agglutinative) show different results. Therefore, at least to me, it seems to\nbe better to focus on the languages tested in this paper instead of drawing a\ngeneral conclusions about all languages. \n-         There is some gap between the claim and the experiments. Is the\nlanguage modeling the best task to prove the claim of this paper? Isn\u2019t there\nany chance that the claim of this paper breaks in other tasks? Further\nexplanation on this issue is needed.\n-         In Section 5.2, this paper evaluated the proposed method only for\nArabic. Is there any reason why the experiment is performed only for Arabic?\nThere are plenty of languages with automatic morphological analyzers such as\nJapanese and Turkish.\n-         This paper considers only character-trigram among various n-grams. Is\nthere any good reason to choose only character-trigram? Is it always better\nthan character-bigram or character-fourgram? In general, language modeling with\nn-grams is affected by corpus size and some other factors. \n\nMinor typos: \n- There is a missing reference in Introduction. (88 line in Page 1)\n- root-and-patter -> root-and-pattern (524 line in Page 6)\ntldr: The authors compare a wide variety of approaches towards sub-word\nmodelling in language modelling, and show that modelling morphol",
        "task3_prompt": "\nComplete a partially written peer review about the paper: From Characters to Words to in Between: Do We Capture Morphology? \n\nHere is the abstract of the paper: Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data. \n\nHere is the partial review: - Strengths:\ni. Motivation is well described.\nii. Provides detailed comparisons with various models across diverse languages\n\n- Weaknesses:\ni.          The conclusion is biased by the selected languages. \nii.           The experiments do not cover the claim of this paper completely.\n\n- General Discussion:\nThis paper issues a simple but fundamental question about word representation:\nwhat subunit of a word is suitable to represent morphologies and how to compose\nthe units. To answer this question, this paper applied word representations\nwith various subunits (characters, character-trigram, and morphs) and\ncomposition functions (LSTM, CNN, and a simple addition) to the language\nmodeling task to find the best combination. In addition, this paper evaluated\nthe task for more than 10 languages. This is because languages are\ntypologically diverse and the results can be different according to the word\nrepresentation and composition function. From their experimental results, this\npaper concluded that character-level representations are more effective, but\nthey are still imperfective in comparing them with a model with explicit\nknowledge of morphology. Another conclusion is that character-trigrams show\nreliable perplexity in the majority of the languages. \n\nHowever, this paper leaves some issues behind.\n-         First of all, there could be some selection bias of the experimental\nlanguages. This paper chose ten languages in four categories (up to three\nlanguages per a category). But, one basic question with the languages is \u201chow\ncan it be claimed that the languages are representatives of each category?\u201d\nAll the languages in the same category have the same tendency of word\nrepresentation and composition function? How can it be proved? For instance,\neven in this paper, two languages belonging to the same typology\n(agglutinative) show different results. Therefore, at least to me, it seems to\nbe better to focus on the languages tested in this paper instead of drawing a\ngeneral conclusions about all languages. \n-         There is some gap between the claim and the experiments. Is the\nlanguage modeling the best task to prove the claim of this paper? Isn\u2019t there\nany chance that the claim of this paper breaks in other tasks? Further\nexplanation on this issue is needed.\n-         In Section 5.2, this paper evaluated the proposed method only for\nArabic. Is there any reason why the experiment is performed only for Arabic?\nThere are plenty of languages with automatic morphological analyzers such as\nJapanese and Turkish.\n-         This paper considers only character-trigram among various n-grams. Is\nthere any good reason to choose only character-trigram? Is it always better\nthan character-bigram or character-fourgram? In general, language modeling with\nn-grams is affected by corpus size and some other factors. \n\nMinor typos: \n- There is a missing reference in Introduction. (88 line in Page 1)\n- root-and-patter -> root-and-pattern (524 line in Page 6)\ntldr: The authors compare a wide variety of approaches towards sub-word\nmodelling in language modelling, and show that modelling morphol \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper provides a comprehensive comparison of different subunit representations for word morphologies and their composition functions. The motivation is clearly described, and the experiments cover a diverse set of languages. However, there are a few weaknesses that should be addressed.\\n\\nFirstly, the conclusion seems biased by the selected languages. The paper claims that character-level representations are more effective, but the fact that two languages belonging to the same typology (agglutinative) show different results raises questions about the generalizability of this conclusion to all languages.\\n\\nSecondly, while the experiments are extensive, it would be helpful to have a discussion on the suitability of language modeling as the task to prove the claim of this paper. It would be beneficial to explore if the claim holds true in other tasks as well.\\n\\nAdditionally, the decision to evaluate the proposed method only for Arabic in Section 5.2 should be explained. There are other languages with automatic morphological analyzers that could have been considered for comparison.\\n\\nFurthermore, the paper only considers character-trigrams among various n-grams. It would be informative to understand the reasons behind this choice and if character-trigrams consistently outperform other n-grams in different scenarios.\\n\\nMinor typos were identified: a missing reference in the Introduction and a typo in the term 'root-and-pattern' (written as 'root-and-patter'). These should be corrected.\\n\\nIn conclusion, this paper provides valuable insights into subword modeling for morphologies, but addressing the weaknesses mentioned above would further strengthen the findings and conclusions.\"\n}",
        "human_end_index": 3119
    },
    {
        "id": "481",
        "source": "PeerRead/acl_2017/",
        "title": "FOIL it! Find One mismatch between Image and Language caption",
        "human_review": "In this work, the authors extend MS-COCO by adding an incorrect\ncaption to each existing caption, with only one word of difference.\nThe authors demonstrate that two state-of-the-art methods (one for VQA\nand one for captioning) perform extremely poorly at a) determining if\na caption is fake, b) determining which word in a fake caption is\nwrong, and c) selecting a replacement word for a given fake word.\n\nThis work builds upon a wealth of literature regarding the\nunderperformance of vision/language models relative to their apparent\ncapacities. I think this work makes concrete some of the big,\nfundamental questions in this area: are vision/language models doing\n\"interesting\" things, or not? The authors consider a nice mix of tasks\nand models to shed light on the \"broken-ness\" of these settings, and\nperform some insightful analyses of factors associated with model\nfailure (e.g., Figure 3).\n\nMy biggest concerns with the paper are similarity to Ding et al. That\nbeing said, I do think the authors make some really good points; Ding\net al. generate similar captions, but the ones here differ by only one\nword and *still* break the models -- I think that's a justifiably\nfundamental difference. That observation demonstrates that Ding et\nal.'s engineering is not a requirement, as this simple approach still\nbreaks things catastrophically.\n\nAnother concern is the use of NeuralTalk to select the \"hardest\"\nfoils.              While a clever idea, I am worried that the use of this model\ncreates a risk of self-reinforcement bias, i.e., NeuralTalk's biases\nare now fundamentally \"baked-in\" to FOIL-COCO. \n\nI think the results section could be a bit longer, relative to the\nrest of the paper (e.g. I would've liked more than one paragraph -- I\nliked this part!)\n\nOverall, I do like this paper, as it nicely builds upon some results\nthat highlight defficiencies in vision/language integration. In the\nend, the Ding et al. similarity is not a \"game-breaker,\" I think -- if\nanything, this work shows that vision/language models are so easy to\nfool, Ding et al.'s method is not even required.\n\nSmall things:\n\nI would've liked to have seen another baseline that simply\nconcatenates BoW + extracted CNN features and trains a softmax\nclassifier over them. The \"blind\" model is a nice touch, but what\nabout a \"dumb\" vision+langauge baseline? I bet that would do close to\nas well as the LSTM/Co-attention. That could've made the point of the\npaper even stronger.\n\n330: What is a supercategory? Is this from WordNet? Is this from COCO?\nI understand the idea, but not the specifics.\n\n397: has been -> were\n\n494: that -> than\n\n693: artefact -> undesirable artifacts (?)\n\n701: I would have included a chance model in T1's table -- is 19.53%\n[Line 592] a constant-prediction baseline? Is it 50% (if so, can't we\nflip all of the \"blind\" predictions to get a better baseline?) I am\nnot entirely clear, and I think a \"chance\" line here would fix a lot\nof this confusion.\n\n719: ariplane\n\n~~\nAfter reading the author response...\n\nI think this author response is spot-on. Both my concerns of NeuralTalk biases\nand additional baselines were addressed, and I am confident that these can be\naddressed in the final version, so I will keep my score as-is.",
        "partial_review": "In this work, the authors extend MS-COCO by adding an incorrect\ncaption to each existing caption, with only one word of difference.\nThe authors demonstrate that two state-of-the-art methods (one for VQA\nand one for captioning) perform extremely poorly at a) determining if\na caption is fake, b) determining which word in a fake caption is\nwrong, and c) selecting a replacement word for a given fake word.\n\nThis work builds upon a wealth of literature regarding the\nunderperformance of vision/language models relative to their apparent\ncapacities. I think this work makes concrete some of the big,\nfundamental questions in this area: are vision/language models doing\n\"interesting\" things, or not? The authors consider a nice mix of tasks\nand models to shed light on the \"broken-ness\" of these settings, and\nperform some insightful analyses of factors associated with model\nfailure (e.g., Figure 3).\n\nMy biggest concerns with the paper are similarity to Ding et al. That\nbeing said, I do think the authors make some really good points; Ding\net al. generate similar captions, but the ones here differ by only one\nword and *still* break the models -- I think that's a justifiably\nfundamental difference. That observation demonstrates that Ding et\nal.'s engineering is not a requirement, as this simple approach still\nbreaks things catastrophically.\n\nAnother concern is the use of NeuralTalk to select the \"hardest\"\nfoils.              While a clever idea, I am worried that the use of this model\ncreates a risk of self-reinforcement bias, i.e., NeuralTalk's biases\nare now fundamentall",
        "task3_prompt": "\nComplete a partially written peer review about the paper: FOIL it! Find One mismatch between Image and Language caption \n\nHere is the abstract of the paper: In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and `foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (`foil word'). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption  classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image. \n\nHere is the partial review: In this work, the authors extend MS-COCO by adding an incorrect\ncaption to each existing caption, with only one word of difference.\nThe authors demonstrate that two state-of-the-art methods (one for VQA\nand one for captioning) perform extremely poorly at a) determining if\na caption is fake, b) determining which word in a fake caption is\nwrong, and c) selecting a replacement word for a given fake word.\n\nThis work builds upon a wealth of literature regarding the\nunderperformance of vision/language models relative to their apparent\ncapacities. I think this work makes concrete some of the big,\nfundamental questions in this area: are vision/language models doing\n\"interesting\" things, or not? The authors consider a nice mix of tasks\nand models to shed light on the \"broken-ness\" of these settings, and\nperform some insightful analyses of factors associated with model\nfailure (e.g., Figure 3).\n\nMy biggest concerns with the paper are similarity to Ding et al. That\nbeing said, I do think the authors make some really good points; Ding\net al. generate similar captions, but the ones here differ by only one\nword and *still* break the models -- I think that's a justifiably\nfundamental difference. That observation demonstrates that Ding et\nal.'s engineering is not a requirement, as this simple approach still\nbreaks things catastrophically.\n\nAnother concern is the use of NeuralTalk to select the \"hardest\"\nfoils.              While a clever idea, I am worried that the use of this model\ncreates a risk of self-reinforcement bias, i.e., NeuralTalk's biases\nare now fundamentall \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"My biggest concerns with the paper are similarity to Ding et al. That being said, I do think the authors make some really good points; Ding et al. generate similar captions, but the ones here differ by only one word and *still* break the models -- I think that's a justifiably fundamental difference. That observation demonstrates that Ding et al.'s engineering is not a requirement, as this simple approach still breaks things catastrophically.\\n\\nAnother concern is the use of NeuralTalk to select the 'hardest' foils. While a clever idea, I am worried that the use of this model creates a risk of self-reinforcement bias, i.e., NeuralTalk's biases are now fundamentall.\"\n}",
        "human_end_index": 1581
    },
    {
        "id": 483,
        "source": "PeerRead/acl_2017/",
        "title": "Here's My Point: Argumentation Mining with Pointer Networks",
        "human_review": "- Strengths:\n\nThis is the first neural network-based approach to argumentation\nmining. The proposed method used a Pointer Network (PN) model with\nmulti-task learning and outperformed previous methods in the\nexperiments on two datasets.\n\n- Weaknesses:\n\nThis is basically an application of PN to argumentation\nmining. Although the combination of PN and multi-task learning for\nthis task is novel, its novelty is not enough for ACL long\npublication. The lack of qualitative analysis and error analysis is\nalso a major concern.\n\n- General Discussion:\n\nBesides the weaknesses mentioned above, the use of PN is not\nwell-motivated. Although three characteristics of PN were described in\nl.138-143, these are not a strong motivation against the use of\nbi-directional LSTMs and the attention mechanism. The authors should\ndescribe what problems are solved by PN and discuss in the experiments\nhow much these problems are solved.\n\nFigures 2 and 3 are difficult to understand. What are the self link to\nD1 and the links from D2 to E1 and D3/D4 to E2? These are just the\noutputs from the decoder and not links. The decoder LSTM does not have\nan input from e_j in these figures, but it does in Equation (3). Also,\nin Figure 3, the abbreviation \"FC\" is not defined.\n\nEquation (8) is strange. To calculate the probability of each\ncomponent type, the probability of E_i is calculated.\n\nIn the experiments, I did not understand why only \"PN\", which is not a\njoint model, was performed for the microtext corpus.\n\nIt is not clear whether the BLSTM model is trained with the joint-task\nobjective.\n\nThere are some studies on discourse parsing using the attention\nmechanism. The authors should describe the differences from these studies.\n\nMinor issues:\n\nl.128: should related -> should be related\n\nl.215: (2015) is floating\n\nl.706: it able -> it is able\n\nI raised my recommendation score after reading the convincing author responses.\nI strongly recommend that the authors should discuss improved examples by PN as\nwell as the details of feature ablation.\nThe paper presents an application of Pointer Networks, a recurrent neural\nnetwork model original used for solving algorithmic tasks, to two subtasks of\nArgumentation Mining: determining the types of Argument Components, and finding\nthe links between them. The model achieves state-of-the-art results.\n\nStrengths:\n\n- Thorough review of prior art in the specific formulation of argument mining\nhandled in this paper.\n- Simple and effective modification of an existing model to make it suitable\nfor\nthe task. The model is mostly explained clearly.\n- Strong results as compared to prior art in this task.\n\nWeaknesses:\n\n- 071: This formulation of argumentation mining is just one of several proposed\nsubtask divisions, and this should be mentioned. For example, in [1], claims\nare detected and classified before any supporting evidence is detected.\nFurthermore, [2] applied neural networks to this task, so it is inaccurate to\nsay (as is claimed in the abstract of this paper) that this work is the first\nNN-based approach to argumentation mining.\n- Two things must be improved in the presentation of the model: (1) What is the\npooling method used for embedding features (line 397)? and (2) Equation (7) in\nline 472 is not clear enough: is E_i the random variable representing the\n*type* of AC i, or its *identity*? Both are supposedly modeled (the latter by\nfeature representation), and need to be defined. Furthermore, it seems like the\nLHS of equation (7) should be a conditional probability.\n- There are several unclear things about Table 2: first, why are the three\nfirst\nbaselines evaluated only by macro f1 and the individual f1 scores are missing?\nThis is not explained in the text. Second, why is only the \"PN\" model\npresented? Is this the same PN as in Table 1, or actually the Joint Model? What\nabout the other three?\n- It is not mentioned which dataset the experiment described in Table 4 was\nperformed on.\n\nGeneral Discussion:\n\n- 132: There has to be a lengthier introduction to pointer networks, mentioning\nrecurrent neural networks in general, for the benefit of readers unfamiliar\nwith \"sequence-to-sequence models\". Also, the citation of Sutskever et al.\n(2014) in line 145 should be at the first mention of the term, and the\ndifference with respect to recursive neural networks should be explained before\nthe paragraph starting in line 233 (tree structure etc.).\n- 348: The elu activation requires an explanation and citation (still not\nenough\nwell-known).\n- 501: \"MC\", \"Cl\" and \"Pr\" should be explained in the label.\n- 577: A sentence about how these hyperparameters were obtained would be\nappropriate.\n- 590: The decision to do early stopping only by link prediction accuracy\nshould\nbe explained (i.e. why not average with type accuracy, for example?).\n- 594: Inference at test time is briefly explained, but would benefit from more\ndetails.\n- 617: Specify what the length of an AC is measured in (words?).\n- 644: The referent of \"these\" in \"Neither of these\" is unclear.\n- 684: \"Minimum\" should be \"Maximum\".\n- 694: The performance w.r.t. the amount of training data is indeed surprising,\nbut other models have also achieved almost the same results - this is\nespecially surprising because NNs usually need more data. It would be good to\nsay this.\n- 745: This could alternatively show that structural cues are less important\nfor\nthis task.\n- Some minor typos should be corrected (e.g. \"which is show\", line 161).\n\n[1] Rinott, Ruty, et al. \"Show Me Your Evidence-an Automatic Method for Context\nDependent Evidence Detection.\" EMNLP. 2015.\n\n[2] Laha, Anirban, and Vikas Raykar. \"An Empirical Evaluation of various Deep\nLearning Architectures for Bi-Sequence Classification Tasks.\" COLING. 2016.",
        "partial_review": "- Strengths:\n\nThis is the first neural network-based approach to argumentation\nmining. The proposed method used a Pointer Network (PN) model with\nmulti-task learning and outperformed previous methods in the\nexperiments on two datasets.\n\n- Weaknesses:\n\nThis is basically an application of PN to argumentation\nmining. Although the combination of PN and multi-task learning for\nthis task is novel, its novelty is not enough for ACL long\npublication. The lack of qualitative analysis and error analysis is\nalso a major concern.\n\n- General Discussion:\n\nBesides the weaknesses mentioned above, the use of PN is not\nwell-motivated. Although three characteristics of PN were described in\nl.138-143, these are not a strong motivation against the use of\nbi-directional LSTMs and the attention mechanism. The authors should\ndescribe what problems are solved by PN and discuss in the experiments\nhow much these problems are solved.\n\nFigures 2 and 3 are difficult to understand. What are the self link to\nD1 and the links from D2 to E1 and D3/D4 to E2? These are just the\noutputs from the decoder and not links. The decoder LSTM does not have\nan input from e_j in these figures, but it does in Equation (3). Also,\nin Figure 3, the abbreviation \"FC\" is not defined.\n\nEquation (8) is strange. To calculate the probability of each\ncomponent type, the probability of E_i is calculated.\n\nIn the experiments, I did not understand why only \"PN\", which is not a\njoint model, was performed for the microtext corpus.\n\nIt is not clear whether the BLSTM model is trained with the joint-task\nobjective.\n\nThere are some studies on discourse parsing using the attention\nmechanism. The authors should describe the differences from these studies.\n\nMinor issues:\n\nl.128: should related -> should be related\n\nl.215: (2015) is floating\n\nl.706: it able -> it is able\n\nI raised my recommendation score after reading the convincing author responses.\nI strongly recommend that the authors should discuss improved examples by PN as\nwell as the details of feature ablation.\nThe paper presents an application of Pointer Networks, a recurrent neural\nnetwork model original used for solving algorithmic tasks, to two subtasks of\nArgumentation Mining: determining the types of Argument Components, and finding\nthe links between them. The model achieves state-of-the-art results.\n\nStrengths:\n\n- Thorough review of prior art in the specific formulation of argument mining\nhandled in this paper.\n- Simple and effective modification of an existing model to make it suitable\nfor\nthe task. The model is mostly explained clearly.\n- Strong results as compared to prior art in this task.\n\nWeaknesses:\n\n- 071: This formulation of argumentation mining is just one of several proposed\nsubtask divisions, and this should be mentioned. For example, in [1], claims\nare detected and classified befor",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Here's My Point: Argumentation Mining with Pointer Networks \n\nHere is the abstract of the paper: Argumentation mining seeks to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the dual tasks of extracting links between argument components, and classifying types of argument components. We propose to use a joint model based on a Pointer Network architecture to simultaneously solve these tasks. In doing so, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks is crucial for high performance. \n\nHere is the partial review: - Strengths:\n\nThis is the first neural network-based approach to argumentation\nmining. The proposed method used a Pointer Network (PN) model with\nmulti-task learning and outperformed previous methods in the\nexperiments on two datasets.\n\n- Weaknesses:\n\nThis is basically an application of PN to argumentation\nmining. Although the combination of PN and multi-task learning for\nthis task is novel, its novelty is not enough for ACL long\npublication. The lack of qualitative analysis and error analysis is\nalso a major concern.\n\n- General Discussion:\n\nBesides the weaknesses mentioned above, the use of PN is not\nwell-motivated. Although three characteristics of PN were described in\nl.138-143, these are not a strong motivation against the use of\nbi-directional LSTMs and the attention mechanism. The authors should\ndescribe what problems are solved by PN and discuss in the experiments\nhow much these problems are solved.\n\nFigures 2 and 3 are difficult to understand. What are the self link to\nD1 and the links from D2 to E1 and D3/D4 to E2? These are just the\noutputs from the decoder and not links. The decoder LSTM does not have\nan input from e_j in these figures, but it does in Equation (3). Also,\nin Figure 3, the abbreviation \"FC\" is not defined.\n\nEquation (8) is strange. To calculate the probability of each\ncomponent type, the probability of E_i is calculated.\n\nIn the experiments, I did not understand why only \"PN\", which is not a\njoint model, was performed for the microtext corpus.\n\nIt is not clear whether the BLSTM model is trained with the joint-task\nobjective.\n\nThere are some studies on discourse parsing using the attention\nmechanism. The authors should describe the differences from these studies.\n\nMinor issues:\n\nl.128: should related -> should be related\n\nl.215: (2015) is floating\n\nl.706: it able -> it is able\n\nI raised my recommendation score after reading the convincing author responses.\nI strongly recommend that the authors should discuss improved examples by PN as\nwell as the details of feature ablation.\nThe paper presents an application of Pointer Networks, a recurrent neural\nnetwork model original used for solving algorithmic tasks, to two subtasks of\nArgumentation Mining: determining the types of Argument Components, and finding\nthe links between them. The model achieves state-of-the-art results.\n\nStrengths:\n\n- Thorough review of prior art in the specific formulation of argument mining\nhandled in this paper.\n- Simple and effective modification of an existing model to make it suitable\nfor\nthe task. The model is mostly explained clearly.\n- Strong results as compared to prior art in this task.\n\nWeaknesses:\n\n- 071: This formulation of argumentation mining is just one of several proposed\nsubtask divisions, and this should be mentioned. For example, in [1], claims\nare detected and classified befor \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This is the first neural network-based approach to argumentation mining. The proposed method used a Pointer Network (PN) model with multi-task learning and outperformed previous methods in the experiments on two datasets. However, the novelty of using PN for this task may not be sufficient for publication in ACL. The lack of qualitative analysis and error analysis is also a major concern. The use of PN is not well-motivated and the authors should discuss in more detail why PN was chosen over bi-directional LSTMs and the attention mechanism. The figures and equations need clarification and there are minor issues with language and floating references. The recommendation score was raised after reading the author responses, but the authors should still provide improved examples by PN and details of feature ablation. Overall, the paper has strengths in its thorough review of prior art and strong results, but weaknesses in terms of novelty, qualitative analysis, and clarity of figures and equations.\"\n}",
        "human_end_index": 2840
    },
    {
        "id": "484",
        "source": "PeerRead/acl_2017/",
        "title": "Joint CTC/attention decoding for end-to-end speech recognition",
        "human_review": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n\n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n\n(a) Title: \nThe title in Ref [Kim et al., 2016] is \u201cJoint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning\u201d, while the title of this paper\nis \u201cJoint CTC-attention End-to-end Speech Recognition\u201d. I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n\n(b) Introduction:\nThe author claims that \u201cWe propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].\u201c Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement \u201cwe propose \u2026 as proposed by [Kim et al,\n2016]\u201d is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and Mandarin Chinese tasks. I think it\nwould be interesting if the paper could explain in more details about the\nspecific problems in Japanese and Mandarin Chinese tasks that may not appear in\nEnglish task. For example, how the system could address multiple possible\noutputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input\nwithout using any linguistic resources. This could be one of the important\ncontributions from this paper.\n\n- General Discussion:\nI think it would be better to cite Ref [Kim et al., 2016] from\nthe official IEEE ICASSP conference, rather than pre-published arXiv:\nKim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech\nRecognition Using Multi-task Learning\", IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear.\nThe paper considers a synergistic combination of two non-HMM based speech\nrecognition techniques: CTC and attention-based seq2seq networks. The\ncombination is two-fold:\n1. first, similarly to Kim et al. 2016 multitask learning is used to train a\nmodel with a joint CTC and seq2seq cost.\n2. second (novel contribution), the scores of the CTC model and seq2seq model\nare ensembled during decoding (results of beam search over the seq2seq model\nare rescored with the CTC model).\n\nThe main novelty of the paper is in using the CTC model not only as an\nauxiliary training objective (originally proposed by Kim et al. 2016), but also\nduring decoding.\n\n- Strengths:\nThe paper identifies several problems stemming from the flexibility offered by\nthe attention mechanism and shows that by combining the seq2seq network with\nCTC the problems are mitigated.\n\n- Weaknesses:\nThe paper is an incremental improvement over Kim et al. 2016 (since two models\nare trained, their outputs can just as well be ensembled). However, it is nice\nto see that such a simple change offers important performance improvements of\nASR systems.\n\n- General Discussion:\nA lot of the paper is spent on explaining the well-known, classical ASR\nsystems. A description of the core improvement of the paper (better decoding\nalgorithm) starts to appear only on p. 5. \n\nThe description of CTC is nonstandard and maybe should either be presented in a\nmore standard way, or the explanation should be expanded. Typically, the\nrelation p(C|Z) (eq. 5) is deterministic - there is one and only one character\nsequence that corresponds to the blank-expanded form Z. I am also unsure about\nthe last transformation of the eq. 5.",
        "partial_review": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n\n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n\n(a) Title: \nThe title in Ref [Kim et al., 2016] is \u201cJoint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning\u201d, while the title of this paper\nis \u201cJoint CTC-attention End-to-end Speech Recognition\u201d. I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n\n(b) Introduction:\nThe author claims that \u201cWe propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].\u201c Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement \u201cwe propose \u2026 as proposed by [Kim et al,\n2016]\u201d is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Joint CTC/attention decoding for end-to-end speech recognition \n\nHere is the abstract of the paper: End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR:  attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources. \n\nHere is the partial review: This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n\n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n\n(a) Title: \nThe title in Ref [Kim et al., 2016] is \u201cJoint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning\u201d, while the title of this paper\nis \u201cJoint CTC-attention End-to-end Speech Recognition\u201d. I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n\n(b) Introduction:\nThe author claims that \u201cWe propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].\u201c Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement \u201cwe propose \u2026 as proposed by [Kim et al,\n2016]\u201d is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper proposes a joint CTC/attention decoding algorithm for end-to-end automatic speech recognition (ASR). The strengths of the paper are that it provides a solid work on the hybrid CTC/attention framework in training and decoding, and the experimental results show improved performance in Japanese and Mandarin Chinese ASR benchmarks. However, a weakness of the paper is that it is not clearly differentiated from a similar paper by Kim et al., 2016. The difference between the two papers is not clearly explained, making it difficult to identify the original contribution of this paper. The title of the paper is also too general and does not specify the main contribution in contrast to the existing publication. The introduction of the paper claims to propose taking advantage of the constrained CTC alignment in a hybrid CTC/attention system, but this idea has already been proposed by Kim et al., 2016. It is important for the authors to clearly state the original contribution of their paper and its position in relation to existing literature. Overall, the paper has potential, but there are some areas that need improvement and clarification.\"\n}",
        "human_end_index": 2807
    },
    {
        "id": 494,
        "source": "PeerRead/acl_2017/",
        "title": "Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules",
        "human_review": "- Strengths:\n\n- nice, clear application of linguistics ideas to distributional semantics\n- demonstrate very clear improvements on both intrinsic and extrinsic eval\n\n- Weaknesses:\n\n- fairly straightforward extension of existing retrofitting work\n- would be nice to see some additional baselines (e.g. character embeddings)\n\n- General Discussion:\n\nThe paper describes \"morph-fitting\", a type of retrofitting for vector spaces\nthat focuses specifically on incorporating morphological constraints into the\nvector space. The framework is based on the idea of \"attract\" and \"repel\"\nconstraints, where attract constraints are used to pull morphological\nvariations close together (e.g. look/looking) and repel constraints are used to\npush derivational antonyms apart (e.g. responsible/irresponsible). They test\ntheir algorithm on multiple different vector spaces and several language, and\nshow consistent improvements on intrinsic evaluation (SimLex-999, and\nSimVerb-3500). They also test on the extrinsic task of dialogue state tracking,\nand again demonstrate measurable improvements over using\nmorphologically-unaware word embeddings.\n\nI think this is a very nice paper. It is a simple and clean way to incorporate\nlinguistic knowledge into distributional models of semantics, and the empirical\nresults are very convincing. I have some questions/comments below, but nothing\nthat I feel should prevent it from being published.\n\n- Comments for Authors\n\n1) I don't really understand the need for the morph-simlex evaluation set. It\nseems a bit suspect to create a dataset using the same algorithm that you\nultimately aim to evaluate. It seems to me a no-brainer that your model will do\nwell on a dataset that was constructed by making the same assumptions the model\nmakes. I don't think you need to include this dataset at all, since it is a\npotentially erroneous evaluation that can cause confusion, and your results are\nconvincing enough on the standard datasets.\n\n2) I really liked the morph-fix baseline, thank you for including that. I would\nhave liked to see a baseline based on character embeddings, since this seems to\nbe the most fashionable way, currently, to side-step dealing with morphological\nvariation. You mentioned it in the related work, but it would be better to\nactually compare against it empirically.\n\n3) Ideally, we would have a vector space where morphological variants are just\nclose together, but where we can assign specific semantics to the different\ninflections. Do you have any evidence that the geometry of the space you end\nwith is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would\nbe nice to have some analysis that suggests the morphfitting results in a more\nmeaningful space, not just better embeddings.\nThe authors propose \u2018morph-fitting\u2019, a method that retrofits any given set\nof trained word embeddings based on a morphologically-driven objective that (1)\npulls inflectional forms of the same word together (as in \u2018slow\u2019 and\n\u2018slowing\u2019) and (2) pushes derivational antonyms apart (as in\n\u2018expensive\u2019 and \u2018inexpensive\u2019). With this, the authors aim to improve\nthe representation of low-frequency inflections of words as well as mitigate\nthe tendency of corpus-based word embeddings to assign similar representations\nto antonyms. The method is based on relatively simple manually-constructed\nmorphological rules and is demonstrated on both English, German, Italian and\nRussian. The experiments include intrinsic word similarity benchmarks, showing\nnotable performance improvements achieved by applying morph-fitting to several\ndifferent corpus-based embeddings. Performance improvement yielding new\nstate-of-the-art results is also demonstrated for German and Italian on an\nextrinsic task - dialog state tracking. \n\nStrengths:\n\n- The proposed method is simple and shows nice performance improvements across\na number of evaluations and in several languages. Compared to previous\nknowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a\nfew manually-constructed rules, instead of a large-scale knowledge base, such\nas an ontology.\n\n- Like previous retrofitting approaches, this method is easy to apply to\nexisting sets of embeddings and therefore it seems like the software that the\nauthors intend to release could be useful to the community.\n\n- The method and experiments are clearly described.\u2028\n\nWeaknesses:\n\n- I was hoping to see some analysis of why the morph-fitted embeddings worked\nbetter in the evaluation, and how well that corresponds with the intuitive\nmotivation of the authors. \n\n- The authors introduce a synthetic word similarity evaluation dataset,\nMorph-SimLex. They create it by applying their presumably\nsemantic-meaning-preserving morphological rules to SimLex999 to generate many\nmore pairs with morphological variability. They do not manually annotate these\nnew pairs, but rather use the original similarity judgements from SimLex999.\nThe obvious caveat with this dataset is that the similarity scores are presumed\nand therefore less reliable. Furthermore, the fact that this dataset was\ngenerated by the very same rules that are used in this work to morph-fit word\nembeddings, means that the results reported on this dataset in this work should\nbe taken with a grain of salt. The authors should clearly state this in their\npaper.\n\n- (Soricut and Och, 2015) is mentioned as a future source for morphological\nknowledge, but in fact it is also an alternative approach to the one proposed\nin this paper for generating morphologically-aware word representations. The\nauthors should present it as such and differentiate their work.\n\n- The evaluation does not include strong morphologically-informed embedding\nbaselines. \n\nGeneral Discussion:\n\nWith the few exceptions noted, I like this work and I think it represents a\nnice contribution to the community. The authors presented a simple approach and\nshowed that it can yield nice improvements using various common embeddings on\nseveral evaluations and four different languages. I\u2019d be happy to see it in\nthe conference.\n\nMinor comments:\n\n- Line 200: I found this phrasing unclear: \u201cWe then query \u2026 of linguistic\nconstraints\u201d.\n\n- Section 2.1: I suggest to elaborate a little more on what the delta is\nbetween the model used in this paper and the one it is based on in Wieting\n2015. It seemed to me that this was mostly the addition of the REPEL part.\n\n- Line 217: \u201cThe method\u2019s cost function consists of three terms\u201d - I\nsuggest to spell this out in an equation.\n\n- Line 223:  x and t in this equation (and following ones) are the vector\nrepresentations of the words. I suggest to denote that somehow. Also, are the\nvectors L2-normalized before this process? Also, when computing \u2018nearest\nneighbor\u2019 examples do you use cosine or dot-product? Please share these\ndetails.\n\n- Line 297-299: I suggest to move this text to Section 3, and make the note\nthat you did not fine-tune the params in the main text and not in a footnote.\n\n- Line 327: (create, creates) seems like a wrong example for that rule.\u2028\n\n* I have read the author response",
        "partial_review": "- Strengths:\n\n- nice, clear application of linguistics ideas to distributional semantics\n- demonstrate very clear improvements on both intrinsic and extrinsic eval\n\n- Weaknesses:\n\n- fairly straightforward extension of existing retrofitting work\n- would be nice to see some additional baselines (e.g. character embeddings)\n\n- General Discussion:\n\nThe paper describes \"morph-fitting\", a type of retrofitting for vector spaces\nthat focuses specifically on incorporating morphological constraints into the\nvector space. The framework is based on the idea of \"attract\" and \"repel\"\nconstraints, where attract constraints are used to pull morphological\nvariations close together (e.g. look/looking) and repel constraints are used to\npush derivational antonyms apart (e.g. responsible/irresponsible). They test\ntheir algorithm on multiple different vector spaces and several language, and\nshow consistent improvements on intrinsic evaluation (SimLex-999, and\nSimVerb-3500). They also test on the extrinsic task of dialogue state tracking,\nand again demonstrate measurable improvements over using\nmorphologically-unaware word embeddings.\n\nI think this is a very nice paper. It is a simple and clean way to incorporate\nlinguistic knowledge into distributional models of semantics, and the empirical\nresults are very convincing. I have some questions/comments below, but nothing\nthat I feel should prevent it from being published.\n\n- Comments for Authors\n\n1) I don't really understand the need for the morph-simlex evaluation set. It\nseems a bit suspect to create a dataset using the same algorithm that you\nultimately aim to evaluate. It seems to me a no-brainer that your model will do\nwell on a dataset that was constructed by making the same assumptions the model\nmakes. I don't think you need to include this dataset at all, since it is a\npotentially erroneous evaluation that can cause confusion, and your results are\nconvincing enough on the standard datasets.\n\n2) I really liked the morph-fix baseline, thank you for including that. I would\nhave liked to see a baseline based on character embeddings, since this seems to\nbe the most fashionable way, currently, to side-step dealing with morphological\nvariation. You mentioned it in the related work, but it would be better to\nactually compare against it empirically.\n\n3) Ideally, we would have a vector space where morphological variants are just\nclose together, but where we can assign specific semantics to the different\ninflections. Do you have any evidence that the geometry of the space you end\nwith is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would\nbe nice to have some analysis that suggests the morphfitting results in a more\nmeaningful space, not just better embeddings.\nThe authors propose \u2018morph-fitting\u2019, a method that retrofits any given set\nof trained word embeddings based on a morphologically-driven objective that (1)\npulls inflectional forms of the same word together (as in \u2018slow\u2019 and\n\u2018slowing\u2019) and (2) pushes derivational antonyms apart (as in\n\u2018expensive\u2019 and \u2018inexpensive\u2019). With this, the authors aim to improve\nthe representation of low-frequency inflections of words as well as mitigate\nthe tendency of corpus-based word embeddings to assign similar representations\nto antonyms. The method is based on relatively simple manually-constructed\nmorphological rules and is demonstrated on both English, German, Italian and\nRussian. The experiments include intrinsic word similarity benchmarks, showing\nnotable performance improvements achieved by applying morph-fitting to several\ndiffe",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules \n\nHere is the abstract of the paper: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that  'inexpensive' is a rephrasing for 'expensive' or may not associate 'acquire' with 'acquires'. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks. \n\nHere is the partial review: - Strengths:\n\n- nice, clear application of linguistics ideas to distributional semantics\n- demonstrate very clear improvements on both intrinsic and extrinsic eval\n\n- Weaknesses:\n\n- fairly straightforward extension of existing retrofitting work\n- would be nice to see some additional baselines (e.g. character embeddings)\n\n- General Discussion:\n\nThe paper describes \"morph-fitting\", a type of retrofitting for vector spaces\nthat focuses specifically on incorporating morphological constraints into the\nvector space. The framework is based on the idea of \"attract\" and \"repel\"\nconstraints, where attract constraints are used to pull morphological\nvariations close together (e.g. look/looking) and repel constraints are used to\npush derivational antonyms apart (e.g. responsible/irresponsible). They test\ntheir algorithm on multiple different vector spaces and several language, and\nshow consistent improvements on intrinsic evaluation (SimLex-999, and\nSimVerb-3500). They also test on the extrinsic task of dialogue state tracking,\nand again demonstrate measurable improvements over using\nmorphologically-unaware word embeddings.\n\nI think this is a very nice paper. It is a simple and clean way to incorporate\nlinguistic knowledge into distributional models of semantics, and the empirical\nresults are very convincing. I have some questions/comments below, but nothing\nthat I feel should prevent it from being published.\n\n- Comments for Authors\n\n1) I don't really understand the need for the morph-simlex evaluation set. It\nseems a bit suspect to create a dataset using the same algorithm that you\nultimately aim to evaluate. It seems to me a no-brainer that your model will do\nwell on a dataset that was constructed by making the same assumptions the model\nmakes. I don't think you need to include this dataset at all, since it is a\npotentially erroneous evaluation that can cause confusion, and your results are\nconvincing enough on the standard datasets.\n\n2) I really liked the morph-fix baseline, thank you for including that. I would\nhave liked to see a baseline based on character embeddings, since this seems to\nbe the most fashionable way, currently, to side-step dealing with morphological\nvariation. You mentioned it in the related work, but it would be better to\nactually compare against it empirically.\n\n3) Ideally, we would have a vector space where morphological variants are just\nclose together, but where we can assign specific semantics to the different\ninflections. Do you have any evidence that the geometry of the space you end\nwith is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would\nbe nice to have some analysis that suggests the morphfitting results in a more\nmeaningful space, not just better embeddings.\nThe authors propose \u2018morph-fitting\u2019, a method that retrofits any given set\nof trained word embeddings based on a morphologically-driven objective that (1)\npulls inflectional forms of the same word together (as in \u2018slow\u2019 and\n\u2018slowing\u2019) and (2) pushes derivational antonyms apart (as in\n\u2018expensive\u2019 and \u2018inexpensive\u2019). With this, the authors aim to improve\nthe representation of low-frequency inflections of words as well as mitigate\nthe tendency of corpus-based word embeddings to assign similar representations\nto antonyms. The method is based on relatively simple manually-constructed\nmorphological rules and is demonstrated on both English, German, Italian and\nRussian. The experiments include intrinsic word similarity benchmarks, showing\nnotable performance improvements achieved by applying morph-fitting to several\ndiffe \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"rent vector spaces. The authors also evaluate the effectiveness of the morph-fitted vectors on the downstream task of dialogue state tracking and show significant improvements. \\n\\nOverall, this paper presents a novel method, morph-fitting, for incorporating morphological constraints into distributional vector spaces. The proposed method is simple yet effective, and the results from the experiments demonstrate substantial improvements in both intrinsic and extrinsic evaluations. Additionally, the paper could benefit from including additional baselines such as character embeddings and providing evidence of the meaningfulness of the resulting vector space. Despite these minor suggestions, the paper is well-written and the findings are convincing. Therefore, I recommend accepting it for publication.\"\n}",
        "human_end_index": 3578
    },
    {
        "id": 501,
        "source": "PeerRead/acl_2017/",
        "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task",
        "human_review": "The paper proposes a task of selecting the most appropriate textual description\nfor a given scene/image from a list of similar options. It also proposes couple\nof baseline models, an evaluation metrics and human evaluation score. \n\n- Strengths:\n\nThe paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical\nevidence. So the paper is very easy to read. \n\nThe paper is well motivated. A method of selecting the most appropriate caption\ngiven a list of misleading candidates will benefit other\nimage-caption/understanding models, by acting as a post-generation re-ranking\nmethod. \n\n- Weaknesses:\n\nI am not sure if the proposed algorithm for decoys generation is effective,\nwhich as a consequence puts the paper on questions.\n\nFor each target caption, the algorithm basically picks out those with similar\nrepresentation and surface form but do not belong to the same image. But a\nfundamentally issue with this approach is: not belonging to the image-A does\nnot mean not appropriate to describe image-A, especially when the\nrepresentation and surface form are close. So the ground-truth labels might not\nbe valid. As we can see in Figure-1, the generated decoys are either too far\nfrom the target to be a *good* decoy (*giraffe* vs *elephant*), or fair\nsubstitutes for the target (*small boy playing kites* vs *boy flies a kite*).\n\nThus, I am afraid that the dataset generated with this algorithm can not train\na model to really *go beyond key word recognition*, which was claimed as\ncontribution in this paper. As shown in Figure-1, most\ndecoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs\nbread*, *frisbee vs kite*, etc. And when they can not be separated by *key word\nmatch*, they look very tempting to be a correct option.\n\nFurthermore, it is interesting that humans only do correctly on 82.8% on a\nsampled test set. Does it mean that those examples are really too hard even for\nhuman to correctly classify? Or are some of the *decoys* in fact good enough to\nbe the target's substitute (or even better) so that human choose them over\nground-truth targets?\n\n- General Discussion:\n\nI think this is a well-written paper with clear motivation and substantial\nexperiments. \nThe major issue is that the data-generating algorithm and the generated dataset\ndo not seem helpful for the motivation. This in turn makes the experimental\nconclusions less convincing. So I tend to reject this paper unless my concerns\ncan be fully addressed in rebuttal.\n- Strengths:\n\nAuthors generate a dataset of \u201crephrased\u201d captions and are planning to make\nthis dataset publicly available.\n\nThe way authors approached DMC task has an advantage over VQA or caption\ngeneration in terms of metrics. It is easier and more straightforward to\nevaluate problem of choosing the best caption. Authors use accuracy metric.\nWhile for instance caption generation requires metrics like BLUE or Meteor\nwhich are limited in handling semantic similarity.\n\nAuthors propose an interesting approach to \u201crephrasing\u201d, e.g. selecting\ndecoys. They draw decoys form image-caption dataset. E.g. decoys for a single\nimage come from captions for other images. These decoys however are similar to\neach other both in terms of surface (bleu score) and semantics (PV similarity).\nAuthors use lambda factor to decide on the balance between these two components\nof the similarity score. I think it would be interesting to employ these for\nparaphrasing.\n\nAuthors support their motivation for the task with evaluation results. They\nshow that a system trained with the focus on differentiating between similar\ncaptions performs better than a system that is trained to generate captions\nonly. These are, however, showing that system that is tuned for a particular\ntask performs better on this task.\n\n- Weaknesses:\n\n It is not clear why image caption task is not suitable for comprehension task\nand why author\u2019s system is better for this. In order to argue that system can\ncomprehend image and sentence semantics better one should apply learned\nrepresentation, e.g. embeddings. E.g. apply representations learned by\ndifferent systems on the same task for comparison.\n\nMy main worry about the paper is that essentially authors converge to using\nexisting caption generation techniques, e.g. Bahdanau et al., Chen et al.\n\nThey way formula (4) is presented is a bit confusing. From formula it seems\nthat both decoy and true captions are employed for both loss terms. However, as\nit makes sense, authors mention that they do not use decoy for the second term.\nThat would hurt mode performance as model would learn to generate decoys as\nwell. The way it is written in the text is ambiguous, so I would make it more\nclear either in the formula itself or in the text. Otherwise it makes sense for\nthe model to learn to generate only true captions while learning to distinguish\nbetween true caption and a decoy.\n\n- General Discussion:\n\nAuthors formulate a task of Dual Machine Comprehension. They aim to accomplish\nthe task by challenging computer system to solve a problem of choosing between\ntwo very similar captions for a given image. Authors argue that a system that\nis able to solve this problem has to \u201cunderstand\u201d the image and captions\nbeyond just keywords but also capture semantics of captions and their alignment\nwith image semantics.\n\nI think paper need to make more focus on why chosen approach is better than\njust caption generation and why in their opinion caption generation is less\nchallenging for learning image and text representation and their alignment.\n\nFor formula (4). I wonder if in the future it is possible to make model to\nlearn \u201cnot to generate\u201d decoys by adjusting second loss term to include\ndecoys but with a negative sign. Did authors try something similar?\n- Strengths:\n\nThe DMC task seems like a good test of understanding language and vision. I\nlike that the task has a clear evaluation metric.\n\nThe failure of the caption generation model on the DMC task is quite\ninteresting. This result further demonstrates that these models are good\nlanguage models, but not as good at capturing the semantics of the image.\n\n- Weaknesses:\n\nThe experiments are missing a key baseline: a state-of-the-art VQA model\ntrained with only a yes/no label vocabulary. \n\nI would have liked more details on the human performance experiments. How many\nof the ~20% of incorrectly-predicted images are because the captions are\ngenuinely ambiguous? Could the data be further cleaned up to yield an even\nhigher human accuracy?\n\n- General Discussion:\n\nMy concern with this paper is that the data set may prove to be easy or\ngameable in some way. The authors can address this concern by running a suite\nof strong baselines on their data set and demonstrating their accuracies. I'm\nnot convinced by the current set of experiments because the chosen neural\nnetwork architectures appear quite different from the state-of-the-art\narchitectures in similar tasks, which typically rely on attention mechanisms\nover the image.\n\nAnother nice addition to this paper would be an analysis of the data set. How\nmany tokens does the correct caption share with distractors on average? What\nkind of understanding is necessary to distinguish between the correct and\nincorrect captions? I think this kind of analysis really helps the reader\nunderstand why this task is worthwhile relative to the many other similar\ntasks. \n\nThe data generation technique is quite simple and wouldn't really qualify as a\nsignificant contribution, unless it worked surprisingly well.\n\n- Notes\n\nI couldn't find a description of the FFNN architecture in either the paper or\nthe supplementary material. It looks like some kind of convolutional network\nover the tokens, but the details are very unclear. I'm also confused about how\nthe Veq2Seq+FFNN model is applied to both classification and caption\ngeneration. Is the loglikelihood of the caption combined with the FFNN\nprediction during classification? Is the FFNN score incorporated during caption\ngeneration?\n\nThe fact that the caption generation model performs (statistically\nsignificantly) *worse* than random chance needs some explanation. How is this\npossible?\n\n528 - this description of the neural network is hard to understand. The final\nparagraph of the section makes it clear, however. Consider starting the section\nwith it.",
        "partial_review": "The paper proposes a task of selecting the most appropriate textual description\nfor a given scene/image from a list of similar options. It also proposes couple\nof baseline models, an evaluation metrics and human evaluation score. \n\n- Strengths:\n\nThe paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical\nevidence. So the paper is very easy to read. \n\nThe paper is well motivated. A method of selecting the most appropriate caption\ngiven a list of misleading candidates will benefit other\nimage-caption/understanding models, by acting as a post-generation re-ranking\nmethod. \n\n- Weaknesses:\n\nI am not sure if the proposed algorithm for decoys generation is effective,\nwhich as a consequence puts the paper on questions.\n\nFor each target caption, the algorithm basically picks out those with similar\nrepresentation and surface form but do not belong to the same image. But a\nfundamentally issue with this approach is: not belonging to the image-A does\nnot mean not appropriate to describe image-A, especially when the\nrepresentation and surface form are close. So the ground-truth labels might not\nbe valid. As we can see in Figure-1, the generated decoys are either too far\nfrom the target to be a *good* decoy (*giraffe* vs *elephant*), or fair\nsubstitutes for the target (*small boy playing kites* vs *boy flies a kite*).\n\nThus, I am afraid that the dataset generated with this algorithm can not train\na model to really *go beyond key word recognition*, which was claimed as\ncontribution in this paper. As shown in Figure-1, most\ndecoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs\nbread*, *frisbee vs kite*, etc. And when they can not be separated by *key word\nmatch*, they look very tempting to be a correct option.\n\nFurthermore, it is interesting that humans only do correctly on 82.8% on a\nsampled test set. Does it mean that those examples are really too hard even for\nhuman to correctly classify? Or are some of the *decoys* in fact good enough to\nbe the target's substitute (or even better) so that human choose them over\nground-truth targets?\n\n- General Discussion:\n\nI think this is a well-written paper with clear motivation and substantial\nexperiments. \nThe major issue is that the data-generating algorithm and the generated dataset\ndo not seem helpful for the motivation. This in turn makes the experimental\nconclusions less convincing. So I tend to reject this paper unless my concerns\ncan be fully addressed in rebuttal.\n- Strengths:\n\nAuthors generate a dataset of \u201crephrased\u201d captions and are planning to make\nthis dataset publicly available.\n\nThe way authors approached DMC task has an advantage over VQA or caption\ngeneration in terms of metrics. It is easier and more straightforward to\nevaluate problem of choosing the best caption. Authors use accuracy metric.\nWhile for instance caption generation requires metrics like BLUE or Meteor\nwhich are limited in handling semantic similarity.\n\nAuthors propose an interesting approach to \u201crephrasing\u201d, e.g. selecting\ndecoys. They draw decoys form image-caption dataset. E.g. decoys for a single\nimage come from captions for other images. These decoys however are similar to\neach other both in terms of surface (bleu score) and semantics (PV similarity).\nAuthors use lambda factor to decide on the balance between these two components\nof the similarity score. I think it would be interesting to employ these for\nparaphrasing.\n\nAuthors support their motivation for the task with evaluation results. They\nshow that a system trained with the focus on differentiating between similar\ncaptions performs better than a system that is trained to generate captions\nonly. These are, however, showing that system that is tuned for a particular\ntask performs better on this task.\n\n- Weaknesses:\n\n It is not clear why image caption task is not suitable for comprehension task\nand why author\u2019s system is better for this. In order to argue that system can\ncomprehend image and sentence semantics better one should apply learned\nrepresentation, e.g. embeddings. E.g. apply representations lea",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task \n\nHere is the abstract of the paper: We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing ``keywords'' (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded ``understanding'' of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and well-studied metric: the accuracy in detecting the true target among the decoys.  The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a                    multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning. \n\nHere is the partial review: The paper proposes a task of selecting the most appropriate textual description\nfor a given scene/image from a list of similar options. It also proposes couple\nof baseline models, an evaluation metrics and human evaluation score. \n\n- Strengths:\n\nThe paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical\nevidence. So the paper is very easy to read. \n\nThe paper is well motivated. A method of selecting the most appropriate caption\ngiven a list of misleading candidates will benefit other\nimage-caption/understanding models, by acting as a post-generation re-ranking\nmethod. \n\n- Weaknesses:\n\nI am not sure if the proposed algorithm for decoys generation is effective,\nwhich as a consequence puts the paper on questions.\n\nFor each target caption, the algorithm basically picks out those with similar\nrepresentation and surface form but do not belong to the same image. But a\nfundamentally issue with this approach is: not belonging to the image-A does\nnot mean not appropriate to describe image-A, especially when the\nrepresentation and surface form are close. So the ground-truth labels might not\nbe valid. As we can see in Figure-1, the generated decoys are either too far\nfrom the target to be a *good* decoy (*giraffe* vs *elephant*), or fair\nsubstitutes for the target (*small boy playing kites* vs *boy flies a kite*).\n\nThus, I am afraid that the dataset generated with this algorithm can not train\na model to really *go beyond key word recognition*, which was claimed as\ncontribution in this paper. As shown in Figure-1, most\ndecoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs\nbread*, *frisbee vs kite*, etc. And when they can not be separated by *key word\nmatch*, they look very tempting to be a correct option.\n\nFurthermore, it is interesting that humans only do correctly on 82.8% on a\nsampled test set. Does it mean that those examples are really too hard even for\nhuman to correctly classify? Or are some of the *decoys* in fact good enough to\nbe the target's substitute (or even better) so that human choose them over\nground-truth targets?\n\n- General Discussion:\n\nI think this is a well-written paper with clear motivation and substantial\nexperiments. \nThe major issue is that the data-generating algorithm and the generated dataset\ndo not seem helpful for the motivation. This in turn makes the experimental\nconclusions less convincing. So I tend to reject this paper unless my concerns\ncan be fully addressed in rebuttal.\n- Strengths:\n\nAuthors generate a dataset of \u201crephrased\u201d captions and are planning to make\nthis dataset publicly available.\n\nThe way authors approached DMC task has an advantage over VQA or caption\ngeneration in terms of metrics. It is easier and more straightforward to\nevaluate problem of choosing the best caption. Authors use accuracy metric.\nWhile for instance caption generation requires metrics like BLUE or Meteor\nwhich are limited in handling semantic similarity.\n\nAuthors propose an interesting approach to \u201crephrasing\u201d, e.g. selecting\ndecoys. They draw decoys form image-caption dataset. E.g. decoys for a single\nimage come from captions for other images. These decoys however are similar to\neach other both in terms of surface (bleu score) and semantics (PV similarity).\nAuthors use lambda factor to decide on the balance between these two components\nof the similarity score. I think it would be interesting to employ these for\nparaphrasing.\n\nAuthors support their motivation for the task with evaluation results. They\nshow that a system trained with the focus on differentiating between similar\ncaptions performs better than a system that is trained to generate captions\nonly. These are, however, showing that system that is tuned for a particular\ntask performs better on this task.\n\n- Weaknesses:\n\n It is not clear why image caption task is not suitable for comprehension task\nand why author\u2019s system is better for this. In order to argue that system can\ncomprehend image and sentence semantics better one should apply learned\nrepresentation, e.g. embeddings. E.g. apply representations lea \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, the paper is well-written and well-structured, making it easy to read. The authors present a clear motivation for their proposed task and provide empirical evidence to support their contributions. However, there are some weaknesses in the paper that need to be addressed. The algorithm for generating decoys may not be effective, as the generated decoys may not truly go beyond keyword recognition. Additionally, the dataset generated with this algorithm may not train a model to achieve the intended goal. The high error rate in human evaluation raises questions about the validity of the ground-truth labels and the quality of the decoys. The paper would benefit from addressing these concerns and providing a more convincing experimental conclusion. The strengths of the paper include the generation of a publicly available dataset and the use of an easy-to-compute metric for evaluation. The approach to rephrasing is interesting and could have potential for further applications. However, it is not clear why the image caption task is not suitable for the comprehension task and why the authors' system is better. This can be further supported by applying learned representations like embeddings. Overall, the paper has potential but requires addressing the mentioned weaknesses.\"\n}",
        "human_end_index": 4125
    },
    {
        "id": 503,
        "source": "PeerRead/acl_2017/",
        "title": "Probabilistic Regular Graph Languages",
        "human_review": "This one is a tough call, because I do think that there are some\nimportant, salvageable technial results in here (notably the parsing\nalgorithm), but the paper as a whole has very little cohesion.        It is\nunited around an overarching view of formal languages in which a language\nbeing \"probabilistic\" or not is treated as a formal property of the same \nvariety as being closed under intersection or not.  In my opinion, what it \nmeans for a formal language to be probabilistic in this view has not been \nconsidered with sufficient rigor for this viewpoint to be compelling.\n\nI should note, by the way, that the value of the formal results provided\nmostly does not depend on the flimsiness of the overarching story.  So\nwhat we have here is not bad research, but a badly written paper.  This needs \nmore work.\n\nI find it particulary puzzling that the organization of the paper\nleaves so little space for elucidating the parsing result that\nsoundness and completeness are relegated to a continuation of the\npaper in the form of supplementary notes.  I also find the mention of\nprobabilistic languages in the title of the paper to be very\ndisingenuous --- there is in fact no probabilistic reasoning in this\nsubmission.\n\nThe sigificance of the intersection-closure result of section 3 is\nalso being somewhat overstated, I think.  Unless there is something\nI'm not understanding about the restrictions on the right-hand sides\nof rules (in which case, please elaborate), this is merely a matter of\nfolding a finite intersection into the set of non-terminal labels.\nThe paper is concerned in finding such a family of graph languages that is\nclosed under intersection and can be made probabilistic.\n\n- Strengths:\n\nThe introduction shows relevance, the overall aim, high level context and is\nnice to read.\nThe motivation is clear and interesting.\n\nThe paper  is extremely clear but requires close reading and much formal\nbackground.\nIt nicely takes into account certain differences in terminology.\n\nIt was interesting to see how the hyper-edge grammars generalize familiar\ngrammars \nand Earley's algorithm.  For example, Predict applies to nonterminal edges, and\nScan applies to terminal edges.  \n\nIf the parsing vs. validation in NLP context is clarified, the paper is useful\nbecause it is formally correct, nice contribution, instructive and can give new\nideas to other researchers.  \n\nThe described algorithm can be used in semantic parsing to rerank hypergraphs\nthat are produced by another parser.   In this restricted way, the method can\nbe part of the machinery what we in NLP use in natural language parsing and\nthus relevant to the ACL.\n\n- Weaknesses:\n\nReranking use is not mentioned in the introduction.\n\nIt would be a great news in NLP context if an Earley parser would run in linear\ntime for NLP grammars (unlike special kinds of formal language grammars). \nUnfortunately, this result involves deep assumptions about the grammar and the\nkind of input. \n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nTo me, the paper should be more clear in this as a random reader may miss the\ndifference between semantic parsing (from strings) and parsing of semantic\nparses \n(the current work).\n\nThere does not seem to be any control of the linear order of 0-arity edges.  It\nmight be useful to mention that if the parser is extended to string inputs with\nthe aim to find the (best?) hypergraph for a given external nodes, then the\nitem representations of the subgraphs must also keep track of the covered\n0-arity edges.                          This makes the string-parser variant\nexponential.  \n\n- Easily correctable typos or textual problems:\n\n1)  Lines 102-106 is misleading.   While intersection and probs are true, \"such\ndistribution\" cannot refer to the discussion in the above.\n\n2) line 173:  I think you should rather talk about validation or recognition\nalgorithms than parsing algorithms as \"parsing\" in NLP means usually completely\ndifferent thing that is much more challenging due to the lexical and structural\nambiguity.\n\n3) lines 195-196 are unclear:  what are the elements of att_G; in what sense\nthey are pairwise distinct.  Compare Example 1 where ext_G and att_G(e_1) are\nnot disjoint sets.\n\n4) l.206.  Move *rank* definition earlier and remove redundancy.\n\n5) l. 267:  rather \"immediately derives\", perhaps.\n\n6) 279: add \"be\"\n\n7) l. 352:  give an example of a nontrivial internal path.\n\n8) l. 472:   define a subgraph of a hypergraph\n\n9) l. 417, l.418:  since there are two propositions, you may want to tell how\nthey contribute to what is quoted.\n\n10) l. 458:  add \"for\"\n\nTable:                          Axiom:              this is only place where this is\nintroduced as an\naxiom.                    Link\nto the text that says it is a trigger.\n\n- General Discussion:\n\nIt might be useful to tell about MSOL graph languages and their yields, which\nare\ncontext-free string languages.                          \n\nWhat happens if the grammar is ambiguous and not top-down deterministic? \nWhat if there are exponential number of parses even for the input graph due to\nlexical ambiguity or some other reasons.  How would the parser behave then? \nWouldn't the given Earley recogniser actually be strictly polynomial to m or k\n?\n\nEven a synchronous derivation of semantic graphs can miss some linguistic\nphenomena where a semantic distinction is expressed by different linguistic\nmeans.                    E.g. one language may add an affix to a verb when another\nlanguage may\nexpress the same distinction by changing the object.  I am suggesting that\nalthough AMR increases language independence in parses it may have such\ncross-lingual\nchallenges.\n\nI did not fully understand the role of the marker in subgraphs.  It was elided\nlater\nand not really used.\n\nl. 509-510:                 I already started to miss the remark of lines 644-647\nat\nthis\npoint.\n\nIt seems that the normal order is not unique.  Can you confirm this?\n\nIt is nice that def 7, cond 1 introduces lexical anchors to predictions. \nCompare the anchors in lexicalized grammars.\n\nl. 760.  Are you sure that non-crossing links do not occur when parsing\nlinearized sentences to semantic graphs?\n\n- Significant questions to the Authors:\n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nWhat would you say about parsing complexity in the case the RGG is a\nnon-deterministic, possibly ambiguous regular tree grammar, but one is\ninterested to use it to assign trees to frontier strings like a context-free\ngrammar?  Can one adapt the given Earley algorithm to this purpose (by guessing\ninternal nodes and their edges)?\nAlthough this question might seem like a confusion, it is relevant in the NLP\ncontext.\n\nWhat prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are\nthen linearised?   What principle determines how they are linearised?               \n  Is\nthe\nlinear order determined by the Earley paths (and normal order used in\nproductions) or can one consider an actual word order in strings of a natural\nlanguage? \n\nThere is no clear connection to (non)context-free string languages or sets of\n(non)projective dependency graphs used in semantic parsing.  What is written on\nlines 757-758 is just misleading:  Lines 757-758 mention that HRGs can be used\nto generate non-context-free languages.  Are these graph languages or string\nlanguages?    How an NLP expert should interpret the (implicit) fact that RGGs\ngenerate only context-free languages?  Does this mean that the graphs are\nnoncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?",
        "partial_review": "This one is a tough call, because I do think that there are some\nimportant, salvageable technial results in here (notably the parsing\nalgorithm), but the paper as a whole has very little cohesion.        It is\nunited around an overarching view of formal languages in which a language\nbeing \"probabilistic\" or not is treated as a formal property of the same \nvariety as being closed under intersection or not.  In my opinion, what it \nmeans for a formal language to be probabilistic in this view has not been \nconsidered with sufficient rigor for this viewpoint to be compelling.\n\nI should note, by the way, that the value of the formal results provided\nmostly does not depend on the flimsiness of the overarching story.  So\nwhat we have here is not bad research, but a badly written paper.  This needs \nmore work.\n\nI find it particulary puzzling that the organization of the paper\nleaves so little space for elucidating the parsing result that\nsoundness and completeness are relegated to a continuation of the\npaper in the form of supplementary notes.  I also find the mention of\nprobabilistic languages in the title of the paper to be very\ndisingenuous --- there is in fact no probabilistic reasoning in this\nsubmission.\n\nThe sigificance of the intersection-closure result of section 3 is\nalso being somewhat overstated, I think.  Unless there is something\nI'm not understanding about the restrictions on the right-hand sides\nof rules (in which case, please elaborate), this is merely a matter of\nfolding a finite intersection into the set of non-terminal labels.\nThe paper is concerned in finding such a family of graph languages that is\nclosed under intersection and can be made probabilistic.\n\n- Strengths:\n\nThe introduction shows relevance, the overall aim, high level context and is\nnice to read.\nThe motivation is clear and interesting.\n\nThe paper  is extremely clear but requires close reading and much formal\nbackground.\nIt nicely takes into account certain differences in terminology.\n\nIt was interesting to see how the hyper-edge grammars generalize familiar\ngrammars \nand Earley's algorithm.  For example, Predict applies to nonterminal edges, and\nScan applies to terminal edges.  \n\nIf the parsing vs. validation in NLP context is clarified, the paper is useful\nbecause it is formally correct, nice contribution, instructive and can give new\nideas to other researchers.  \n\nThe described algorithm can be used in semantic parsing to rerank hypergraphs\nthat are produced by another parser.   In this restricted way, the method can\nbe part of the machinery what we in NLP use in natural language parsing and\nthus relevant to the ACL.\n\n- Weaknesses:\n\nReranking use is not mentioned in the introduction.\n\nIt would be a great news in NLP context if an Earley parser would run in linear\ntime for NLP grammars (unlike special kinds of formal language grammars). \nUnfortunately, this result involves deep assumptions about the grammar and the\nkind of input. \n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nTo me, the paper should be more clear in this as a random reader may miss the\ndifference between semantic parsing (from strings) and parsing of semantic\nparses \n(the current work).\n\nThere does not seem to be any control of the linear order of 0-arity edges.  It\nmight be useful to mention that if the parser is extended to string inputs with\nthe aim to find the (best?) hypergraph for a given external nodes, then the\nitem representations of the subgraphs must also keep track of the covered\n0-arity edges.                          This makes the string-parser variant\nexponential.  \n\n- Easily correctable typos or textual problems:\n\n1)  Lines 102-106 is misleading.   While ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Probabilistic Regular Graph Languages \n\nHere is the abstract of the paper: Distributions over strings and trees can be represented by probabilistic regular languages, which characterize many models in natural language processing. Recently, several datasets have become available which represent natural language phenomena as graphs, so it is natural to ask whether there is an equivalent of probabilistic regular languages for graphs. To answer this question, we review three families of graph languages: Hyperedge Replacement Languages (HRL), which can be made probabilistic; Monadic Second Order Languages (MSOL), which support the crucial property of closure under intersection; and Regular Graph Languages (RGL; \\citealt{Courcelle:V}), a subfamily of both HRL and MSOL which inherits these properties, and has not been widely studied or applied to NLP. We prove that RGLs are closed under intersection and provide an efficient parsing algorithm, with runtime linear in the size of the input graph. \n\nHere is the partial review: This one is a tough call, because I do think that there are some\nimportant, salvageable technial results in here (notably the parsing\nalgorithm), but the paper as a whole has very little cohesion.        It is\nunited around an overarching view of formal languages in which a language\nbeing \"probabilistic\" or not is treated as a formal property of the same \nvariety as being closed under intersection or not.  In my opinion, what it \nmeans for a formal language to be probabilistic in this view has not been \nconsidered with sufficient rigor for this viewpoint to be compelling.\n\nI should note, by the way, that the value of the formal results provided\nmostly does not depend on the flimsiness of the overarching story.  So\nwhat we have here is not bad research, but a badly written paper.  This needs \nmore work.\n\nI find it particulary puzzling that the organization of the paper\nleaves so little space for elucidating the parsing result that\nsoundness and completeness are relegated to a continuation of the\npaper in the form of supplementary notes.  I also find the mention of\nprobabilistic languages in the title of the paper to be very\ndisingenuous --- there is in fact no probabilistic reasoning in this\nsubmission.\n\nThe sigificance of the intersection-closure result of section 3 is\nalso being somewhat overstated, I think.  Unless there is something\nI'm not understanding about the restrictions on the right-hand sides\nof rules (in which case, please elaborate), this is merely a matter of\nfolding a finite intersection into the set of non-terminal labels.\nThe paper is concerned in finding such a family of graph languages that is\nclosed under intersection and can be made probabilistic.\n\n- Strengths:\n\nThe introduction shows relevance, the overall aim, high level context and is\nnice to read.\nThe motivation is clear and interesting.\n\nThe paper  is extremely clear but requires close reading and much formal\nbackground.\nIt nicely takes into account certain differences in terminology.\n\nIt was interesting to see how the hyper-edge grammars generalize familiar\ngrammars \nand Earley's algorithm.  For example, Predict applies to nonterminal edges, and\nScan applies to terminal edges.  \n\nIf the parsing vs. validation in NLP context is clarified, the paper is useful\nbecause it is formally correct, nice contribution, instructive and can give new\nideas to other researchers.  \n\nThe described algorithm can be used in semantic parsing to rerank hypergraphs\nthat are produced by another parser.   In this restricted way, the method can\nbe part of the machinery what we in NLP use in natural language parsing and\nthus relevant to the ACL.\n\n- Weaknesses:\n\nReranking use is not mentioned in the introduction.\n\nIt would be a great news in NLP context if an Earley parser would run in linear\ntime for NLP grammars (unlike special kinds of formal language grammars). \nUnfortunately, this result involves deep assumptions about the grammar and the\nkind of input. \n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nTo me, the paper should be more clear in this as a random reader may miss the\ndifference between semantic parsing (from strings) and parsing of semantic\nparses \n(the current work).\n\nThere does not seem to be any control of the linear order of 0-arity edges.  It\nmight be useful to mention that if the parser is extended to string inputs with\nthe aim to find the (best?) hypergraph for a given external nodes, then the\nitem representations of the subgraphs must also keep track of the covered\n0-arity edges.                          This makes the string-parser variant\nexponential.  \n\n- Easily correctable typos or textual problems:\n\n1)  Lines 102-106 is misleading.   While  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "Overall, this partial review provides a fair assessment of the strengths and weaknesses of the paper. The reviewer acknowledges the relevance and clear presentation of the paper but points out several weaknesses, such as the lack of clarity regarding the parsing vs. validation in NLP context and the assumptions made in the parsing complexity result. The reviewer also suggests mentioning the potential use of the algorithm in reranking and addressing the issue of controlling the linear order of 0-arity edges. Additionally, the reviewer points out a misleading section in the paper that needs correction. Overall, the review provides useful feedback for the authors to improve the clarity and depth of their paper.",
        "human_end_index": 4101
    },
    {
        "id": "516",
        "source": "PeerRead/acl_2017/",
        "title": "Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling",
        "human_review": "- Strengths:\n\nThe paper offers a natural and useful extension to recent efforts in\ninteractive topic modeling, namely by allowing human annotators to provide\nmultiple \"anchor words\" to machine-induced topics. The paper is well-organized\nand the combination of synthetic and user experiments make for a strong paper.\n\n- Weaknesses:\n\nThe paper is fairly limited in scope in terms of the interactive topic model\napproaches it compares against. I am willing to accept this, since they do make\nreference to most of them and explain that these other approaches are not\nnecessarily fast enough for interactive experimentation or not conducive to the\ntypes of interaction being considered with an \"anchoring\" interface. Some level\nof empirical support for these claims would have been nice, though.\n\nIt would also have been nice to see experiments on more than one data set (20\nnewsgroups, which is now sort of beaten-to-death).\n\n- General Discussion:\n\nIn general, this is a strong paper that appears to offer an incremental but\nnovel and practical contribution to interactive topic modeling. The authors\nmade the effort to vet several variants of the approach in simulated\nexperiments, and to conduct fairly exhaustive quantitative analyses of both\nsimulated and user experiments using a variety of metrics that measure\ndifferent facets of topic quality.\n- Strengths:\nClear description of methods and evaluation\nSuccessfully employs and interprets a variety of evaluations\nSolid demonstration of practicality of technique in real-world interactive\ntopic modeling\n\n- Weaknesses:\nMissing related work on anchor words\nEvaluation on 20 Newsgroups is not ideal\nTheoretical contribution itself is small \n\n- General Discussion:\nThe authors propose a new method of interactive user specification of topics\ncalled Tandem Anchors. The approach leverages the anchor words algorithm, a\nmatrix-factorization approach to learning topic models, by replacing the\nindividual anchors inferred from the Gram-Schmidt algorithm with constructed\nanchor pseudowords created by combining the sparse vector representations of\nmultiple words that for a topic facet. The authors determine that the use of a\nharmonic mean function to construct pseudowords is optimal by demonstrating\nthat classification accuracy of document-topic distribution vectors using these\nanchors produces the most improvement over Gram-Schmidt. They also demonstrate\nthat their work is faster than existing interactive methods, allowing\ninteractive iteration, and show in a user study that the multiword anchors are\neasier and more effective for users.\n\nGenerally, I like this contribution a lot: it is a straightforward modification\nof an existing algorithm that actually produces a sizable benefit in an\ninteractive setting. I appreciated the authors\u2019 efforts to evaluate their\nmethod on a variety of scales. While I think the technical contribution in\nitself is relatively small (a strategy to assemble pseudowords based on topic\nfacets) the thoroughness of the evaluation merited having it be a full paper\ninstead of a short paper. It would have been nice to see more ideas as to how\nto build these facets in the absence of convenient sources like category titles\nin 20 Newsgroups or when initializing a topic model for interactive learning.\n\nOne frustration I had with this paper is that I find evaluation on 20\nNewsgroups to not be great for topic modeling: the documents are widely\ndifferent lengths, preprocessing matters a lot, users have trouble making sense\nof many of the messages, and naive bag-of-words models beat topic models by a\nsubstantial margin. Classification tasks are useful shorthand for how well a\ntopic model corresponds to meaningful distinctions in the text by topic; a task\nlike classifying news articles by section or reviews by the class of the\nsubject of the review might be more appropriate. It would also have been nice\nto see a use case that better appealed to a common expressed application of\ntopic models, which is the exploration of a corpus.\n\nThere were a number of comparisons I think were missing, as the paper contains\nlittle reference to work since the original proposal of the anchor word model.\nIn addition to comparing against standard Gram-Schmidt, it would have been good\nto see the method from Lee et. al. (2014), \u201cLow-dimensional Embeddings for\nInterpretable Anchor-based Topic Inference\u201d. I also would have liked to have\nseen references to Nguyen et. al. (2013), \u201cEvaluating Regularized Anchor\nWords\u201d and Nguyen et. al. (2015) \u201cIs Your Anchor Going Up or Down? Fast and\nAccurate Supervised Topic Models\u201d, both of which provide useful insights into\nthe anchor selection process.\n\nI had some smaller notes:\n- 164: \u2026entire dataset\n- 164-166: I\u2019m not quite sure what you mean here. I think you are claiming\nthat it takes too long to do one pass? My assumption would have been you would\nuse only a subset of the data to retrain the model instead of a full sweep, so\nit would be good to clarify what you mean.\n- 261&272: any reason you did not consider the and operator or element-wise\nmax? They seem to correspond to the ideas of union and intersection from the or\noperator and element-wise min, and it wasn\u2019t clear to me why the ones you\nchose were better options.\n- 337: Usenet should be capitalized\n- 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also,\ndid you remove headers, footers, and/or quotes from the messages?\n- 436-440: I would have liked to see a bit more explanation of what this tells\nus about confusion.\n- 692: using tandem anchors\n\nOverall, I think this paper is a meaningful contribution to interactive topic\nmodeling that I would like to see available for people outside the machine\nlearning community to investigate, classify, and test hypotheses about their\ncorpora.\n\nPOST-RESPONSE: I appreciate the thoughtful responses of the authors to my\nquestions. I would maintain that for some of the complimentary related work\nthat it's useful to compare to non-interactive work, even if it does something\ndifferent.",
        "partial_review": "- Strengths:\n\nThe paper offers a natural and useful extension to recent efforts in\ninteractive topic modeling, namely by allowing human annotators to provide\nmultiple \"anchor words\" to machine-induced topics. The paper is well-organized\nand the combination of synthetic and user experiments make for a strong paper.\n\n- Weaknesses:\n\nThe paper is fairly limited in scope in terms of the interactive topic model\napproaches it compares against. I am willing to accept this, since they do make\nreference to most of them and explain that these other approaches are not\nnecessarily fast enough for interactive experimentation or not conducive to the\ntypes of interaction being considered with an \"anchoring\" interface. Some level\nof empirical support for these claims would have been nice, though.\n\nIt would also have been nice to see experiments on more than one data set (20\nnewsgroups, which is now sort of beaten-to-death).\n\n- General Discussion:\n\nIn general, this is a strong paper that appears to offer an incremental but\nnovel and practical contribution to interactive topic modeling. The authors\nmade the effort to vet several variants of the approach in simulated\nexperiments, and to conduct fairly exhaustive quantitative analyses of both\nsimulated and user experiments using a variety of metrics that measure\ndifferent facets of topic quality.\n- Strengths:\nClear description of methods and evaluation\nSuccessfully employs and interprets a variety of evaluations\nSolid demonstration of practicality of technique in real-world interactive\ntopic modeling\n\n- Weaknesses:\nMissing related work on anchor words\nEvaluation on 20 Newsgroups is not ideal\nTheoretical contribution itself is small \n\n- General Discussion:\nThe authors propose a new method of interactive user specification of topics\ncalled Tandem Anchors. The approach leverages the anchor words algorithm, a\nmatrix-factorization approach to learning topic models, by replacing the\nindividual anchors inferred from the Gram-Schmidt algorithm with constructed\nanchor pseudowords created by combining the sparse vector representations of\nmultiple words that for a topic facet. The authors determine that the use of a\nharmonic mean function to construct pseudowords is optimal by demonstrating\nthat classification accuracy of document-topic distribution vectors using these\nanchors produces the most improvement over Gram-Schmidt. They also demonstrate\nthat their work is faster than existing interactive methods, allowing\ninteractive iteration, and show in a user study that the multiword anchors are\neasier and more effective for users.\n\nGenerally, I like this contribution a lot: it is a straightforward modification\nof an existing algorithm that actually produces a sizable benefit in an\ninteractive setting. I appreciated the authors\u2019 efforts to evaluate their\nmethod on a variety of scales. While I think the technical contribution in\nitself is relatively small (a strategy to assemble pseudowords based on topic\nfacets) the thoroughness of the evaluation merited having it be a full paper\ninstead",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling \n\nHere is the abstract of the paper: Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, go- ing beyond existing single word anchor algorithms\u2014an approach we call \u201cTan- dem Anchors\u201d. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interac- tive and non-interactive approaches. Tan- dem anchors are faster and more intuitive than existing interactive approaches. \n\nHere is the partial review: - Strengths:\n\nThe paper offers a natural and useful extension to recent efforts in\ninteractive topic modeling, namely by allowing human annotators to provide\nmultiple \"anchor words\" to machine-induced topics. The paper is well-organized\nand the combination of synthetic and user experiments make for a strong paper.\n\n- Weaknesses:\n\nThe paper is fairly limited in scope in terms of the interactive topic model\napproaches it compares against. I am willing to accept this, since they do make\nreference to most of them and explain that these other approaches are not\nnecessarily fast enough for interactive experimentation or not conducive to the\ntypes of interaction being considered with an \"anchoring\" interface. Some level\nof empirical support for these claims would have been nice, though.\n\nIt would also have been nice to see experiments on more than one data set (20\nnewsgroups, which is now sort of beaten-to-death).\n\n- General Discussion:\n\nIn general, this is a strong paper that appears to offer an incremental but\nnovel and practical contribution to interactive topic modeling. The authors\nmade the effort to vet several variants of the approach in simulated\nexperiments, and to conduct fairly exhaustive quantitative analyses of both\nsimulated and user experiments using a variety of metrics that measure\ndifferent facets of topic quality.\n- Strengths:\nClear description of methods and evaluation\nSuccessfully employs and interprets a variety of evaluations\nSolid demonstration of practicality of technique in real-world interactive\ntopic modeling\n\n- Weaknesses:\nMissing related work on anchor words\nEvaluation on 20 Newsgroups is not ideal\nTheoretical contribution itself is small \n\n- General Discussion:\nThe authors propose a new method of interactive user specification of topics\ncalled Tandem Anchors. The approach leverages the anchor words algorithm, a\nmatrix-factorization approach to learning topic models, by replacing the\nindividual anchors inferred from the Gram-Schmidt algorithm with constructed\nanchor pseudowords created by combining the sparse vector representations of\nmultiple words that for a topic facet. The authors determine that the use of a\nharmonic mean function to construct pseudowords is optimal by demonstrating\nthat classification accuracy of document-topic distribution vectors using these\nanchors produces the most improvement over Gram-Schmidt. They also demonstrate\nthat their work is faster than existing interactive methods, allowing\ninteractive iteration, and show in a user study that the multiword anchors are\neasier and more effective for users.\n\nGenerally, I like this contribution a lot: it is a straightforward modification\nof an existing algorithm that actually produces a sizable benefit in an\ninteractive setting. I appreciated the authors\u2019 efforts to evaluate their\nmethod on a variety of scales. While I think the technical contribution in\nitself is relatively small (a strategy to assemble pseudowords based on topic\nfacets) the thoroughness of the evaluation merited having it be a full paper\ninstead \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper offers a natural and useful extension to interactive topic modeling by introducing the concept of tandem anchors. The authors effectively explain and evaluate their approach through synthetic investigations and a user study. The paper is well-organized and the evaluation metrics used are comprehensive. However, the scope of the paper is limited in terms of the interactive topic model approaches it compares against. Additionally, the lack of related work on anchor words and the choice of evaluation on the 20 Newsgroups dataset could have been improved. Nevertheless, the authors' contribution in proposing tandem anchors as a more intuitive and efficient approach for interactive topic modeling is valuable. The technical contribution itself may be relatively small, but the thoroughness of the evaluation justifies its presentation as a full paper.\"\n}",
        "human_end_index": 3058
    },
    {
        "id": 520,
        "source": "PeerRead/acl_2017/",
        "title": "ShapeWorld: A new test methodology for multimodal language understanding",
        "human_review": "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as corresponding logical forms and language\ndescriptions.\nThe goal seems to be to have a method where the complexity of pictures and\ncorresponding desciptions can be controlled and parametrized. \n\n - The biggest downside seems to be that the maximally achievable complexity is\nvery limited, and way below the complexity typically faced with\nimage-captioning and other multimodal tasks. \n - The relative simplicity is also a big difference to the referenced bAbI\ntasks (which cover the whole qualitative spectrum of easy-to-hard reasoning\ntasks), whereas in the proposed method a (qualitatively) easy image reconition\ntask can only be quantitatively made harder, by increasing the number of\nobjects, noise etc in unnatural ways.\n - This is also reflected in the experimental section. Whenever the\nexperimental performance results are not satisfying, these cases seem like\nbasic over/underfitting issues that may easily be tackled by\nrestricting/extending the capacity of the networks or using more data. It is\nhard for me to spot any other qualitative insight.\n - In the introduction it is stated that the \"goal is not too achieve optimal\nperformance\" but to find out whether \"architectures are able to successfully\ndemonstrate the desired understanding\" - there is a fundamental contradiction\nhere, in that the proposed task on the one side is meant to provide a measure\nas to whether architectures demontrate \"understanding\", on the other hand the\nscore is not supposed to be taken as meaningful/seriously.\n\nGeneral comments:\nThe general approach should be made more tangible earlier (i.e. in the\nintroction rather than in section 3)\n- Strengths:\n\nThe authors introduce a new software package called ShapeWorld for\nautomatically generating data for image captioning problems. The microworld\nused to generate the image captions is simple enough to make the data being\ngenerated and errors by a model readily interpretable. However, the authors\ndemonstrate that configurations of the packages produce data that is\nchallenging enough to serve as a good benchmark for ongoing research.\n\n- Weaknesses:\n\nThe primary weakness of this paper is that it does look a bit like a demo\npaper. The authors do provides experiments that evaluate a reasonable baseline\nimage captioning system on the data generated by ShapeWorld. However, similar\nexperiments are included in demo papers.\n\nThe paper includes a hyperlink to the software package on github that\npresumably unmasks the authors of the paper.\n\n- General Discussion:\n\nScientific progress often involves some something analogous to vygotsky's zone\nof proximal development, whereby progress can be made more quickly if research\nfocuses on problems with just the right level of difficulty (e.g., the use of\ntidigits for speech recognition research in the early 90s). This paper is\nexciting since it offers a simple microworld that is easy for researchers to\ncompletely comprehend but that also is just difficult enough for existing\nmodels.\n\nThe strengths of the work are multiplied by the fact that the software is\nopensource, is readily available on github and generates the data in a format\nthat can be easily used with models built using modern deep learning libraries\n(e.g., TensorFlow).\n\nThe methods used by the software package to generate the artificial data are\nclearly explained. It is also great that the authors did experiments with\ndifferent configurations of their software and a baseline image caption model\nin order to demonstrate the strengths and weakness of existing techniques. \n\nMy only real concern with this paper is whether the community would be better\nserved by placing it in the demo section. Publishing it in the non-demo long\npaper track might cause confusion as well as be unfair to authors who correctly\nsubmitted similar papers to the ACL demo track.",
        "partial_review": "This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as corresponding logical forms and language\ndescriptions.\nThe goal seems to be to have a method where the complexity of pictures and\ncorresponding desciptions can be controlled and parametrized. \n\n - The biggest downside seems to be that the maximally achievable complexity is\nvery limited, and way below the complexity typically faced with\nimage-captioning and other multimodal tasks. \n - The relative simplicity is also a big difference to the referenced bAbI\ntasks (which cover the whole qualitative spectrum of easy-to-hard reasoning\ntasks), whereas in the proposed method a (qualitatively) easy image reconition\ntask can only be quantitatively made harder, by increasing the number of\nobjects, noise etc in unnatural ways.\n - This is also reflected in the experimental section. Whenever the\nexperimental performance results are not satisfying, these cases seem like\nbasic over/underfitting issues that may easily be tackled by\nrestricting/extending the capacity of the networks or using more data. It is\nhard for me to spot any other qualitative insight.\n - In the introduction it is stated that the \"goal is not too achieve optimal\nperformance\" but to find out whether \"architectures are able to successfully\ndemonstrate the desired understanding\" - there is a fundamental contradiction\nhere, in that the proposed task on the one side is meant to provide a measure\nas to whether architectures demontrate \"understanding\", on the other hand the\nscore is not supposed to be taken as meaningful/seriously.\n\nGeneral comments:\nThe general approach should be made more tangible earlier (i.e. in the\nintroction rather than in section 3)\n- Strengths:\n\nThe authors introduce a new software package called ShapeWorld for\nautomatically generating data for image captioning problems. The microworld\nused to generate the image captions is simple enough to make the data being\ngenerated and errors by a model read",
        "task3_prompt": "\nComplete a partially written peer review about the paper: ShapeWorld: A new test methodology for multimodal language understanding \n\nHere is the abstract of the paper: We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter's specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created which require generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating a multimodal architecture on four different tasks, and show that our framework gives us insights into the model's capabilities and limitations. \n\nHere is the partial review: This paper proposes a method for generating datasets of pictures from simple\nbuilding blocks, as well as corresponding logical forms and language\ndescriptions.\nThe goal seems to be to have a method where the complexity of pictures and\ncorresponding desciptions can be controlled and parametrized. \n\n - The biggest downside seems to be that the maximally achievable complexity is\nvery limited, and way below the complexity typically faced with\nimage-captioning and other multimodal tasks. \n - The relative simplicity is also a big difference to the referenced bAbI\ntasks (which cover the whole qualitative spectrum of easy-to-hard reasoning\ntasks), whereas in the proposed method a (qualitatively) easy image reconition\ntask can only be quantitatively made harder, by increasing the number of\nobjects, noise etc in unnatural ways.\n - This is also reflected in the experimental section. Whenever the\nexperimental performance results are not satisfying, these cases seem like\nbasic over/underfitting issues that may easily be tackled by\nrestricting/extending the capacity of the networks or using more data. It is\nhard for me to spot any other qualitative insight.\n - In the introduction it is stated that the \"goal is not too achieve optimal\nperformance\" but to find out whether \"architectures are able to successfully\ndemonstrate the desired understanding\" - there is a fundamental contradiction\nhere, in that the proposed task on the one side is meant to provide a measure\nas to whether architectures demontrate \"understanding\", on the other hand the\nscore is not supposed to be taken as meaningful/seriously.\n\nGeneral comments:\nThe general approach should be made more tangible earlier (i.e. in the\nintroction rather than in section 3)\n- Strengths:\n\nThe authors introduce a new software package called ShapeWorld for\nautomatically generating data for image captioning problems. The microworld\nused to generate the image captions is simple enough to make the data being\ngenerated and errors by a model read \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper proposes a method for generating datasets of pictures from simple building blocks, as well as corresponding logical forms and language descriptions. The goal seems to be to have a method where the complexity of pictures and corresponding descriptions can be controlled and parametrized. However, the complexity achievable with this method is limited, which is a major downside. The proposed method is also different from the referenced bAbI tasks, as it only allows for quantitatively making easy image recognition tasks harder, rather than providing a qualitative spectrum of reasoning tasks. The experimental section mainly focuses on addressing over/underfitting issues rather than providing qualitative insights. Additionally, there is a contradiction in the goal stated in the introduction regarding measuring architecture understanding. On a positive note, the authors introduce a new software package called ShapeWorld for generating data for image captioning problems, and the microworld used is simple enough to make the data and model errors more readable. Overall, the general approach should be made more tangible earlier in the introduction.\"\n}",
        "human_end_index": 2006
    },
    {
        "id": 524,
        "source": "PeerRead/acl_2017/",
        "title": "A Comparison of Robust Parsing Methods for HPSG",
        "human_review": "- Strengths:\n * Elaborate evaluation data creation and evaluation scheme.\n * Range of compared techniques: baseline/simple/complex\n\n- Weaknesses:\n * No in-depth analysis beyond overall evaluation results.\n\n- General Discussion:\nThis paper compares several techniques for robust HPSG parsing.\n\nSince the main contribution of the paper is not a novel parsing technique but\nthe empirical evaluation, I would like to see a more in-depth analysis of the\nresults summarized in Table 1 and 2.\nIt would be nice to show some representative example sentences and sketches of\nits analyses, on which the compared methods behaved differently.\n\nPlease add EDM precision and recall figures to Table 2.\nThe EDM F1 score is a result of a mixed effects of (overall and partial)\ncoverage, parse ranking, efficiency of search, etc.\nThe overall coverage figures in Table 1 are helpful but addition of EDM recall\nto Table 2 would make the situations clearer.\n\nMinor comment:\n- Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?\n- Strengths:\n\nWell-written.\n\n- Weaknesses:\n\nAlthough the title and abstract of the paper suggest that robust parsing\nmethods for HPSG are being compared, the actual comparison is limited to only a\nfew techniques applied to a single grammar, the ERG (where in the past the \nchoice has been made to create a treebank for only those sentences that are in\nthe coverage of the grammar). Since the ERG is quite idiosyncratic in this\nrespect, I fear that the paper is not interesting for researchers working in\nother precision grammar frameworks.\n\nThe paper lacks comparison with robustness techniques that are routinely\napplied for systems based on other precision grammars such as various systems\nbased on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is\nprobably more. In the same spirit, there is a reference for supertagging to\nDridan 2013 which is about supertagging for ERG whereas supertagging for other\nprecision grammar systems has been proposed at least a decade earlier.\n\nThe paper lacks enough detail to make the results replicable. Not only are\nvarious details not spelled out (e.g. what are those limits on resource\nallocation), but perhaps more importantly, for some of the techniques that are\nbeing compared (eg the robust unification), and for the actual evaluation\nmetric, the paper refers to another paper that is still in preparation.\n\nThe actual results of the various techniques are somewhat disappointing. With\nthe exception of the csaw-tb method, the resulting parsing speed is extreme -\nsometimes much slower than the baseline method - where the baseline method is a\nmethod in which the standard resource limitations do not apply. The csaw-tb\nmethod is faster but not very accurate, and in any case it is not a method\nintroduced in this paper but an existing PCFG approximation technique.\n\nIt would be (more) interesting to have an idea of the results on a\nrepresentative dataset (consisting of both sentences that are in the coverage\nof the grammar and those that are not). In that case, a comparison with the\n\"real\" baseline system (ERG with standard settings) could be obtained.\n\nMethodological issue: the datasets semcor and wsj00ab consist of sentences\nwhich an older version of ERG could not parse, but a newer version could. For\nthis reason, the problems in these two datasets are clearly very much biased.\nIt is no suprise therefore that the various techniques obtain much better\nresults on those datasets. But to this reviewer, those results are somewhat\nmeaningless. \n\nminor:\n\nEDM is used before explained\n\n\"reverseability\"\n\n- General Discussion:\n- Strengths:\n\n- technique for creating dataset for evaluation of out-of-coverage items, that\ncould possibly be used to evaluation other grammars as well. \n- the writing in this paper is engaging, and clear (a pleasant surprise, as\ncompared to the typical ACL publication.)\n\n- Weaknesses:\n- The evaluation datasets used are small and hence results are not very\nconvincing (particularly wrt to the alchemy45 dataset on which the best results\nhave been obtained)\n- It is disappointing to see only F1 scores and coverage scores, but virtually\nno deeper analysis of the results. For instance, a breakdown by type of\nerror/type of grammatical construction would be interesting. \n- it is still not clear to this reviewer what is the proportion of out of\ncoverage items due to various factors (running out of resources,  lack of\ncoverage for \"genuine\" grammatical constructions in the long tail, lack of\ncoverage due to extra-grammatical factors like interjections, disfluencies,\nlack of lexical coverage, etc. \n\n- General Discussion:\n\nThis paper address the problem of \"robustness\" or lack of coverage for a\nhand-written HPSG grammar (English Resource Grammar). The paper compares\nseveral approaches for increasing coverage, and also presents two creative ways\nof obtaining evaluation datasets (a non-trivial issue due to the fact that gold\nstandard evaluation data is by definition available only for in-coverage\ninputs). \n\nAlthough hand-written precision grammars have been very much out of fashion\nfor a long time now and have been superseded by statistical treebank-based\ngrammars, it is important to continue research on these in my opinion. The\nadvantages of high precision and deep semantic analysis provided by these\ngrammars has not been\nreproduced by non-handwritten grammars as yet. For this reason, I am giving\nthis paper a score of 4, despite the shortcomings mentioned above.",
        "partial_review": "- Strengths:\n * Elaborate evaluation data creation and evaluation scheme.\n * Range of compared techniques: baseline/simple/complex\n\n- Weaknesses:\n * No in-depth analysis beyond overall evaluation results.\n\n- General Discussion:\nThis paper compares several techniques for robust HPSG parsing.\n\nSince the main contribution of the paper is not a novel parsing technique but\nthe empirical evaluation, I would like to see a more in-depth analysis of the\nresults summarized in Table 1 and 2.\nIt would be nice to show some representative example sentences and sketches of\nits analyses, on which the compared methods behaved differently.\n\nPlease add EDM precision and recall figures to Table 2.\nThe EDM F1 score is a result of a mixed effects of (overall and partial)\ncoverage, parse ranking, efficiency of search, etc.\nThe overall coverage figures in Table 1 are helpful but addition of EDM recall\nto Table 2 would make the situations clearer.\n\nMinor comment:\n- Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?\n- Strengths:\n\nWell-written.\n\n- Weaknesses:\n\nAlthough the title and abstract of the paper suggest that robust parsing\nmethods for HPSG are being compared, the actual comparison is limited to only a\nfew techniques applied to a single grammar, the ERG (where in the past the \nchoice has been made to create a treebank for only those sentences that are in\nthe coverage of the grammar). Since the ERG is quite idiosyncratic in this\nrespect, I fear that the paper is not interesting for researchers working in\nother precision grammar frameworks.\n\nThe paper lacks comparison with robustness techniques that are routinely\napplied for systems based on other precision grammars such as various systems\nbased on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is\nprobably more. In the same spirit, there is a reference for supertagging to\nDridan 2013 which is about supertagging for ERG whereas supertagging for other\nprecision grammar systems has been proposed at least a decade earlier.\n\nThe paper lacks enough detail to make the results replicable. Not only are\nvarious details not spelled out (e.g. what are those limits on resource\nallocation), but perhaps more importantly, for some of the techniques that are\nbeing compared (eg the robust unification), and for the actual evaluation\nmetric, the paper refers to another paper that is still in preparation.\n\nThe actual results of the various techniques are somewhat disappointing. With\nthe exception of the csaw-tb method, the resulting parsing speed is extreme -\nsometimes much slower than the baseline method - where the baseline method is a\nmethod in which the standard resource limitations do not apply. The csaw-tb\nmethod is faster but not very accurate, and in any case it is not a method\nintroduced in this paper but an existing P",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A Comparison of Robust Parsing Methods for HPSG \n\nHere is the abstract of the paper: This paper explores several techniques for enhancing coverage when parsing with HPSG grammars, determines appropriate evaluation methods, and uses them to compare performance.  Depending on the dataset, baseline coverage gaps can be reduced by between 75% and 100%, while simultaneously improving EDM F1 scores. \n\nHere is the partial review: - Strengths:\n * Elaborate evaluation data creation and evaluation scheme.\n * Range of compared techniques: baseline/simple/complex\n\n- Weaknesses:\n * No in-depth analysis beyond overall evaluation results.\n\n- General Discussion:\nThis paper compares several techniques for robust HPSG parsing.\n\nSince the main contribution of the paper is not a novel parsing technique but\nthe empirical evaluation, I would like to see a more in-depth analysis of the\nresults summarized in Table 1 and 2.\nIt would be nice to show some representative example sentences and sketches of\nits analyses, on which the compared methods behaved differently.\n\nPlease add EDM precision and recall figures to Table 2.\nThe EDM F1 score is a result of a mixed effects of (overall and partial)\ncoverage, parse ranking, efficiency of search, etc.\nThe overall coverage figures in Table 1 are helpful but addition of EDM recall\nto Table 2 would make the situations clearer.\n\nMinor comment:\n- Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?\n- Strengths:\n\nWell-written.\n\n- Weaknesses:\n\nAlthough the title and abstract of the paper suggest that robust parsing\nmethods for HPSG are being compared, the actual comparison is limited to only a\nfew techniques applied to a single grammar, the ERG (where in the past the \nchoice has been made to create a treebank for only those sentences that are in\nthe coverage of the grammar). Since the ERG is quite idiosyncratic in this\nrespect, I fear that the paper is not interesting for researchers working in\nother precision grammar frameworks.\n\nThe paper lacks comparison with robustness techniques that are routinely\napplied for systems based on other precision grammars such as various systems\nbased on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is\nprobably more. In the same spirit, there is a reference for supertagging to\nDridan 2013 which is about supertagging for ERG whereas supertagging for other\nprecision grammar systems has been proposed at least a decade earlier.\n\nThe paper lacks enough detail to make the results replicable. Not only are\nvarious details not spelled out (e.g. what are those limits on resource\nallocation), but perhaps more importantly, for some of the techniques that are\nbeing compared (eg the robust unification), and for the actual evaluation\nmetric, the paper refers to another paper that is still in preparation.\n\nThe actual results of the various techniques are somewhat disappointing. With\nthe exception of the csaw-tb method, the resulting parsing speed is extreme -\nsometimes much slower than the baseline method - where the baseline method is a\nmethod in which the standard resource limitations do not apply. The csaw-tb\nmethod is faster but not very accurate, and in any case it is not a method\nintroduced in this paper but an existing P \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper compares several techniques for robust HPSG parsing. The main contribution of the paper is the empirical evaluation, but the in-depth analysis of the results is lacking. It would be beneficial to include representative example sentences and sketches to show how the compared methods behaved differently. Additionally, the inclusion of EDM precision and recall figures in Table 2 would provide a clearer understanding of the results. The paper is well-written, but limited in its comparison to only a few techniques applied to the ERG grammar. It does not provide comparisons with other robustness techniques used in different precision grammar frameworks. The lack of detail to make the results replicable and reference to a paper still in preparation for some techniques and evaluation metrics is a drawback. The parsing speed of the techniques, excluding csaw-tb, is slower than the baseline method and csaw-tb does not offer high accuracy. Overall, the results of the compared techniques are somewhat disappointing. \",\n    \n}",
        "human_end_index": 2826
    },
    {
        "id": "543",
        "source": "PeerRead/acl_2017/",
        "title": "Learning Character-level Compositionality with Visual Features",
        "human_review": "- Update after rebuttal\n\nI appreciate the authors taking the time to clarify their implementation of the\nbaseline and to provide some evidence of the significance of the improvements\nthey report. These clarifications should definitely be included in the\ncamera-ready version. I very much like the idea of using visual features for\nthese languages, and I am looking forward to seeing how they help more\ndifficult tasks in future work.\n\n- Strengths:\n\n- Thinking about Chinese/Japanese/Korean characters visually is a great idea!\n\n- Weaknesses:\n\n- Experimental results show only incremental improvement over baseline, and the\nchoice of evaluation makes it hard to verify one of the central arguments: that\nvisual features improve performance when processing rare/unseen words.\n\n- Some details about the baseline are missing, which makes it difficult to\ninterpret the results, and would make it hard to reproduce the work.\n\n- General Discussion:\n\nThe paper proposes the use of computer vision techniques (CNNs applied to\nimages of text) to improve language processing for Chinese, Japanese, and\nKorean, languages in which characters themselves might be compositional. The\nauthors evaluate their model on a simple text-classification task (assigning\nWikipedia page titles to categories). They show that a simple one-hot\nrepresentation of the characters outperforms the CNN-based representations, but\nthat the combination of the visual representations with standard one-hot\nencodings performs better than the visual or the one-hot alone. They also\npresent some evidence that the visual features outperform the one-hot encoding\non rare words, and present some intuitive qualitative results suggesting the\nCNN learns good semantic embeddings of the characters.\n\nI think the idea of processing languages like Chinese and Japanese visually is\na great one, and the motivation for this paper makes a lot of sense. However, I\nam not entirely convinced by the experimental results. The evaluations are\nquite weak, and it is hard to say whether these results are robust or simply\ncoincidental. I would prefer to see some more rigorous evaluation to make the\npaper publication-ready. If the results are statistically significant (if the\nauthors can indicate this in the author response), I would support accepting\nthe paper, but ideally, I would prefer to see a different evaluation entirely.\n\nMore specific comments below:\n\n- In Section 3, paragraph \"lookup model\", you never explicitly say which\nembeddings you use, or whether they are tuned via backprop the way the visual\nembeddings are. You should be more clear about how the baseline was\nimplemented. If the baseline was not tuned in a task-specific way, but the\nvisual embeddings were, this is even more concerning since it makes the\nperformances substantially less comparable.\n\n- I don't entirely understand why you chose to evaluate on classifying\nwikipedia page titles. It seems that the only real argument for using the\nvisual model is its ability to generalize to rare/unseen characters. Why not\nfocus on this task directly? E.g. what about evaluating on machine translation\nof OOV words? I agree with you that some languages should be conceptualized\nvisually, and sub-character composition is important, but the evaluation you\nuse does not highlight weaknesses of the standard approach, and so it does not\nmake a good case for why we need the visual features. \n\n- In Table 5, are these improvements statistically significant?\n\n- It might be my fault, but I found Figure 4 very difficult to understand.\nSince this is one of your main results, you probably want to present it more\nclearly, so that the contribution of your model is very obvious. As I\nunderstand it, \"rank\" on the x axis is a measure of how rare the word is (I\nthink log frequency?), with the rarest word furthest to the left? And since the\nvisual model intersects the x axis to the left of the lookup model, this means\nthe visual model was \"better\" at ranking rare words? Why don't both models\nintersect at the same point on the x axis, aren't they being evaluated on the\nsame set of titles and trained with the same data? In the author response, it\nwould be helpful if you could summarize the information this figure is supposed\nto show, in a more concise way. \n\n- On the fallback fusion, why not show performance for for different\nthresholds? 0 seems to be an edge-case threshold that might not be\nrepresentative of the technique more generally.\n\n- The simple/traditional experiment for unseen characters is a nice idea, but\nis presented as an afterthought. I would have liked to see more eval in this\ndirection, i.e. on classifying unseen words\n\n- Maybe add translations to Figure 6, for people who do not speak Chinese?",
        "partial_review": "- Update after rebuttal\n\nI appreciate the authors taking the time to clarify their implementation of the\nbaseline and to provide some evidence of the significance of the improvements\nthey report. These clarifications should definitely be included in the\ncamera-ready version. I very much like the idea of using visual features for\nthese languages, and I am looking forward to seeing how they help more\ndifficult tasks in future work.\n\n- Strengths:\n\n- Thinking about Chinese/Japanese/Korean characters visually is a great idea!\n\n- Weaknesses:\n\n- Experimental results show only incremental improvement over baseline, and the\nchoice of evaluation makes it hard to verify one of the central arguments: that\nvisual features improve performance when processing rare/unseen words.\n\n- Some details about the baseline are missing, which makes it difficult to\ninterpret the results, and would make it hard to reproduce the work.\n\n- General Discussion:\n\nThe paper proposes the use of computer vision techniques (CNNs applied to\nimages of text) to improve language processing for Chinese, Japanese, and\nKorean, languages in which characters themselves might be compositional. The\nauthors evaluate their model on a simple text-classification task (assigning\nWikipedia page titles to categories). They show that a simple one-hot\nrepresentation of the characters outperforms the CNN-based representations, but\nthat the combination of the visual representations with standard one-hot\nencodings performs better than the visual or the one-hot alone. They also\npresent some evidence that the visual features outperform the one-hot encoding\non rare words, and present some intuitive qualitative results suggesting the\nCNN learns good semantic embeddings of the characters.\n\nI think the idea of processing languages like Chinese and Japanese visually is\na great one, and the motivation for this paper makes a lot of sense. However, I\nam not entirely convinced by the experimental results. The evaluations are\nquite weak, and it is hard to say whether these results are robust or simply\ncoincidental. I would prefer to see some more rigorous evaluation to make the\npaper publication-ready. If the results are statistically significant (if the\nauthors can indicate this in the author response), I would support accepting\nthe paper, but ideally, I would prefer to see a different e",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Learning Character-level Compositionality with Visual Features \n\nHere is the abstract of the paper: Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space. \n\nHere is the partial review: - Update after rebuttal\n\nI appreciate the authors taking the time to clarify their implementation of the\nbaseline and to provide some evidence of the significance of the improvements\nthey report. These clarifications should definitely be included in the\ncamera-ready version. I very much like the idea of using visual features for\nthese languages, and I am looking forward to seeing how they help more\ndifficult tasks in future work.\n\n- Strengths:\n\n- Thinking about Chinese/Japanese/Korean characters visually is a great idea!\n\n- Weaknesses:\n\n- Experimental results show only incremental improvement over baseline, and the\nchoice of evaluation makes it hard to verify one of the central arguments: that\nvisual features improve performance when processing rare/unseen words.\n\n- Some details about the baseline are missing, which makes it difficult to\ninterpret the results, and would make it hard to reproduce the work.\n\n- General Discussion:\n\nThe paper proposes the use of computer vision techniques (CNNs applied to\nimages of text) to improve language processing for Chinese, Japanese, and\nKorean, languages in which characters themselves might be compositional. The\nauthors evaluate their model on a simple text-classification task (assigning\nWikipedia page titles to categories). They show that a simple one-hot\nrepresentation of the characters outperforms the CNN-based representations, but\nthat the combination of the visual representations with standard one-hot\nencodings performs better than the visual or the one-hot alone. They also\npresent some evidence that the visual features outperform the one-hot encoding\non rare words, and present some intuitive qualitative results suggesting the\nCNN learns good semantic embeddings of the characters.\n\nI think the idea of processing languages like Chinese and Japanese visually is\na great one, and the motivation for this paper makes a lot of sense. However, I\nam not entirely convinced by the experimental results. The evaluations are\nquite weak, and it is hard to say whether these results are robust or simply\ncoincidental. I would prefer to see some more rigorous evaluation to make the\npaper publication-ready. If the results are statistically significant (if the\nauthors can indicate this in the author response), I would support accepting\nthe paper, but ideally, I would prefer to see a different e \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"I appreciate the authors taking the time to clarify their implementation of the baseline and to provide some evidence of the significance of the improvements they report. These clarifications should definitely be included in the camera-ready version. I very much like the idea of using visual features for these languages, and I am looking forward to seeing how they help more difficult tasks in future work.\\n\\n\\nStrengths:\\n\\n- Thinking about Chinese/Japanese/Korean characters visually is a great idea!\\n\\n\\nWeaknesses:\\n\\n- Experimental results show only incremental improvement over baseline, and the choice of evaluation makes it hard to verify one of the central arguments: that visual features improve performance when processing rare/unseen words.\\n\\n- Some details about the baseline are missing, which makes it difficult to interpret the results, and would make it hard to reproduce the work.\\n\\n\\nGeneral Discussion:\\n\\nThe paper proposes the use of computer vision techniques (CNNs applied to images of text) to improve language processing for Chinese, Japanese, and Korean, languages in which characters themselves might be compositional. The authors evaluate their model on a simple text-classification task (assigning Wikipedia page titles to categories). They show that a simple one-hot representation of the characters outperforms the CNN-based representations, but that the combination of the visual representations with standard one-hot encodings performs better than the visual or the one-hot alone. They also present some evidence that the visual features outperform the one-hot encoding on rare words, and present some intuitive qualitative results suggesting the CNN learns good semantic embeddings of the characters.\\n\\nI think the idea of processing languages like Chinese and Japanese visually is a great one, and the motivation for this paper makes a lot of sense. However, I am not entirely convinced by the experimental results. The evaluations are quite weak, and it is hard to say whether these results are robust or simply coincidental. I would prefer to see some more rigorous evaluation to make the paper publication-ready. If the results are statistically significant (if the authors can indicate this in the author response), I would support accepting the paper, but ideally, I would prefer to see a different evaluation setup altogether.\"\n}",
        "human_end_index": 2357
    },
    {
        "id": 553,
        "source": "PeerRead/acl_2017/",
        "title": "Cross-Context Lexical Analysis",
        "human_review": "- Strengths: A nice, solid piece of work that builds on previous studies in a\nproductive way. Well-written and clear. \n\n- Weaknesses:\n\n Very few--possibly avoid some relatively \"empty\" statements:\n\n191 : For example, if our task is to identify words used similarly across\ncontexts, our scoring function can be specified to give high scores to terms\nwhose usage is similar across the contexts.\n\n537 : It is educational to study how annotations drawn from the same data are\nsimilar or different.\n\n- General Discussion:\nIn the first sections I was not sure that much was being done that was new or\ninteresting, as the methods seemed very reminiscent of previous methods used\nover the past 25 years to measure similarity, albeit with a few new statistical\ntwists, but conceptually in the same vein. Section 5, however, describes an\ninteresting and valuable piece of work that will be useful for future studies\non the topic. In retrospect, the background provided in sections 2-4 is useful,\nif not necessary, to support the experiments in section 5. \n\nIn short, the work and results described will be useful to others working in\nthis area, and the paper is worthy of presentation at ACL.\n\nMinor comments:\n\nWord, punctuation missing?\n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative\nsampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al.\n(2016).\n\nUnclear what \"multiple methods\" refers to :\n278 : some words were detected by multiple methods with CCLA\nThis paper propose a general framework for analyzing similarities and\ndifferences in term meaning and representation in different contexts.\n\n- Strengths:\n* The framework proposed in this paper is generalizable and can be applied to\ndifferent applications, and accommodate difference notation of context,\ndifferent similarity functions, different type of word annotations. \n* The paper is well written. Very easy to follow.\n\n- Weaknesses:\n* I have concerns in terms of experiment evaluation. The paper uses qualitative\nevaluation metrics, which makes it harder to evaluate the effectiveness, or\neven the validity of proposed method. For example, table 1 compares the result\nwith Hamilton et, al using different embedding vector by listing top 10 words\nthat changed from 1900 to 1990. It's hard to tell, quantitatively, the\nperformances of CCLA. The same issue also applies to experiment 2 (comparative\nlexical analysis over context). The top 10 words may be meaningful, but what\nabout top 20, 100? what about the words that practitioner actually cares?\nWithout addressing the evaluation issue, I find it difficult to claim that CCLA\nwill benefit downstream applications.",
        "partial_review": "- Strengths: A nice, solid piece of work that builds on previous studies in a\nproductive way. Well-written and clear. \n\n- Weaknesses:\n\n Very few--possibly avoid some relatively \"empty\" statements:\n\n191 : For example, if our task is to identify words used similarly across\ncontexts, our scoring function can be specified to give high scores to terms\nwhose usage is similar across the contexts.\n\n537 : It is educational to study how annotations drawn from the same data are\nsimilar or different.\n\n- General Discussion:\nIn the first sections I was not sure that much was being done that was new or\ninteresting, as the methods seemed very reminiscent of previous methods used\nover the past 25 years to measure similarity, albeit with a few new statistical\ntwists, but conceptually in the same vein. Section 5, however, describes an\ninteresting and valuable piece of work that will be useful for future studies\non the topic. In retrospect, the background provided in sections 2-4 is useful,\nif not necessary, to support the experiments in section 5. \n\nIn short, the work and results described will be useful to others working in\nthis area, and the paper is worthy of presentation at ACL.\n\nMinor comments:\n\nWord, punctuation missing?\n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative\nsampling fro",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Cross-Context Lexical Analysis \n\nHere is the abstract of the paper: We propose a general framework for performing cross-context lexical                  analysis; that is, analyzing similarities and differences in term meaning         and representation with respect to different, potentially overlapping             partitions of a text collection.                                                       We apply our framework to three different tasks: semantic change detection        (discovering words whose meanings changed over time), comparative lexical         analysis over context (finding context-sensitive and                              context-\\emph{in}sensitive terms), and word representation comparison             (investigating randomness inherent in word embeddings). \n\nHere is the partial review: - Strengths: A nice, solid piece of work that builds on previous studies in a\nproductive way. Well-written and clear. \n\n- Weaknesses:\n\n Very few--possibly avoid some relatively \"empty\" statements:\n\n191 : For example, if our task is to identify words used similarly across\ncontexts, our scoring function can be specified to give high scores to terms\nwhose usage is similar across the contexts.\n\n537 : It is educational to study how annotations drawn from the same data are\nsimilar or different.\n\n- General Discussion:\nIn the first sections I was not sure that much was being done that was new or\ninteresting, as the methods seemed very reminiscent of previous methods used\nover the past 25 years to measure similarity, albeit with a few new statistical\ntwists, but conceptually in the same vein. Section 5, however, describes an\ninteresting and valuable piece of work that will be useful for future studies\non the topic. In retrospect, the background provided in sections 2-4 is useful,\nif not necessary, to support the experiments in section 5. \n\nIn short, the work and results described will be useful to others working in\nthis area, and the paper is worthy of presentation at ACL.\n\nMinor comments:\n\nWord, punctuation missing?\n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative\nsampling fro \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a general framework for cross-context lexical analysis and applies it to three different tasks effectively. The strengths of the paper include its solid methodology, clear writing style, and productive use of previous studies. However, there are a few weaknesses, such as the presence of relatively 'empty' statements that could be avoided. Additionally, in the initial sections, it may appear that not much new or interesting work is being done, as the methods seem reminiscent of previous approaches. However, Section 5 presents an interesting and valuable contribution, which makes the background provided in sections 2-4 essential. In conclusion, this paper will be useful to other researchers in the field and is suitable for presentation at ACL.\"\n}",
        "human_end_index": 1320
    },
    {
        "id": "554",
        "source": "PeerRead/acl_2017/",
        "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling",
        "human_review": "- Strengths:\n\na) The paper presents a Bayesian learning approach for recurrent neural network\nlanguage model. The method outperforms standard SGD with dropout on three\ntasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of\ninterest to the NLP community for various applications.\n\n- Weaknesses:\n\nPrimary concern is about evaluation:\n\nSec 5.1: The paper reports the performance of difference types of architectures\n(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning\nalgorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are\ncompared for the character LM while SGD +/- dropout is compared with SGLD +/-\ndropout on word language model task. This is inconsistent!  I would suggest\nreporting both these dimensions (i.e. architectures and the exact same learning\nalgorithms) on both character and word LM tasks. It would be useful to know if\nthe results from the proposed Bayesian learning approaches are portable across\nboth these tasks and data sets.\n\nL529: The paper states that 'the performance gain mainly comes from adding\ngradient noise and model averaging'. This statement is not justified\nempirically. To arrive at this conclusion, an A/B experiment with/without\nadding gradient noise and/or model averaging needs to be done. \n\nL724: Gal's dropout is run on the sentence classification task but not on\nlanguage model/captions task. Since Gal's dropout is not specific to sentence\nclassification,  I would suggest reporting the performance of this method on\nall three tasks. This would allow the readers to fully assess the utility of\nthe proposed algorithms relative to all existing dropout approaches.\n\nL544: Is there any sort order for the samples? (\\theta_1, ..., \\theta_K)? e.g.\nare samples with higher posterior probabilities likely to be at higher indices?\nWhy not report the result of randomly selecting K out of S samples, as an\nadditional alternative?\n\nRegular RNN LMs are known to be expensive to train and evaluate. It would be\nvery useful to compare the training/evaluation times for the proposed Bayesian\nlearning algorithms with SGD+ dropout. That would allow the readers to\ntrade-off improvements versus increase in training/run times.\n\nClarifications:\nL346: What does \\theta_s refer to? Is this a MAP estimate of parameters based\non only the sample s?\nL453-454: Clarify what \\theta means in the context of dropout/dropconnect. \n\nTypos:\nL211: output\nL738: RMSProp\n- Strengths:\n1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and\nStochastic Optimization in deep learning context. Given dropout/dropConnect and\nvariational inference are commonly used to reduce the overfit, the more\nsystematic way to introduce/analyse such bayesian learning based algorithms\nwould benefit deep learning community.\n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout\noutperforms RMSProp + dropout, which clearly shows that uncertainty modeling\nwould help reducing the over-fitting, hence improving accuracy.\n3) The paper has provided the details about the model/experiment setups so the\nresults should be easily reproduced.\n\n- Weaknesses:\n1) The paper does not dig into the theory profs and show the convergence\nproperties of the proposed algorithm.\n2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not\nconduct other comparison. It should explain more about the relation between\npSGLD vs RMSProp other than just mentioning they are conterparts in two\nfamilies.\n2) The paper does not talk about the training speed impact with more details.\n\n- General Discussion:\n- Strengths: This paper explores a relatively under-explored area of practical\napplication of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian\ntreatment of the parameters of RNNs, it is possible to incorporate benefits of\nmodel averaging during inference. Further, their gradient\nbased sampling approximation to the posterior estimation leads to a procedure\nwhich is easy to implement and is potentially much cheaper than other\nwell-known techniques for model averaging like ensembling.  \nThe effectiveness of this approach is shown on three different tasks --\nlanguage modeling, image captioning and sentence classification; and\nperformance gains are observed over the baseline of single model optimization.\n\n- Weaknesses: Exact experimental setup is unclear. The supplementary material\ncontains important details about burn-in, number of epochs and samples\ncollected that should be in the main paper itself. Moreover, details on how the\ninference is performed would be helpful. Were the samples that were taken\nfollowing HMC for a certain number of epochs after burn in on the training data\nfixed for inference (for every \\tilda{Y} during test time, same samples were\nused according to eqn 5) ? Also, an explicit clarification regarding an\nindependence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X),\nwhich lets one use the conditional RNN model (if I understand correctly) for\nthe potential U(\\theta) would be nice for completeness.\n\nIn terms of comparison, this paper would also greatly benefit from a\ndiscussion/ experimental comparison with ensembling and distillation methods\n(\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble\nof Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are\nintimately related by a similar goal of incorporating effects of model\naveraging.\n\nFurther discussion related to preference of HMC related sampling\nmethods over other sampling methods or variational approximation would be\nhelpful.\n\nFinally, equation 8 hints at the potential equivalence between dropout and the\nproposed approach and the theoretical justification behind combining SGLD and\ndropout (by making the equivalence more concrete) would lead to a better\ninsight into the effectiveness of the proposed approach.  \n\n- General Discussion: Points addressed above.",
        "partial_review": "- Strengths:\n\na) The paper presents a Bayesian learning approach for recurrent neural network\nlanguage model. The method outperforms standard SGD with dropout on three\ntasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of\ninterest to the NLP community for various applications.\n\n- Weaknesses:\n\nPrimary concern is about evaluation:\n\nSec 5.1: The paper reports the performance of difference types of architectures\n(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning\nalgorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are\ncompared for the character LM while SGD +/- dropout is compared with SGLD +/-\ndropout on word language model task. This is inconsistent!  I would suggest\nreporting both these dimensions (i.e. architectures and the exact same learning\nalgorithms) on both character and word LM tasks. It would be useful to know if\nthe results from the proposed Bayesian learning approaches are portable across\nboth these tasks and data sets.\n\nL529: The paper states that 'the performance gain mainly comes from adding\ngradient noise and model averaging'. This statement is not justified\nempirically. To arrive at this conclusion, an A/B experiment with/without\nadding gradient noise and/or model averaging needs to be done. \n\nL724: Gal's dropout is run on the sentence classification task but not on\nlanguage model/captions task. Since Gal's dropout is not specific to sentence\nclassification,  I would suggest reporting the performance of this method on\nall three tasks. This would allow the readers to fully assess the utility of\nthe proposed algorithms relative to all existing dropout approaches.\n\nL544: Is there any sort order for the samples? (\\theta_1, ..., \\theta_K)? e.g.\nare samples with higher posterior probabilities likely to be at higher indices?\nWhy not report the result of randomly selecting K out of S samples, as an\nadditional alternative?\n\nRegular RNN LMs are known to be expensive to train and evaluate. It would be\nvery useful to compare the training/evaluation times for the proposed Bayesian\nlearning algorithms with SGD+ dropout. That would allow the readers to\ntrade-off improvements versus increase in training/run times.\n\nClarifications:\nL346: What does \\theta_s refer to? Is this a MAP estimate of parameters based\non only the sample s?\nL453-454: Clarify what \\theta means in the context of dropout/dropconnect. \n\nTypos:\nL211: output\nL738: RMSProp\n- Strengths:\n1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and\nStochastic Optimization in deep learning context. Given dropout/dropConnect and\nvariational inference are commonly used to reduce the overfit, the more\nsystematic way to introduce/analyse such bayesian learning based algorithms\nwould benefit deep learning community.\n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout\noutperforms RMSProp + dropout, which clearly shows that uncertainty modeling\nwould help reducing th",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling \n\nHere is the abstract of the paper: Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization. \n\nHere is the partial review: - Strengths:\n\na) The paper presents a Bayesian learning approach for recurrent neural network\nlanguage model. The method outperforms standard SGD with dropout on three\ntasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of\ninterest to the NLP community for various applications.\n\n- Weaknesses:\n\nPrimary concern is about evaluation:\n\nSec 5.1: The paper reports the performance of difference types of architectures\n(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning\nalgorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are\ncompared for the character LM while SGD +/- dropout is compared with SGLD +/-\ndropout on word language model task. This is inconsistent!  I would suggest\nreporting both these dimensions (i.e. architectures and the exact same learning\nalgorithms) on both character and word LM tasks. It would be useful to know if\nthe results from the proposed Bayesian learning approaches are portable across\nboth these tasks and data sets.\n\nL529: The paper states that 'the performance gain mainly comes from adding\ngradient noise and model averaging'. This statement is not justified\nempirically. To arrive at this conclusion, an A/B experiment with/without\nadding gradient noise and/or model averaging needs to be done. \n\nL724: Gal's dropout is run on the sentence classification task but not on\nlanguage model/captions task. Since Gal's dropout is not specific to sentence\nclassification,  I would suggest reporting the performance of this method on\nall three tasks. This would allow the readers to fully assess the utility of\nthe proposed algorithms relative to all existing dropout approaches.\n\nL544: Is there any sort order for the samples? (\\theta_1, ..., \\theta_K)? e.g.\nare samples with higher posterior probabilities likely to be at higher indices?\nWhy not report the result of randomly selecting K out of S samples, as an\nadditional alternative?\n\nRegular RNN LMs are known to be expensive to train and evaluate. It would be\nvery useful to compare the training/evaluation times for the proposed Bayesian\nlearning algorithms with SGD+ dropout. That would allow the readers to\ntrade-off improvements versus increase in training/run times.\n\nClarifications:\nL346: What does \\theta_s refer to? Is this a MAP estimate of parameters based\non only the sample s?\nL453-454: Clarify what \\theta means in the context of dropout/dropconnect. \n\nTypos:\nL211: output\nL738: RMSProp\n- Strengths:\n1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and\nStochastic Optimization in deep learning context. Given dropout/dropConnect and\nvariational inference are commonly used to reduce the overfit, the more\nsystematic way to introduce/analyse such bayesian learning based algorithms\nwould benefit deep learning community.\n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout\noutperforms RMSProp + dropout, which clearly shows that uncertainty modeling\nwould help reducing th \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Strengths:\\n1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and Stochastic Optimization in deep learning context. Given dropout/dropConnect and variational inference are commonly used to reduce the overfit, the more systematic way to introduce/analyse such bayesian learning based algorithms would benefit deep learning community.\\n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout outperforms RMSProp + dropout, which clearly shows that uncertainty modeling would help reducing the overfitting.\\n\\nWeaknesses:\\nPrimary concern is about evaluation:\\n\\nSec 5.1: The paper reports the performance of difference types of architectures (LSTM/GRU/vanilla RNN) on character LM task while comparing the learning algorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are compared for the character LM while SGD +/- dropout is compared with SGLD +/- dropout on word language model task. This is inconsistent! I would suggest reporting both these dimensions (i.e. architectures and the exact same learning algorithms) on both character and word LM tasks. It would be useful to know if the results from the proposed Bayesian learning approaches are portable across both these tasks and data sets.\\n\\nL529: The paper states that 'the performance gain mainly comes from adding gradient noise and model averaging'. This statement is not justified empirically. To arrive at this conclusion, an A/B experiment with/without adding gradient noise and/or model averaging needs to be done.\\n\\nL724: Gal's dropout is run on the sentence classification task but not on language model/captions task. Since Gal's dropout is not specific to sentence classification, I would suggest reporting the performance of this method on all three tasks. This would allow the readers to fully assess the utility of the proposed algorithms relative to all existing dropout approaches.\\n\\nL544: Is there any sort order for the samples? (\\u03b8_1, ..., \\u03b8_K)? e.g. are samples with higher posterior probabilities likely to be at higher indices? Why not report the result of randomly selecting K out of S samples, as an additional alternative?\\n\\nRegular RNN LMs are known to be expensive to train and evaluate. It would be very useful to compare the training/evaluation times for the proposed Bayesian learning algorithms with SGD+ dropout. That would allow the readers to trade-off improvements versus increase in training/run times.\\n\\nClarifications:\\nL346: What does \\u03b8_s refer to? Is this a MAP estimate of parameters based on only the sample s?\\nL453-454: Clarify what \\u03b8 means in the context of dropout/dropconnect.\\n\\nTypos:\\nL211: output\\nL738: RMSProp\"\n}",
        "human_end_index": 3044
    },
    {
        "id": 557,
        "source": "PeerRead/acl_2017/",
        "title": "End-to-End Neural Relation Extraction with Global Optimization",
        "human_review": "- Strengths:\n\n - The paper is clearly written and well-structured. \n\n - The system newly applied several techniques including global optimization to\nend-to-end neural relation extraction, and the direct incorporation of the\nparser representation is interesting.\n\n - The proposed system has achieved the state-of-the-art performance on both\nACE05 and CONLL04 data sets.\n\n - The authors include several analyses.\n\n- Weaknesses:\n\n - The approach is incremental and seems like just a combination of existing\nmethods.  \n\n - The improvements on the performance (1.2 percent points on dev) are\nrelatively small, and no significance test results are provided.\n\n- General Discussion:\n\n- Major comments:\n\n - The model employed a recent parser and glove word embeddings. How did they\naffect the relation extraction performance?\n\n - In prediction, how did the authors deal with illegal predictions?\n\n- Minor comments:\n\n - Local optimization is not completely \"local\". It \"considers structural\ncorrespondences between incremental decisions,\" so this explanation in the\nintroduction is misleading.\n\n - Points in Figures 6 and 7 should be connected with straight lines, not\ncurves.\n\n - How are entities represented in \"-segment\"?\n\n - Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR,\nand Li et al. (2014) misses pages.",
        "partial_review": "- Strengths:\n\n - The paper is clearly written and well-structured. \n\n - The system newly applied several techniques including global optimization to\nend-to-end neural relation extraction, and the direct incorporation of the\nparser representation is interesting.\n\n - The proposed system has achieved the state-of-the-art performance on both\nACE05 and CONLL04 data sets.\n\n - The authors include several analyses.\n\n- Weaknesses:\n\n - The approach is incremental and seems like just a combination of existing\nmethods.  \n\n - The improvements on the performance (1.2 percent points on dev) are\nrelatively small, and no significance test resu",
        "task3_prompt": "\nComplete a partially written peer review about the paper: End-to-End Neural Relation Extraction with Global Optimization \n\nHere is the abstract of the paper: Neural networks have recently shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn representations. Experiments show that our model is highly effective, achieving the best performances on two standard benchmarks. \n\nHere is the partial review: - Strengths:\n\n - The paper is clearly written and well-structured. \n\n - The system newly applied several techniques including global optimization to\nend-to-end neural relation extraction, and the direct incorporation of the\nparser representation is interesting.\n\n - The proposed system has achieved the state-of-the-art performance on both\nACE05 and CONLL04 data sets.\n\n - The authors include several analyses.\n\n- Weaknesses:\n\n - The approach is incremental and seems like just a combination of existing\nmethods.  \n\n - The improvements on the performance (1.2 percent points on dev) are\nrelatively small, and no significance test resu \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a clearly written and well-structured approach to end-to-end neural relation extraction using global optimization. The authors have incorporated novel LSTM features and the direct incorporation of parser representation, which adds to the novelty of their system. Their experimental results indicate that the proposed model achieves state-of-the-art performance on both ACE05 and CONLL04 datasets. The inclusion of several analyses further strengthens the paper. However, one weakness of the approach is its incremental nature, which gives the impression of it being a combination of existing methods. Additionally, the improvements in performance are relatively small (1.2 percent points on dev), and no significance test results are provided. It would be beneficial if the authors could address these concerns in their final version of the paper.\"\n}",
        "human_end_index": 634
    },
    {
        "id": 56,
        "source": "PeerRead/acl_2017/",
        "title": "Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics",
        "human_review": "- Strengths:\nThis paper presents an extension of many popular methods for learning vector\nrepresentations of text.  The original methods, such as skip-gram with negative\nsampling, Glove, or other PMI based approaches currently use word cooccurrence\nstatistics, but all of those approaches could be extended to n-gram based\nstatistics.  N-gram based statistics would increase the complexity of every\nalgorithm because both the vocabulary of the embeddings and the context space\nwould be many times larger.  This paper presents a method to learn embeddings\nfor ngrams with ngram context, and efficiently computes these embeddings.  On\nsimilarity and analogy tasks, they present strong results.\n\n- Weaknesses:\nI would have loved to see some experiments on real tasks where these embeddings\nare used as input beyond the experiments presented in the paper.  That would\nhave made the paper far stronger.\n\n- General Discussion:\nEven with the aforementioned weakness, I think this is a nice paper to have at\nACL.\n\nI have read the author response.\n- Strengths: The idea to train word2vec-type models with ngrams (here\nspecifically: bigrams) instead of words is excellent. The range of experimental\nsettings (four word2vec-type algorithms, several word/bigram conditions) covers\nquite a bit of ground. The qualitative inspection of the bigram embeddings is\ninteresting and shows the potential of this type of model for multi-word\nexpressions. \n\n- Weaknesses: This paper would benefit from a check by a native speaker of\nEnglish, especially regarding the use of articles. The description of the\nsimilarity and analogy tasks comes at a strange place in the paper (4.1\nDatasets). \n\n- General Discussion: As is done at some point well into the paper, it could be\nclarified from the start that this is simply a generalization of the original\nword2vec idea, redefining the word as an ngram (unigram) and then also using\nbigrams. It would be good to give a rationale why larger ngrams have not been\nused.\n\n(I have read the author response.)\nThis paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,\nSVD) to include ngram-ngram cooccurance statistics. To deal with the large\ncomputational costs of storing such expensive matrices, the authors propose an\nalgorithm that uses two different strategies to collect counts.  \n\n- Strengths:\n\n* The proposed work seems like a natural extension of existing work on learning\nword embeddings. By integrating bigram information, one can expect to capture\nricher syntactic and semantic information.\n\n- Weaknesses:\n\n* While the authors propose learning embeddings for bigrams (bi_bi case), they\nactually do not evaluate the embeddings for the learned bigrams except for the\nqualitative evaluation in Table 7. A more quantitative evaluation on\nparaphrasing or other related tasks that can include bigram representations\ncould have been a good contribution.\n\n* The evaluation and the results are not very convincing - the results do not\nshow consistent trends, and some of the improvements are not necessarily\nstatistically significant.\n\n* The paper reads clunkily due to significant grammar and spelling errors,\nand needs a major editing pass.\n\n- General Discussion:\n\nThis paper is an extension of standard embedding learning techniques to include\ninformation from bigram-bigram coocurance. While the work is interesting and a\nnatural extension of existing work, the evaluation and methods leaves some open\nquestions. Apart from the ones mentioned in the weaknesses, some minor\nquestions for the authors :\n\n* Why is there significant difference between the overlap and non-overlap\ncases? I would be more interested in finding out more than the quantitative\ndifference shown on the tasks.\n\nI have read the author response. I look forward to seeing the revised version\nof the paper.",
        "partial_review": "- Strengths:\nThis paper presents an extension of many popular methods for learning vector\nrepresentations of text.  The original methods, such as skip-gram with negative\nsampling, Glove, or other PMI based approaches currently use word cooccurrence\nstatistics, but all of those approaches could be extended to n-gram based\nstatistics.  N-gram based statistics would increase the complexity of every\nalgorithm because both the vocabulary of the embeddings and the context space\nwould be many times larger.  This paper presents a method to learn embeddings\nfor ngrams with ngram context, and efficiently computes these embeddings.  On\nsimilarity and analogy tasks, they present strong results.\n\n- Weaknesses:\nI would have loved to see some experiments on real tasks where these embeddings\nare used as input beyond the experiments presented in the paper.  That would\nhave made the paper far stronger.\n\n- General Discussion:\nEven with the aforementioned weakness, I think this is a nice paper to have at\nACL.\n\nI have read the author response.\n- Strengths: The idea to train word2vec-type models with ngrams (here\nspecifically: bigrams) instead of words is excellent. The range of experimental\nsettings (four word2vec-type algorithms, several word/bigram conditions) covers\nquite a bit of ground. The qualitative inspection of the bigram embeddings is\ninteresting and shows the potential of this type of model for multi-word\nexpressions. \n\n- Weaknesses: This paper would benefit from a check by a native speaker of\nEnglish, especially regarding the use of articles. The description of the\nsimilarity and analogy tasks comes at a strange place in the paper (4.1\nDatasets). \n\n- General Discussion: As is done at some point well into the paper, it could be\nclarified from the start that this is simply a generalization of the original\nword2vec idea, redefining the word as an ngram (unigram) an",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics \n\nHere is the abstract of the paper: The existing word representation methods mostly limit their information source to word co-occurrence statistics. In this paper, we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix and its SVD factorization. Comprehensive experiments are conducted on word analogy and similarity tasks. The results show that improved word representations are learned from ngram co-occurrence statistics. We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations. Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burden brought by ngrams. \n\nHere is the partial review: - Strengths:\nThis paper presents an extension of many popular methods for learning vector\nrepresentations of text.  The original methods, such as skip-gram with negative\nsampling, Glove, or other PMI based approaches currently use word cooccurrence\nstatistics, but all of those approaches could be extended to n-gram based\nstatistics.  N-gram based statistics would increase the complexity of every\nalgorithm because both the vocabulary of the embeddings and the context space\nwould be many times larger.  This paper presents a method to learn embeddings\nfor ngrams with ngram context, and efficiently computes these embeddings.  On\nsimilarity and analogy tasks, they present strong results.\n\n- Weaknesses:\nI would have loved to see some experiments on real tasks where these embeddings\nare used as input beyond the experiments presented in the paper.  That would\nhave made the paper far stronger.\n\n- General Discussion:\nEven with the aforementioned weakness, I think this is a nice paper to have at\nACL.\n\nI have read the author response.\n- Strengths: The idea to train word2vec-type models with ngrams (here\nspecifically: bigrams) instead of words is excellent. The range of experimental\nsettings (four word2vec-type algorithms, several word/bigram conditions) covers\nquite a bit of ground. The qualitative inspection of the bigram embeddings is\ninteresting and shows the potential of this type of model for multi-word\nexpressions. \n\n- Weaknesses: This paper would benefit from a check by a native speaker of\nEnglish, especially regarding the use of articles. The description of the\nsimilarity and analogy tasks comes at a strange place in the paper (4.1\nDatasets). \n\n- General Discussion: As is done at some point well into the paper, it could be\nclarified from the start that this is simply a generalization of the original\nword2vec idea, redefining the word as an ngram (unigram) an \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper presents a novel approach of incorporating ngrams into popular word representation methods. The authors conduct comprehensive experiments on word analogy and similarity tasks and demonstrate that improved word representations are learned from ngram co-occurrence statistics. The paper also introduces a novel approach of building co-occurrence matrix to alleviate the hardware burden brought by ngrams. \n\nOne strength of this paper is that it extends well-known methods for learning vector representations of text to incorporate ngrams, which increases the complexity of the algorithms but has the potential to improve word representations. The authors provide strong results on similarity and analogy tasks, showcasing the effectiveness of their approach.\n\nA weakness of this paper is the lack of experiments on real tasks where these embeddings are used as input. Including such experiments would have strengthened the paper and shown the practical application of the learned ngram representations.\n\nIn general, I think this is a valuable contribution in the field of word representation learning, especially in terms of capturing multi-word expressions. However, the paper could benefit from a native English speaker checking the use of articles and the description of the similarity and analogy tasks. With some revisions, I believe this paper would be a strong addition to the conference.\"\n}",
        "human_end_index": 1886
    },
    {
        "id": 561,
        "source": "PeerRead/acl_2017/",
        "title": "Semi-supervised sequence tagging with bidirectional language models",
        "human_review": "The paper introduces a general method for improving NLP tasks using embeddings\nfrom language models. Context independent word representations have been very\nuseful, and this paper proposes a nice extension by using context-dependent\nword representations obtained from the hidden states of neural language models.\nThey show significant improvements in tagging and chunking tasks from including\nembeddings from large language models. There is also interesting analysis which\nanswers several natural questions.\n\nOverall this is a very good paper, but I have several suggestions:\n- Too many experiments are carried out on the test set. Please change Tables 5\nand 6 to use development data\n- It would be really nice to see results on some more tasks - NER tagging and\nchunking don't have many interesting long range dependencies, and the language\nmodel might really help in those cases. I'd love to see results on SRL or CCG\nsupertagging.\n- The paper claims that using a task specific RNN is necessary because a CRF on\ntop of language model embeddings performs poorly. It wasn't clear to me if they\nwere backpropagating into the language model in this experiment - but if not,\nit certainly seems like there is potential for that to make a task specific RNN\nunnecessary.\nThe paper proposes an approach where pre-trained word embeddings and\npre-trained neural language model embeddings are leveraged (i.e., concatenated)\nto improve the performance in English chunking and NER on the respective CoNLL\nbenchmarks, and on an out-of-domain English NER test set. The method records\nstate-of-the-art scores for the two tasks.\n\n- Strengths:\n\nFor the most part, the paper is well-written and easy to follow. The method is\nextensively documented. The discussion is broad and thorough.\n\n- Weaknesses:\n\nSequence tagging does not equal chunking and NER. I am surprised not to see POS\ntagging included in the experiment, while more sequence tagging tasks would be\nwelcome: grammatical error detection, supersense tagging, CCG supertagging,\netc. This way, the paper is on chunking and NER for English, not for sequence\ntagging in general, as it lacks both the multilingual component and the breadth\nof tasks.\n\nWhile I welcomed the extensive description of the method, I do think that\nfigures 1 and 2 overlap and that only one would have sufficed.\n\nRelated to that, the method itself is rather straightforward and simple. While\nthis is by all means not a bad thing, it seems that this contribution could\nhave been better suited for a short paper. Since I do enjoy the more extensive\ndiscussion section, I do not necessarily see it as a flaw, but the core of the\nmethod itself does not strike me as particularly exciting. It's more of a\n\"focused contribution\" (short paper description from the call) than\n\"substantial\" work (long paper).\n\n- General Discussion:\n\nBottomline, the paper concatenates two embeddings, and sees improvements in\nEnglish chunking and NER.\n\nAs such, does it warrant publication as an ACL long paper? I am ambivalent, so\nI will let my score reflect that, even if I slightly lean towards a negative\nanswer. Why? Mainly because I would have preferred to see more breadth: a) more\nsequence tagging tasks and b) more languages.\n\nAlso, we do not know how well this method scales to low(er)-resource scenarios.\nWhat if the pre-trained embeddings are not available? What if they were not as\nsizeable as they are? The experiments do include a notion of that, but still\nfar above the low-resource range. Could they not have been learned in a\nmulti-task learning setup in your model? That would have been more substantial\nin my view.\n\nFor these reasons, I vote borderline, but with a low originality score. The\nidea of introducing context via the embeddings is nice in itself, but this\nparticular instantiation of it leaves a lot to ask for.",
        "partial_review": "The paper introduces a general method for improving NLP tasks using embeddings\nfrom language models. Context independent word representations have been very\nuseful, and this paper proposes a nice extension by using context-dependent\nword representations obtained from the hidden states of neural language models.\nThey show significant improvements in tagging and chunking tasks from including\nembeddings from large language models. There is also interesting analysis which\nanswers several natural questions.\n\nOverall this is a very good paper, but I have several suggestions:\n- Too many experiments are carried out on the test set. Please change Tables 5\nand 6 to use development data\n- It would be really nice to see results on some more tasks - NER tagging and\nchunking don't have many interesting long range dependencies, and the language\nmodel might really help in those cases. I'd love to see results on SRL or CCG\nsupertagging.\n- The paper claims that using a task specific RNN is necessary because a CRF on\ntop of language model embeddings performs poorly. It wasn't clear to me if they\nwere backpropagating into the language model in this experiment - but if not,\nit certainly seems like there is potential for that to make a task specific RNN\nunnecessary.\nThe paper proposes an approach where pre-trained word embeddings and\npre-trained neural language model embeddings are leveraged (i.e., concatenated)\nto improve the performance in English chunking and NER on the respective CoNLL\nbenchmarks, and on an out-of-domain English NER test set. The method records\nstate-of-the-art scores for the two tasks.\n\n- Strengths:\n\nFor the most part, the paper is well-written and easy to follow. The method is\nextensively documented. The discussion is broad and thorough.\n\n- Weaknesses:\n\nSequence tagging does not equal chunking and NER. I am surprised not to see POS\ntagging included in the experiment, while more sequence tagging tasks would be\nwelcome: grammatical error detection, supersense tagging, CCG ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Semi-supervised sequence tagging with bidirectional language models \n\nHere is the abstract of the paper: Pre-trained word embeddings learned from unlabeled text have become a stan- dard component of neural network archi- tectures for NLP tasks. However, in most cases, the recurrent network that oper- ates on word-level representations to pro- duce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidi- rectional language models to NLP sys- tems and apply it to sequence labeling tasks. We evaluate our model on two stan- dard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers. \n\nHere is the partial review: The paper introduces a general method for improving NLP tasks using embeddings\nfrom language models. Context independent word representations have been very\nuseful, and this paper proposes a nice extension by using context-dependent\nword representations obtained from the hidden states of neural language models.\nThey show significant improvements in tagging and chunking tasks from including\nembeddings from large language models. There is also interesting analysis which\nanswers several natural questions.\n\nOverall this is a very good paper, but I have several suggestions:\n- Too many experiments are carried out on the test set. Please change Tables 5\nand 6 to use development data\n- It would be really nice to see results on some more tasks - NER tagging and\nchunking don't have many interesting long range dependencies, and the language\nmodel might really help in those cases. I'd love to see results on SRL or CCG\nsupertagging.\n- The paper claims that using a task specific RNN is necessary because a CRF on\ntop of language model embeddings performs poorly. It wasn't clear to me if they\nwere backpropagating into the language model in this experiment - but if not,\nit certainly seems like there is potential for that to make a task specific RNN\nunnecessary.\nThe paper proposes an approach where pre-trained word embeddings and\npre-trained neural language model embeddings are leveraged (i.e., concatenated)\nto improve the performance in English chunking and NER on the respective CoNLL\nbenchmarks, and on an out-of-domain English NER test set. The method records\nstate-of-the-art scores for the two tasks.\n\n- Strengths:\n\nFor the most part, the paper is well-written and easy to follow. The method is\nextensively documented. The discussion is broad and thorough.\n\n- Weaknesses:\n\nSequence tagging does not equal chunking and NER. I am surprised not to see POS\ntagging included in the experiment, while more sequence tagging tasks would be\nwelcome: grammatical error detection, supersense tagging, CCG  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper proposes a general semi-supervised approach for adding pre-trained context embeddings from bidirectional language models to NLP systems for sequence labeling tasks. It demonstrates significant improvements in named entity recognition (NER) and chunking, surpassing previous systems that use other forms of transfer or joint learning. The paper is well-written and easy to follow, with extensive documentation of the method. The discussion is broad and thorough. However, there are a few suggestions for improvement. Firstly, it is recommended to use development data instead of the test set for experiments in Tables 5 and 6. Secondly, the paper focuses on NER and chunking, but it would be interesting to see results on tasks with more long-range dependencies, such as SRL or CCG supertagging. Lastly, the claim that a task-specific RNN is necessary could benefit from further clarification on whether backpropagation into the language model was considered. Overall, the paper has several strengths but could benefit from addressing these weaknesses.\"\n}",
        "human_end_index": 2006
    },
    {
        "id": 562,
        "source": "PeerRead/acl_2017/",
        "title": "Zero-Shot Relation Extraction via Reading Comprehension",
        "human_review": "- Strengths:\nZero-shot relation extraction is an interesting problem. The authors have\ncreated a large dataset for relation extraction as question answering which\nwould likely be useful to the community.\n\n- Weaknesses:\nComparison and credit to existing work is severely lacking. Contributions of\nthe paper don't seen particularly novel.\n\n- General Discussion:\n\nThe authors perform relation extraction as reading comprehension. In order to\ntrain reading comprehension models to perform relation extraction, they create\na large dataset of 30m \u201cquerified\u201d (converted to natural language)\nrelations by asking mechanical turk annotators to write natural language\nqueries for relations from a schema. They use the reading comprehension model\nof Seo et al. 2016, adding the ability to return \u201cno relation,\u201d as the\noriginal model must always return an answer. The main motivation/result of the\npaper appears to be that the authors can perform zero-shot relation extraction,\nextracting relations only seen at test time.\n\nThis paper is well-written and the idea is interesting. However, there are\ninsufficient experiments and comparison to previous work to convince me that\nthe paper\u2019s contributions are novel and impactful.\n\nFirst, the authors are missing a great deal of related work: Neelakantan at al.\n2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction\nusing RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)\nperform relation extraction on unseen entities. The authors cite Bordes et al.\n(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and\nperform relation extraction using memory networks (which are commonly used for\nreading comprehension). However, they merely note that their data was annotated\nat the \u201crelation\u201d level rather than at the triple (relation, entity pair)\nlevel\u2026 but couldn\u2019t Bordes et al. have done the same in their annotation?\nIf there is some significant difference here, it is not made clear in the\npaper. There is also a NAACL 2016 paper\n(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation\nextraction using a new model based on memory networks\u2026 and I\u2019m sure there\nare more. Your work is so similar to much of this work that you should really\ncite and establish novelty wrt at least some of them as early as the\nintroduction -- that's how early I was wondering how your work differed, and it\nwas not made clear.\n\nSecond, the authors neither 1) evaluate their model on another dataset or 2)\nevaluate any previously published models on their dataset. This makes their\nempirical results extremely weak. Given that there is a wealth of existing work\nthat performs the same task and the lack of novelty of this work, the authors\nneed to include experiments that demonstrate that their technique outperforms\nothers on this task, or otherwise show that their dataset is superior to others\n(e.g. since it is much larger than previous, does it allow for better\ngeneralization?)\nThe paper presents a method for relation extraction based on converting the\ntask into a question answering task. The main hypothesis of the paper is that\nquestions are a more generic vehicle for carrying content than particular\nexamples of relations, and are easier to create. The results seem to show good\nperformance, though a direct comparison on a standard relation extraction task\nis not performed.\n- Strengths:\nThe technique seems to be adept at identifying relations (a bit under 90\nF-measure). It works well both on unseen questions (for seen relations) and\nrelatively well on unseen relations. The authors describe a method for\nobtaining a large training dataset\n\n- Weaknesses:\nI wish performance was also shown on standard relation extraction datasets - it\nis impossible to determine what types of biases the data itself has here\n(relations are generated from Wikidata via WikiReading - extracted from\nWikipedia, not regular newswire/newsgroups/etc). It seems to me that the NIST\nTAC-KBP slot filling dataset is good and appropriate to run a comparison.\n\nOne comparison that the authors did not do here (but should) is to train a\nrelation detection model on the generated data, and see how well it compares\nwith the QA approach.\n\n- General Discussion:\nI found the paper to be well written and argued, and the idea is interesting,\nand it seems to work decently. I also found it interesting that the zero-shot\nNL method behaved indistinguishably from the single question baseline, and not\nvery far from the multiple questions system.\nThe paper models the relation extraction problem as reading comprehension and\nextends a previously proposed reading comprehension (RC) model to extract\nunseen relations. The approach has two main components:\n\n1. Queryfication: Converting a relation into natural question. Authors use\ncrowdsourcing for this part.\n\n2. Applying RC model on the generated questions and sentences to get the answer\nspans. Authors extend a previously proposed approach to accommodate situations\nwhere there is no correct answer in the sentence.\n\nMy comments:\n\n1. The paper reads very well and the approach is clearly explained.\n\n2. In my opinion, though the idea of using RC for relation extraction is\ninteresting and novel, the approach is not novel. A part of the approach is\ncrowdsourced and the other part is taken directly from a previous work, as I\nmention above.\n\n3. Relation extraction is a well studied problem and there are plenty of\nrecently published works on the problem. However, authors do not compare their\nmethods against any of the previous works. This raises suspicion on the\neffectiveness of the approach. As seen from Table 2, the performance numbers of\nthe proposed method on the core task are not very convincing. However, this\nmaybe because of the dataset used in the paper. Hence, a comparison with\nprevious methods would actually help assess how the current method stands with\nthe state-of-the-art.\n\n4. Slot-filling data preparation: You say \"we took the first sentence s in D to\ncontain both e and a\". How can you get the answer sentence for (all) the\nrelations of an entity from the first sentence of the entity's Wikipedia\narticle? Please clarify this. See the following paper. They have a set of rules\nto locate (answer) sentences corresponding to an entity property in its\nWikipedia page:\n\nWu, Fei, and Daniel S. Weld. \"Open information extraction using Wikipedia.\"\nProceedings of the 48th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, 2010.\n\nOverall, I think the paper presents an interesting approach. However, unless\nthe effectiveness of the approach is demonstrated by comparing it against\nrecent works on relation extraction, the paper is not ready for publication.",
        "partial_review": "- Strengths:\nZero-shot relation extraction is an interesting problem. The authors have\ncreated a large dataset for relation extraction as question answering which\nwould likely be useful to the community.\n\n- Weaknesses:\nComparison and credit to existing work is severely lacking. Contributions of\nthe paper don't seen particularly novel.\n\n- General Discussion:\n\nThe authors perform relation extraction as reading comprehension. In order to\ntrain reading comprehension models to perform relation extraction, they create\na large dataset of 30m \u201cquerified\u201d (converted to natural language)\nrelations by asking mechanical turk annotators to write natural language\nqueries for relations from a schema. They use the reading comprehension model\nof Seo et al. 2016, adding the ability to return \u201cno relation,\u201d as the\noriginal model must always return an answer. The main motivation/result of the\npaper appears to be that the authors can perform zero-shot relation extraction,\nextracting relations only seen at test time.\n\nThis paper is well-written and the idea is interesting. However, there are\ninsufficient experiments and comparison to previous work to convince me that\nthe paper\u2019s contributions are novel and impactful.\n\nFirst, the authors are missing a great deal of related work: Neelakantan at al.\n2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction\nusing RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)\nperform relation extraction on unseen entities. The authors cite Bordes et al.\n(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and\nperform relation extraction using memory networks (which are commonly used for\nreading comprehension). However, they merely note that their data was annotated\nat the \u201crelation\u201d level rather than at the triple (relation, entity pair)\nlevel\u2026 but couldn\u2019t Bordes et al. have done the same in their annotation?\nIf there is some significant difference here, it is not made clear in the\npaper. There is also a NAACL 2016 paper\n(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation\nextraction using a new model based on memory networks\u2026 and I\u2019m sure there\nare more. Your work is so similar to much of this work that you should really\ncite and establish novelty wrt at least some of them as early as the\nintroduction -- that's how early I was wondering how your work differed, and it\nwas not made clear.\n\nSecond, the authors neither 1) evaluate their model on another dataset or 2)\nevaluate any previously published models on their dataset. This makes their\nempirical results extremely weak. Given that there is a wealth of existing work\nthat performs the same task and the lack of novelty of this work, the authors\nneed to include experiments that demonstrate that their technique outperforms\nothers on this task, or otherwise show that their dataset is superior to others\n(e.g. since it is much larger than previous, does it allow for better\ngeneralization?)\nThe paper presents a method for relation extraction based on converting the\ntask into a question answering task. The main hypothesis of the paper is that\nquestions are a more generic vehicle for carrying content than particular\nexamples of relations, and are easier to create. The results seem to show good\nperformance, though a direct comparison on a standard relation ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Zero-Shot Relation Extraction via Reading Comprehension \n\nHere is the abstract of the paper: We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn high-quality relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relations that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relations with high accuracy, and that zero-shot generalization to unseen relations is possible, at lower accuracy levels, setting the bar for future work on this task. \n\nHere is the partial review: - Strengths:\nZero-shot relation extraction is an interesting problem. The authors have\ncreated a large dataset for relation extraction as question answering which\nwould likely be useful to the community.\n\n- Weaknesses:\nComparison and credit to existing work is severely lacking. Contributions of\nthe paper don't seen particularly novel.\n\n- General Discussion:\n\nThe authors perform relation extraction as reading comprehension. In order to\ntrain reading comprehension models to perform relation extraction, they create\na large dataset of 30m \u201cquerified\u201d (converted to natural language)\nrelations by asking mechanical turk annotators to write natural language\nqueries for relations from a schema. They use the reading comprehension model\nof Seo et al. 2016, adding the ability to return \u201cno relation,\u201d as the\noriginal model must always return an answer. The main motivation/result of the\npaper appears to be that the authors can perform zero-shot relation extraction,\nextracting relations only seen at test time.\n\nThis paper is well-written and the idea is interesting. However, there are\ninsufficient experiments and comparison to previous work to convince me that\nthe paper\u2019s contributions are novel and impactful.\n\nFirst, the authors are missing a great deal of related work: Neelakantan at al.\n2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction\nusing RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)\nperform relation extraction on unseen entities. The authors cite Bordes et al.\n(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and\nperform relation extraction using memory networks (which are commonly used for\nreading comprehension). However, they merely note that their data was annotated\nat the \u201crelation\u201d level rather than at the triple (relation, entity pair)\nlevel\u2026 but couldn\u2019t Bordes et al. have done the same in their annotation?\nIf there is some significant difference here, it is not made clear in the\npaper. There is also a NAACL 2016 paper\n(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation\nextraction using a new model based on memory networks\u2026 and I\u2019m sure there\nare more. Your work is so similar to much of this work that you should really\ncite and establish novelty wrt at least some of them as early as the\nintroduction -- that's how early I was wondering how your work differed, and it\nwas not made clear.\n\nSecond, the authors neither 1) evaluate their model on another dataset or 2)\nevaluate any previously published models on their dataset. This makes their\nempirical results extremely weak. Given that there is a wealth of existing work\nthat performs the same task and the lack of novelty of this work, the authors\nneed to include experiments that demonstrate that their technique outperforms\nothers on this task, or otherwise show that their dataset is superior to others\n(e.g. since it is much larger than previous, does it allow for better\ngeneralization?)\nThe paper presents a method for relation extraction based on converting the\ntask into a question answering task. The main hypothesis of the paper is that\nquestions are a more generic vehicle for carrying content than particular\nexamples of relations, and are easier to create. The results seem to show good\nperformance, though a direct comparison on a standard relation  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents an interesting approach to relation extraction by reducing it to a reading comprehension task. The authors have created a large dataset for relation extraction which could be valuable to the community. However, the paper suffers from some weaknesses. Firstly, there is a lack of comparison and credit to existing work in the field, and the contributions of the paper do not seem particularly novel. There are several related works that perform similar tasks and should be cited and compared to establish the novelty of this work. Secondly, the authors do not evaluate their model on another dataset or compare it with previously published models, making their empirical results weak. It is crucial to demonstrate that their technique outperforms others on the task or that their dataset is superior to others. Overall, although the paper presents an interesting idea, more experiments and comparisons to existing work are needed to convince readers of the novelty and impact of the contributions.\"\n}",
        "human_end_index": 3353
    },
    {
        "id": 563,
        "source": "PeerRead/acl_2017/",
        "title": "Exploring Vector Spaces for Semantic Relations",
        "human_review": "- Strengths: The idea to investigate the types of relations between lexical\nitems is very interesting and challenging. The authors make a good argument why\ngoing beyond analogy testing makes sense.  \n\n- Weaknesses: The paper does not justify or otherwise contextualize the choice\nof clustering for evaluation, rather than using a classification task, despite\nthe fact that classification tasks are more straightforward to evaluate. No\nattempt is being made to explain the overall level of the results. How well\nwould humans do on this task (given only the words, no context)?\n\n- General Discussion:\n\nI have read the authors' response.\nThis paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations, similar to the\nanalogical reasoning task of Mikolov et al. (2013): Given an expression of the\nform \u201cX is for France what London is for the UK\u201d, X can be approximated by\nthe simple vector arithmetic operation London-UK+France. The authors argue that\nthis simple method can only capture very specific forms of analogies, and they\npresent a measure that aims at identifying a wider range of relations in a more\neffective way.\n\nI admit I find the idea of a single vector space model being able to capture a\nnumber of semantic relationships and analogies rather radical and infeasible.\nAs the authors mention in the paper, a number of studies already suggest for\nthe opposite. The reason is quite simple: behind all these models lies (some\nform of) the distributional hypothesis (words in similar contexts have similar\nmeanings), and this poses certain limitations in their expressive abilities;\nfor example, words like \u201cbig\u201d and \u201csmall\u201d will always be considered as\nsemantically similar from a vector perspective (although they express opposite\nmeanings), since they occur in similar contexts. So I cannot see how the\nexample given in Figure 1 is relevant to the very nature of vector spaces (or\nto any other semantic model for that matter!): there is a certain analogy\nbetween \u201cman-king\u201d, and \u201cwoman-queen\u201d, but asking from a word space to\ncapture \u201chas-a\u201d relationships of the form \u201cowl-has-claws\u201d, hence\n\u201chospital-has-walls\u201d, doesn\u2019t make much sense to me.\n\nThe motivation behind the main proposal of the paper (a similarity measure that\ninvolves a form of cross-comparison between vectors of words and vectors\nrepresenting the contexts of the words) is not clearly explained. Further, the\nmeasure is tested on the relation categories of the SemEval 2010 task with\nrather unsatisfactory results; in almost all cases, a simple baseline that\ntakes into account only partial similarities between the tested word pairs\npresent very high performance, with a difference from the best-performing model\nwhich seems to me statistically insignificant. So from both a methodological\nand an experimental perspective, the paper has weaknesses, and in its current\nform seems to describe work in progress;  as such I am inclined against its\npresentation in ACL.\n\n(Formatting issue: The authors use the LaTeX styles for ACL 2016 \u2014 this\nshould be fixed in case the paper is accepted).\n\nAUTHORS RESPONSE\n================\nThank you for the clarifications. I am still not comfortable with the idea of a\nmetric or a vector space that tries to capture both semantic and relational\nsimilarity, and I think you don't present enough experimental evidence that\nyour method works. I have to agree with one of the other reviewers that a more\nappropriate format for this work would be a short paper.\nThis paper presents a comparison of several vector combination techniques on\nthe task of relation classification.\n\n- Strengths:\n\nThe paper is clearly written and easy to understand.\n\n- Weaknesses:\n\nMy main complaint about the paper is the significance of its contributions. I\nbelieve it might be suitable as a short paper, but certainly not a full-length\npaper.\n\nUnfortunately, there is little original thought and no significantly strong\nexperimental results to back it up. The only contribution of this paper is an\n'in-out' similarity metric, which is itself adapted from previous work. The\nresults seem to be sensitive to the choice of clusters and only majorly\noutperforms a very naive baseline when the number of clusters is set to the\nexact value in the data beforehand.\n\nI think that relation classification or clustering from semantic vector space\nmodels is a very interesting and challenging problem. This work might be useful\nas an experimental nugget for future reference on vector combination and\ncomparison techniques, as a short paper. Unfortunately, it does not have the\nsubstance to merit a full-length paper.",
        "partial_review": "- Strengths: The idea to investigate the types of relations between lexical\nitems is very interesting and challenging. The authors make a good argument why\ngoing beyond analogy testing makes sense.  \n\n- Weaknesses: The paper does not justify or otherwise contextualize the choice\nof clustering for evaluation, rather than using a classification task, despite\nthe fact that classification tasks are more straightforward to evaluate. No\nattempt is being made to explain the overall level of the results. How well\nwould humans do on this task (given only the words, no context)?\n\n- General Discussion:\n\nI have read the authors' response.\nThis paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations, similar to the\nanalogical reasoning task of Mikolov et al. (2013): Given an expression of the\nform \u201cX is for France what London is for the UK\u201d, X can be approximated by\nthe simple vector arithmetic operation London-UK+France. The authors argue that\nthis simple method can only capture very specific forms of analogies, and they\npresent a measure that aims at identifying a wider range of relations in a more\neffective way.\n\nI admit I find the idea of a single vector space model being able to capture a\nnumber of semantic relationships and analogies rather radical and infeasible.\nAs the authors mention in the paper, a number of studies already suggest for\nthe opposite. The reason is quite simple: behind all these models lies (some\nform of) the distributional hypothesis (words in similar contexts have similar\nmeanings), and this poses certain limitations in their expressive abilities;\nfor example, words like \u201cbig\u201d and \u201csmall\u201d will always be considered as\nsemantically similar from a vector perspective (although they express opposite\nmeanings), since they occur in similar contexts. So I cannot see how the\nexample given in Figure 1 is relevant to the very nature of vector spaces (or\nto any other semantic model for that matter!): there is a certain analogy\nbetween \u201cman-king\u201d, and \u201cwoman-queen\u201d, but asking from a word space to\ncapture \u201chas-a\u201d relationships of the form \u201cowl-has-claws\u201d, hence\n\u201chospital-has-walls\u201d, doesn\u2019t make much sense to me.\n\nThe motivation behind the main proposal of the paper (a similarity measure that\ninvolves a form of cross-comparison between vectors of words and vectors\nrepresenting the contexts of the words) is not",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Exploring Vector Spaces for Semantic Relations \n\nHere is the abstract of the paper: Word embeddings are used with success for a variety of tasks involving lexical semantic similarities between individual words. Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities. In this paper, we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment. We propose a new relational similarity measure based on the combination of word2vec's CBOW input and output vectors which outperforms concurrent vector representations, when used for unsupervised clustering on SemEval 2010 Relation Classification data. \n\nHere is the partial review: - Strengths: The idea to investigate the types of relations between lexical\nitems is very interesting and challenging. The authors make a good argument why\ngoing beyond analogy testing makes sense.  \n\n- Weaknesses: The paper does not justify or otherwise contextualize the choice\nof clustering for evaluation, rather than using a classification task, despite\nthe fact that classification tasks are more straightforward to evaluate. No\nattempt is being made to explain the overall level of the results. How well\nwould humans do on this task (given only the words, no context)?\n\n- General Discussion:\n\nI have read the authors' response.\nThis paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations, similar to the\nanalogical reasoning task of Mikolov et al. (2013): Given an expression of the\nform \u201cX is for France what London is for the UK\u201d, X can be approximated by\nthe simple vector arithmetic operation London-UK+France. The authors argue that\nthis simple method can only capture very specific forms of analogies, and they\npresent a measure that aims at identifying a wider range of relations in a more\neffective way.\n\nI admit I find the idea of a single vector space model being able to capture a\nnumber of semantic relationships and analogies rather radical and infeasible.\nAs the authors mention in the paper, a number of studies already suggest for\nthe opposite. The reason is quite simple: behind all these models lies (some\nform of) the distributional hypothesis (words in similar contexts have similar\nmeanings), and this poses certain limitations in their expressive abilities;\nfor example, words like \u201cbig\u201d and \u201csmall\u201d will always be considered as\nsemantically similar from a vector perspective (although they express opposite\nmeanings), since they occur in similar contexts. So I cannot see how the\nexample given in Figure 1 is relevant to the very nature of vector spaces (or\nto any other semantic model for that matter!): there is a certain analogy\nbetween \u201cman-king\u201d, and \u201cwoman-queen\u201d, but asking from a word space to\ncapture \u201chas-a\u201d relationships of the form \u201cowl-has-claws\u201d, hence\n\u201chospital-has-walls\u201d, doesn\u2019t make much sense to me.\n\nThe motivation behind the main proposal of the paper (a similarity measure that\ninvolves a form of cross-comparison between vectors of words and vectors\nrepresenting the contexts of the words) is not \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The motivation behind the main proposal of the paper (a similarity measure that involves a form of cross-comparison between vectors of words and vectors representing the contexts of the words) is not fully justified. The author raises valid concerns regarding the limitations of vector spaces in capturing semantic relationships and analogies. However, it is not clear how the proposed method addresses these limitations. The choice of evaluating the method through unsupervised clustering instead of a classification task is not well justified and lacks explanation. Furthermore, the paper does not provide any insights into the overall level of the results and how they compare to human performance. Additional clarification and contextualization are needed throughout the paper to strengthen the arguments and justify the proposed approach.\"\n}",
        "human_end_index": 2431
    },
    {
        "id": "564",
        "source": "PeerRead/acl_2017/",
        "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
        "human_review": "This paper describes a straightforward extension to left-to-right beam search\nin order to allow it to incorporate lexical constraints in the form of word\nsequences that must appear in MT output. This algorithm is shown to be\neffective for interactive translation and domain adaptation.\n\nAlthough the proposed extension is very simple, I think the paper makes a\nuseful contribution by formalizing it. It is also interesting to know that NMT\ncopes well with a set of unordered constraints having no associated alignment\ninformation. There seem to be potential applications for this technique beyond\nthe ones investigated here, for example improving NMT\u2019s ability to handle\nnon-compositional constructions, which is one of the few areas where it still\nmight lag traditional SMT.\n\nThe main weakness of the paper is that the experiments are somewhat limited.\nThe interactive MT simulation shows that the method basically works, but it is\ndifficult to get a sense of how well - for instance, in how many cases the\nconstraint was incorporated in an acceptable manner (the large BLEU score\nincreases are only indirect evidence). Similarly, adaptation should have been \ncompared to the standard \u201cfine-tuning\u201d baseline, which would be relatively\ninexpensive to run on the 100K Autodesk corpus.\n\nDespite this weakness, I think this is a decent contribution that deserves to\nbe published.\n\nFurther details:\n\n422 Given its common usage in PBMT, \u201ccoverage vector\u201d is a potentially\nmisleading term. The appropriate data structure seems more likely to be a\ncoverage set.\n\nTable 2 should also give some indication of the number of constraints per\nsource sentence in the test corpora, to allow for calibration of the BLEU\ngains.",
        "partial_review": "This paper describes a straightforward extension to left-to-right beam search\nin order to allow it to incorporate lexical constraints in the form of word\nsequences that must appear in MT output. This algorithm is shown to be\neffective for interactive translation and domain adaptation.\n\nAlthough the proposed extension is very simple, I think the paper makes a\nuseful contribution by formalizing it. It is also interesting to know that NMT\ncopes well with a set of unordered constraints having no associated alignment\ninformation. There seem to be potential applications for this technique beyond\nthe ones investigated here, for example improving NMT\u2019s ability to handle\nnon-compositional constructions, which is one of the few areas where it still\nmight lag traditional SMT.\n\nThe main weakness of the paper is that the experiments are somewhat l",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search \n\nHere is the abstract of the paper: We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model's output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios. \n\nHere is the partial review: This paper describes a straightforward extension to left-to-right beam search\nin order to allow it to incorporate lexical constraints in the form of word\nsequences that must appear in MT output. This algorithm is shown to be\neffective for interactive translation and domain adaptation.\n\nAlthough the proposed extension is very simple, I think the paper makes a\nuseful contribution by formalizing it. It is also interesting to know that NMT\ncopes well with a set of unordered constraints having no associated alignment\ninformation. There seem to be potential applications for this technique beyond\nthe ones investigated here, for example improving NMT\u2019s ability to handle\nnon-compositional constructions, which is one of the few areas where it still\nmight lag traditional SMT.\n\nThe main weakness of the paper is that the experiments are somewhat l \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The main weakness of the paper is that the experiments are somewhat limited in scope. While the authors demonstrate the feasibility and flexibility of Lexically Constrained Decoding in the context of Neural Interactive-Predictive Translation and Domain Adaptation for Neural Machine Translation, it would have been beneficial to explore other potential applications of this technique. For instance, the paper mentions improving NMT's ability to handle non-compositional constructions, but does not provide any empirical evidence or analysis in this regard. Additionally, the paper could have benefited from more comprehensive comparisons with existing approaches or alternative methods for incorporating lexical constraints into sequence generation. Overall, while the paper presents a valuable contribution by formalizing the extension to left-to-right beam search, further experimentation and analysis would have strengthened the findings and conclusions of this research.\"\n}",
        "human_end_index": 846
    },
    {
        "id": 578,
        "source": "PeerRead/acl_2017/",
        "title": "Robust Incremental Neural Semantic Graph Parsing",
        "human_review": "- Strengths:\nThe paper proposes an end-to-end neural model for semantic graph parsing,\nbased on a well-designed transition system. \nThe work is interesting, learning\nsemantic representations of DMRS, which is capable of resolving semantics\nsuch as scope underspecification. This work shows a new scheme for\ncomputational semantics, benefiting from an end-to-end transition-based\nincremental framework, which resolves the parsing with low cost.\n\n- Weaknesses:\n  My major concern is that the paper only gives a very common introduction for\nthe\ndefinition of DMRS and EP, and the example even makes me a little confused\nbecause I cannot see anything special for DMRS. The description can be a little\nmore detailed, I think. However, upon the space limitation, it is\nunderstandable. The same problem exists for the transition system of the\nparsing model. If I do not have any background of MRS and EP, I can hardly\nlearn something from the paper, just seeing that this paper is very good.\n\n- General Discussion:\n  Overall, this paper is very interesting to me. I like the DMRS for semantic\nparsing very much and like the paper very much. Hope that the open-source codes\nand datasets can make this line of research being a hot topic.",
        "partial_review": "- Strengths:\nThe paper proposes an end-to-end neural model for semantic graph parsing,\nbased on a well-designed transition system. \nThe work is interesting, learning\nsemantic representations of DMRS, which is capable of resolving semantics\nsuch as scope underspecification. This work shows a new scheme for\ncomputational semantics, benefiting from an end-to-end transition-based\nincremental framework, which resolves the parsing with low cost.\n\n- Weaknesses:\n  My major concern is that the paper only gives a very common introduction for\nthe\ndefinition of DMRS and EP, and the example even makes me a little confused\nbecause I cannot see anything special for ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Robust Incremental Neural Semantic Graph Parsing \n\nHere is the abstract of the paper: Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation. \n\nHere is the partial review: - Strengths:\nThe paper proposes an end-to-end neural model for semantic graph parsing,\nbased on a well-designed transition system. \nThe work is interesting, learning\nsemantic representations of DMRS, which is capable of resolving semantics\nsuch as scope underspecification. This work shows a new scheme for\ncomputational semantics, benefiting from an end-to-end transition-based\nincremental framework, which resolves the parsing with low cost.\n\n- Weaknesses:\n  My major concern is that the paper only gives a very common introduction for\nthe\ndefinition of DMRS and EP, and the example even makes me a little confused\nbecause I cannot see anything special for  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"the DMRS and EP. The paper could provide more in-depth explanation and examples to clarify these concepts. Additionally, the paper could benefit from more thorough evaluation and comparison with existing approaches in terms of accuracy and computational efficiency. Overall, the work is interesting and provides a valuable contribution to the field of semantic parsing, but further improvements in clarity and evaluation would enhance its impact.\"\n}",
        "human_end_index": 659
    },
    {
        "id": 579,
        "source": "PeerRead/acl_2017/",
        "title": "MinIE: Minimizing Facts in Open Information Extraction",
        "human_review": "- Strengths:\n\n[+] Well motivated, tackles an interesting problem;\n\n[+] Clearly written and structured, accompanied by documented code and dataset;\n\n[+] Encouraging results.\n\n- Weaknesses:\n\n[-] Limited to completely deterministic, hand-engineered minimization rules;\n\n[-] Some relevant literature on OIE neglected;\n\n[-] Sound but not thorough experimental evaluation.\n\n- General Discussion:\n\nThis paper tackles a practical issue of most OIE systems, i.e. redundant,\nuninformative and inaccurate extractions. The proposed approach, dubbed MinOIE,\nis designed to actually \"minimize\" extractions by removing overly specific\nportions and turning them into structured annotations of various types\n(similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE\nsystem (ClausIE) and test it on two publicly available datasets, showing that\nit effectively leads to more concise extractions compared to standard OIE\napproaches, while at the same time retaining accuracy.\n\nOverall, this work focuses on an interesting (and perhaps underinvestigated)\naspect of OIE in a sound and principled way. The paper is clearly written,\nsufficiently detailed, and accompanied by supplementary material and a neat\nJava implementation.\nMy main concern is, however, with the entirely static, deterministic and\nrule-based structure of MinIE. Even though I understand that a handful of\nmanually engineered rules is technically the best strategy when precision is\nkey, these approaches are typically very hard to scale, e.g. in terms of\nlanguages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al.,\n2016). In other words, I think that this contribution somehow falls short of\nnovelty and substance in proposing a pipeline of engineered rules that are\nmostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance,\nI would have really appreciated an attempt to learn these minimization rules\ninstead of hard-coding them.\n\nFurthermore, the authors completely ignore a recent research thread on\n\u201csemantically-informed\u201d OIE (Nakashole et al., 2012; Moro and Navigli,\n2012; 2013; Delli Bovi et al., 2015) where traditional extractions are\naugmented with links to underlying knowledge bases and sense inventories\n(Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only\nrelevant in terms of related literature: in fact, having text fragments (or\nconstituents) explicitly linked to a knowledge base would reduce the need for\nad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example\nwith \"Bill of Rights\" provided by the authors (line 554), an OIE pipeline with\na proper Entity Linking module would recognize automatically the phrase as\nmention of a registered entity, regardless of the shape of its subconstituents.\nAlso, an underlying sense inventory would seamlessly incorporate the external\ninformation about collocations and multi-word expressions used in Section 6.2:\nnot by chance, the authors rely on WordNet and Wiktionary to compile their\ndictionary of collocations.\n\nFinally, some remarks on the experimental evaluation:\n\n- Despite the claim of generality of MinIE, the authors choose to experiment\nonly with ClausIE as underlying OIE system (most likely the optimal match). It\nwould have been very interesting to see if the improvement brought by MinIE is\nconsistent also with other OIE systems, in order to actually assess its\nflexibility as a post-processing tool.\n\n- Among the test datasets used in Section 7, I would have included the recent\nOIE benchmark of Stanovsky and Dagan (2016), where results are reported also\nfor comparison systems not included in this paper (TextRunner, WOIE, KrakeN).\n\nReferences:\n\n- Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using\nCross-lingual Projection. NAACL-HLT, 2015.\n\n- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an\nOpen Information Extraction System from English to German. EMNLP 2016.\n\n- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy\nof Relational Patterns with Semantic Types. EMNLP 2012.\n\n- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic\nNetwork with Ontologized Relations. CIKM 2012.\n\n- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis\ninto the Open Information Extraction Paradigm. IJCAI 2013.\n\n- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information\nExtraction from Textual Definitions through Deep Syntactic and Semantic\nAnalysis. TACL vol. 3, 2015.\n\n- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open\nInformation Extraction. EMNLP 2016.",
        "partial_review": "- Strengths:\n\n[+] Well motivated, tackles an interesting problem;\n\n[+] Clearly written and structured, accompanied by documented code and dataset;\n\n[+] Encouraging results.\n\n- Weaknesses:\n\n[-] Limited to completely deterministic, hand-engineered minimization rules;\n\n[-] Some relevant literature on OIE neglected;\n\n[-] Sound but not thorough experimental evaluation.\n\n- General Discussion:\n\nThis paper tackles a practical issue of most OIE systems, i.e. redundant,\nuninformative and inaccurate extractions. The proposed approach, dubbed MinOIE,\nis designed to actually \"minimize\" extractions by removing overly specific\nportions and turning them into structured annotations of various types\n(similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE\nsystem (ClausIE) and test it on two publicly available datasets, showing that\nit effectively leads to more concise extractions compared to standard OIE\napproaches, while at the same time retaining accuracy.\n\nOverall, this work focuses on an interesting (and perhaps underinvestigated)\naspect of OIE in a sound and principled way. The paper is clearly written,\nsufficiently detailed, and accompanied by supplementary material and a neat\nJava implementation.\nMy main concern is, however, with the entirely static, deterministic and\nrule-based structure of MinIE. Even though I understand that a handful of\nmanually engineered rules is technically the best strategy when precision is\nkey, these approaches are typically very hard to scale, e.g. in terms of\nlanguages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al.,\n2016). In other words, I think that this contribution somehow falls short of\nnovelty and substance in proposing a pipeline of engineered rules that are\nmostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance,\nI would have really appreciated an attempt to learn these minimization rules\ninstead of hard-coding them.\n\nFurthermore, the authors completely ignore a recent research thread on\n\u201csemantically-informed\u201d OIE (Nakashole et al., 2012; Moro and Navigli,\n2012; 2013; Delli Bovi et al., 2015) where traditional extractions are\naugmented with links to underlying knowledge bases and sense inventories\n(Wikipedia, Wikidata, ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: MinIE: Minimizing Facts in Open Information Extraction \n\nHere is the abstract of the paper: The goal of open information extraction (OIE) is to extract surface relations and their arguments from natural language text in an unsupervised, domain-independent manner. In this paper, we explore how overly-specific extractions can be reduced in OIE systems without producing uninformative or inaccurate results. We propose MinIE, an OIE system that produces minimized, annotated extractions. At its heart, MinIE rewrites OIE extractions by (1) identifying and removing parts that are considered overly specific; (2) representing information about polarity, modality, attribution, and quantities with suitable annotations instead of in the actual extraction. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing much shorter extractions. \n\nHere is the partial review: - Strengths:\n\n[+] Well motivated, tackles an interesting problem;\n\n[+] Clearly written and structured, accompanied by documented code and dataset;\n\n[+] Encouraging results.\n\n- Weaknesses:\n\n[-] Limited to completely deterministic, hand-engineered minimization rules;\n\n[-] Some relevant literature on OIE neglected;\n\n[-] Sound but not thorough experimental evaluation.\n\n- General Discussion:\n\nThis paper tackles a practical issue of most OIE systems, i.e. redundant,\nuninformative and inaccurate extractions. The proposed approach, dubbed MinOIE,\nis designed to actually \"minimize\" extractions by removing overly specific\nportions and turning them into structured annotations of various types\n(similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE\nsystem (ClausIE) and test it on two publicly available datasets, showing that\nit effectively leads to more concise extractions compared to standard OIE\napproaches, while at the same time retaining accuracy.\n\nOverall, this work focuses on an interesting (and perhaps underinvestigated)\naspect of OIE in a sound and principled way. The paper is clearly written,\nsufficiently detailed, and accompanied by supplementary material and a neat\nJava implementation.\nMy main concern is, however, with the entirely static, deterministic and\nrule-based structure of MinIE. Even though I understand that a handful of\nmanually engineered rules is technically the best strategy when precision is\nkey, these approaches are typically very hard to scale, e.g. in terms of\nlanguages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al.,\n2016). In other words, I think that this contribution somehow falls short of\nnovelty and substance in proposing a pipeline of engineered rules that are\nmostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance,\nI would have really appreciated an attempt to learn these minimization rules\ninstead of hard-coding them.\n\nFurthermore, the authors completely ignore a recent research thread on\n\u201csemantically-informed\u201d OIE (Nakashole et al., 2012; Moro and Navigli,\n2012; 2013; Delli Bovi et al., 2015) where traditional extractions are\naugmented with links to underlying knowledge bases and sense inventories\n(Wikipedia, Wikidata,  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"- Strengths:\\n\\n[+] Well motivated, tackles an interesting problem;\\n\\n[+] Clearly written and structured, accompanied by documented code and dataset;\\n\\n[+] Encouraging results.\\n\\n- Weaknesses:\\n\\n[-] Limited to completely deterministic, hand-engineered minimization rules;\\n\\n[-] Some relevant literature on OIE neglected;\\n\\n[-] Sound but not thorough experimental evaluation.\\n\\n- General Discussion:\\n\\nThis paper tackles a practical issue of most OIE systems, i.e. redundant, uninformative and inaccurate extractions. The proposed approach, dubbed MinOIE, is designed to actually \\\"minimize\\\" extractions by removing overly specific portions and turning them into structured annotations of various types (similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE system (ClausIE) and test it on two publicly available datasets, showing that it effectively leads to more concise extractions compared to standard OIE approaches, while at the same time retaining accuracy.\\n\\nOverall, this work focuses on an interesting (and perhaps underinvestigated) aspect of OIE in a sound and principled way. The paper is clearly written, sufficiently detailed, and accompanied by supplementary material and a neat Java implementation.\\nMy main concern is, however, with the entirely static, deterministic and rule-based structure of MinIE. Even though I understand that a handful of manually engineered rules is technically the best strategy when precision is key, these approaches are typically very hard to scale, e.g. in terms of languages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al., 2016). In other words, I think that this contribution somehow falls short of novelty and substance in proposing a pipeline of engineered rules that are mostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance, I would have really appreciated an attempt to learn these minimization rules instead of hard-coding them.\\n\\nFurthermore, the authors completely ignore a recent research thread on \u201csemantically-informed\u201d OIE (Nakashole et al., 2012; Moro and Navigli, 2012; 2013; Delli Bovi et al., 2015) where traditional extractions are augmented with links to underlying knowledge bases and sense inventories (Wikipedia, Wikidata, etc.). This would have been an interesting discussion in the context of minimizing extractions while still providing valuable and accurate information.\\n\\nIn conclusion, while the paper addresses an important problem in OIE and provides promising results, the limited scope of the proposed approach and the neglect of relevant literature prevent it from making a stronger contribution. I would recommend further exploration of learning-based approaches to minimization, as well as considering the integration of semantically-informed OIE techniques in future work.\"\n}",
        "human_end_index": 2246
    },
    {
        "id": 588,
        "source": "PeerRead/acl_2017/",
        "title": "Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs",
        "human_review": "- Contents:\nThis paper proposes a new task, and provides a dataset. The task is to predict\nblanked-out named entities from a text with the help of an external\ndefinitional resource, in particular FreeBase. These named entities are\ntypically rare, that is, they do not appear often in the corpus, such that it\nis not possible to train models specifically for each entity. The paper argues\nconvincingly that this is an important setting to explore. Along with multiple\nbaselines, two neural network models for the problem are presented that make\nuse of the external resource, one of which also accumulates evidence across\ncontexts in the same text. \n\n- Strengths:\n\nThe collection of desiderata for the task is well-chosen to advance the field:\npredicting blanked-out named entities, a task that has already shown to be\ninteresting in the CNN/Daily Mail dataset, but in a way that makes the task\nhard for language models; and the focus on rare entities should drive the field\ntowards more interesting models. \n\nThe collection of baselines is well chosen to show that neither a NN model\nwithout external knowledge nor a simple cosine similarity based model with\nexternal knowledge can do the task well.\n\nThe two main models are chosen well.\n\nThe text is clear and well argued. \n\n- Weaknesses:\n\nI was a bit puzzled by the fact that using larger contexts, beyond the\nsentences with blanks in them, did not help the models. After all, you were in\na way using additional context in the HierEnc model, which accumulates\nknowledge from other contexts. There are two possible explanations: Either the\nsentences with blanks in them are across the board more informative for the\ntask than the sentences without. This is the explanation suggested in the\npaper, but it seems a bit unintuitive that this should be the case. Another\npossible explanation is that the way that you were using additional context in\nHierEnc, using the temporal network, is much more useful than by enlarging\nindividual contexts C and feeding that larger C into the recurrent network.  Do\nyou think that that could be what is going on?\n\n- General Discussion:\n\nI particularly like the task and the data that this paper proposes. This setup\ncan really drive the field forward, I think. This in my mind is the main\ncontribution.\n- General Discussion:\n\nThe paper deals with the task of predicting missing entities in a given context\nusing the Freebase definitions of those entities. The authors highlight the\nimportance of the problem, given that the entities come from a long-tailed\ndistribution. They use popular sequence encoders to encode the context and the\ndefinitions of candidate entities, and score them based on their similarity\nwith the context. While it is clear that the task is indeed important, and the\ndataset may be useful as a benchmark, the approach has some serious weaknesses\nand the evaluation leaves some questions unanswered. \n\n- Strengths:\n\nThe proposed task requires encoding external knowledge, and the associated\ndataset may serve as a good benchmark for evaluating hybrid NLU systems.\n\n- Weaknesses:\n\n1) All the models evaluated, except the best performing model (HIERENC), do not\nhave access to contextual information beyond a sentence. This does not seem\nsufficient to predict a missing entity. It is unclear whether any attempts at\ncoreference and anaphora resolution have been made. It would generally help to\nsee how well humans perform at the same task.\n\n2) The choice of predictors used in all models is unusual. It is unclear why\nsimilarity between context embedding and the definition of the entity is a good\nindicator of the goodness of the entity as a filler.\n\n3) The description of HIERENC is unclear. From what I understand, each input\n(h_i) to the temporal network is the average of the representations of all\ninstantiations of context filled by every possible entity in the vocabulary.\nThis does not seem to be a good idea since presumably only one of those\ninstantiations is correct. This would most likely introduce a lot of noise.\n\n4) The results are not very informative. Given that this is a rare entity\nprediction problem, it would help to look at type-level accuracies, and \nanalyze how the accuracies of the proposed models vary with frequencies of\nentities.\n\n- Questions to the authors:\n\n1) An important assumption being made is that d_e are good replacements for\nentity embeddings. Was this assumption tested?\n\n2) Have you tried building a classifier that just takes h_i^e as inputs?\n\nI have read the authors' responses. I still think the task+dataset could\nbenefit from human evaluation. This task can potentially be a good benchmark\nfor NLU systems, if we know how difficult the task is. The results presented in\nthe paper are not indicative of this due to the reasons stated above. Hence, I\nam not changing my scores.\n- Strengths:\nThe paper empirically verifies that using external knowledge is a benefit.\n\n- Weaknesses:\nReal world NLP applications should utilize external knowledge for making better\npredictions. The authors propose Rare Entity prediction task to demonstrate\nthis is the case. However, the motivation of the task is not fully justified.\nWhy is this task important? How would real world NLP applications benefit from\nthis task? The paper lacks a convincing argument for proposing a new task. For\ncurrent reading comprehension task, the evidence for a correct answer can be\nfound in a given text, thus we are interested in learning a model of the world\n(i.e causality for example), or a basic reasoning model. Comparing to reading\ncomprehension, rare entity prediction is rather unrealistic as humans are\nterrible with remembering name. The authors mentioned that the task is\ndifficult due to the large number of rare entities, however challenging tasks\nwith the same or even more difficult level exist, such as predicting correct\nmorphological form of a word in morphologically rich languages. Such tasks have\nobvious applications in machine translation for example.\n\n- General Discussion:\nIt would be helpful if the authors characterize the dataset in more details.\nFrom figure 1 and table 4, it seems to me that overlapping entities is an\nimportant feature. There is noway i can predict the **blank** in figure 1 if I\ndon't see the word London in Peter Ackoyd description. That's being said,\nbefore brutalizing neural networks, it is essential to understand the\ncharacteristic of the data and the cognitive process that searches for the\nright answer.\n\nGiven the lack of characteristic of the dataset, I find that the baselines are\ninappropriate. First of all, the CONTENC is a natural choice at the first sigh.\nHowever as the authors mentioned that candidate entities are rare, the\nembeddings of those entities are unrealizable. As a consequence, it is expected\nthat CONTENC doesn't work well. Would it is fairer if the embeddings are\ninitialized from pre-trained vectors on massive dataset? One would expect some\nsort of similarity between Larnaca and Cyprus in the embedding space and\nCONTENC would make a correct prediction in Table 4. What would be the\nperformance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute\nthose vectors?\n\nFrom modeling perspective, I appreciate that the authors chose a sigmoid\npredictor that output a numerical score between (0,1). This would help avoiding\nnormalization over the list of candidates, which are rare and is difficult to\nlearn reliable weights for those. However, a sidestep technique does exist,\nsuch as Pointer Network. A representation h_i for C_i (*blank* included) can be\ncomputed by an LSTM or BiLSTM, then Pointer Network would give a probabilistic\ninterpretation p(e_k|C_i) \\propto exp(dot(d_{e_k}, h_i)). In my opinion,\nPointer Network would be an appropriate baseline. Another related note: Does\nthe unbalanced set of negative/positive labels affect the training? During\ntraining, the models make 1 positive prediction while number of negative\npredictions is at least 4 times higher?\n\nWhile I find the task of Rare Entity prediction is unrealistic, having the\ndataset, it would be more interesting to learn about the reasoning process that\nleads to the right answer such as which set of words the model attends to when\nmaking prediction.",
        "partial_review": "- Contents:\nThis paper proposes a new task, and provides a dataset. The task is to predict\nblanked-out named entities from a text with the help of an external\ndefinitional resource, in particular FreeBase. These named entities are\ntypically rare, that is, they do not appear often in the corpus, such that it\nis not possible to train models specifically for each entity. The paper argues\nconvincingly that this is an important setting to explore. Along with multiple\nbaselines, two neural network models for the problem are presented that make\nuse of the external resource, one of which also accumulates evidence across\ncontexts in the same text. \n\n- Strengths:\n\nThe collection of desiderata for the task is well-chosen to advance the field:\npredicting blanked-out named entities, a task that has already shown to be\ninteresting in the CNN/Daily Mail dataset, but in a way that makes the task\nhard for language models; and the focus on rare entities should drive the field\ntowards more interesting models. \n\nThe collection of baselines is well chosen to show that neither a NN model\nwithout external knowledge nor a simple cosine similarity based model with\nexternal knowledge can do the task well.\n\nThe two main models are chosen well.\n\nThe text is clear and well argued. \n\n- Weaknesses:\n\nI was a bit puzzled by the fact that using larger contexts, beyond the\nsentences with blanks in them, did not help the models. After all, you were in\na way using additional context in the HierEnc model, which accumulates\nknowledge from other contexts. There are two possible explanations: Either the\nsentences with blanks in them are across the board more informative for the\ntask than the sentences without. This is the explanation suggested in the\npaper, but it seems a bit unintuitive that this should be the case. Another\npossible explanation is that the way that you were using additional context in\nHierEnc, using the temporal network, is much more useful than by enlarging\nindividual contexts C and feeding that larger C into the recurrent network.  Do\nyou think that that could be what is going on?\n\n- General Discussion:\n\nI particularly like the task and the data that this paper proposes. This setup\ncan really drive the field forward, I think. This in my mind is the main\ncontribution.\n- General Discussion:\n\nThe paper deals with the task of predicting missing entities in a given context\nusing the Freebase definitions of those entities. The authors highlight the\nimportance of the problem, given that the entities come from a long-tailed\ndistribution. They use popular sequence encoders to encode the context and the\ndefinitions of candidate entities, and score them based on their similarity\nwith the context. While it is clear that the task is indeed important, and the\ndataset may be useful as a benchmark, the approach has some serious weaknesses\nand the evaluation leaves some questions unanswered. \n\n- Strengths:\n\nThe proposed task requires encoding external knowledge, and the associated\ndataset may serve as a good benchmark for evaluating hybrid NLU systems.\n\n- Weaknesses:\n\n1) All the models evaluated, except the best performing model (HIERENC), do not\nhave access to contextual information beyond a sentence. This does not seem\nsufficient to predict a missing entity. It is unclear whether any attempts at\ncoreference and anaphora resolution have been made. It would generally help to\nsee how well humans perform at the same task.\n\n2) The choice of predictors used in all models is unusual. It is unclear why\nsimilarity between context embedding and the definition of the entity is a good\nindicator of the goodness of the entity as a filler.\n\n3) The description of HIERENC is unclear. From what I understand, each input\n(h_i) to the temporal network is the average of the representations of all\ninstantiations of context filled by every possible entity in the vocabulary.\nThis does not seem to be a good idea since presumably only one of those\ninstantiations is correct. This would most likely introduce a lot of",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs \n\nHere is the abstract of the paper: Reading comprehension in NLP refers to the ability of models to answer any question about a passage accurately. An important open problem is how to effectively use external knowledge to answer such questions. In this paper, we introduce a new task and derive new models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. Our experiments show that models that make use of external knowledge in the form of lexical resources, particularly our model using hierarchical LSTMs, perform significantly better at rare entity prediction than those that do not. \n\nHere is the partial review: - Contents:\nThis paper proposes a new task, and provides a dataset. The task is to predict\nblanked-out named entities from a text with the help of an external\ndefinitional resource, in particular FreeBase. These named entities are\ntypically rare, that is, they do not appear often in the corpus, such that it\nis not possible to train models specifically for each entity. The paper argues\nconvincingly that this is an important setting to explore. Along with multiple\nbaselines, two neural network models for the problem are presented that make\nuse of the external resource, one of which also accumulates evidence across\ncontexts in the same text. \n\n- Strengths:\n\nThe collection of desiderata for the task is well-chosen to advance the field:\npredicting blanked-out named entities, a task that has already shown to be\ninteresting in the CNN/Daily Mail dataset, but in a way that makes the task\nhard for language models; and the focus on rare entities should drive the field\ntowards more interesting models. \n\nThe collection of baselines is well chosen to show that neither a NN model\nwithout external knowledge nor a simple cosine similarity based model with\nexternal knowledge can do the task well.\n\nThe two main models are chosen well.\n\nThe text is clear and well argued. \n\n- Weaknesses:\n\nI was a bit puzzled by the fact that using larger contexts, beyond the\nsentences with blanks in them, did not help the models. After all, you were in\na way using additional context in the HierEnc model, which accumulates\nknowledge from other contexts. There are two possible explanations: Either the\nsentences with blanks in them are across the board more informative for the\ntask than the sentences without. This is the explanation suggested in the\npaper, but it seems a bit unintuitive that this should be the case. Another\npossible explanation is that the way that you were using additional context in\nHierEnc, using the temporal network, is much more useful than by enlarging\nindividual contexts C and feeding that larger C into the recurrent network.  Do\nyou think that that could be what is going on?\n\n- General Discussion:\n\nI particularly like the task and the data that this paper proposes. This setup\ncan really drive the field forward, I think. This in my mind is the main\ncontribution.\n- General Discussion:\n\nThe paper deals with the task of predicting missing entities in a given context\nusing the Freebase definitions of those entities. The authors highlight the\nimportance of the problem, given that the entities come from a long-tailed\ndistribution. They use popular sequence encoders to encode the context and the\ndefinitions of candidate entities, and score them based on their similarity\nwith the context. While it is clear that the task is indeed important, and the\ndataset may be useful as a benchmark, the approach has some serious weaknesses\nand the evaluation leaves some questions unanswered. \n\n- Strengths:\n\nThe proposed task requires encoding external knowledge, and the associated\ndataset may serve as a good benchmark for evaluating hybrid NLU systems.\n\n- Weaknesses:\n\n1) All the models evaluated, except the best performing model (HIERENC), do not\nhave access to contextual information beyond a sentence. This does not seem\nsufficient to predict a missing entity. It is unclear whether any attempts at\ncoreference and anaphora resolution have been made. It would generally help to\nsee how well humans perform at the same task.\n\n2) The choice of predictors used in all models is unusual. It is unclear why\nsimilarity between context embedding and the definition of the entity is a good\nindicator of the goodness of the entity as a filler.\n\n3) The description of HIERENC is unclear. From what I understand, each input\n(h_i) to the temporal network is the average of the representations of all\ninstantiations of context filled by every possible entity in the vocabulary.\nThis does not seem to be a good idea since presumably only one of those\ninstantiations is correct. This would most likely introduce a lot of \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper proposes a new task of predicting rare entities in a given context using external knowledge and provides a dataset for evaluation. The selection of desiderata and baselines is appropriate and demonstrates the importance of external knowledge for the task. The two main models are well-chosen, but it is unclear why larger contexts did not improve model performance. The overall clarity and argumentation of the paper is commendable. However, there are weaknesses in the approach and evaluation. Firstly, the models except HIERENC lack access to sufficient contextual information, and it is unclear if attempts at coreference and anaphora resolution were made. Secondly, the choice of predictors used in the models is unusual and the rationale behind using similarity as a measure of entity goodness is unclear. Lastly, the description of HIERENC is confusing, and the use of average representations of all instantiations may introduce noise. Despite these weaknesses, the proposed task and dataset have significant potential for advancing the field of NLU.\"\n}",
        "human_end_index": 4027
    },
    {
        "id": 606,
        "source": "PeerRead/acl_2017/",
        "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision",
        "human_review": "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\u201cprogrammer\u201d) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a \u2018key variable\u2019 memory\ncomponent which stores (a) entities in the questions (b) values of intermediate\nvariables formed during execution of intermediate programs. These variables are\nreferred to further build the program.                    The model is also equipped\nwith\ncertain\ndiscrete operations (such as argmax or 'hop to next edges in a KB'). A separate\ncomponent (\"interpreter/computer\") executes these operations and stores\nintermediate values (as explained before). Since the \u2018programmer' is\ninherently a seq2seq model, the \"interpreter/computer\u201d also acts as a\nsyntax/type checker only allowing the decoder to generate valid tokens. For\nexample, the second argument to the \u201chop\u201d operation has to be a KB\npredicate. Finally the model is trained with weak supervision and directly\noptimizes the metric which is used to evaluate the performance (F score).\nBecause of the discrete operations and the non differentiable reward functions,\nthe model is trained with policy gradients (REINFORCE). Since gradients\nobtained through REINFORCE have high variance, it is common to first pretrain\nthe model with a max-likelihood objective or find some good sequences of\nactions trained through some auxiliary objective. This paper takes a latter\napproach in which it finds good sequences via an iterative maximum likelihood\napproach. The results and discussion sections are presented in a very nice way\nand the model achieves SOTA results on the WebQuestions dataset when compared\nto other weakly supervised model.\n\nThe paper is written clearly and is very easy to follow.\n\nThis paper presents a new and exciting direction and there is scope for a lot\nof future research in this direction. I would definitely love to see this\npresented in the conference.\n\nQuestions for the authors (important ones first)\n\n1. Another alternative way of training the model would be to bootstrap the\nparameters (\\theta) from the iterative ML method instead of adding pseudo gold\nprograms in the beam (Line 510 would be deleted). Did you try that and if so\nwhy do you think it didn\u2019t work?\n2. What was the baseline model in REINFORCE. Did you have a separate network\nwhich predicts the value function. This must be discussed in the paper in\ndetail.\n3. Were there programs which required multiple hop operations? Or were they\nlimited to single hops. If there were, can you provide an example? (I will\nunderstand if you are bound by word limit of the response)\n4. Can you give an example where the filter operation would be used?\n5. I did not follow the motivation behind replacing the entities in the\nquestion with special ENT symbol\n\nMinor comments:\nLine 161 describe -> describing\nLine 318 decoder reads \u2018)\u2019 -> decoder generates \u2018)'\nThis paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model\nequipped with discrete memory to facilitate symbolic execution. An NSM includes\nthree components: (1) a manager that provides weak supervision for learning,\n(2) a differentiable programmer based on neural sequence to sequence model,\nwhich encodes input instructions and predicts simplified Lisp programs using\npartial execution results stored in external discrete memories. (3) a symbolic\ncomputer that executes programs and provide code assistance to the programmer\nto prune search space. The authors conduct experiments on a semantic parsing\ntask (WebQuestionsSP), and show that (1) NSM is able to model language\ncompositionality by saving and reusing intermediate execution results, (2)\nAugmented REINFORCE is superior than vanilla REINFROCE for sequence prediction\nproblems, and (3) NSM trained end-to-end with weak supervision is able to\noutperform existing sate-of-the-art method (STAGG).\n\n- Strengths\n\n* The idea of using discrete, symbolic memories for neural execution models is\nnovel.                    Although in implementation it may simply reduce to copying\npreviously\nexecuted variable tokens from an extra buffer, this approach is still\nimpressive since it works well for a large-scale semantic parsing task.\n\n* The proposed revised REINFORCE training schema using imperfect hypotheses\nderived from maximum likelihood training is interesting and effective, and\ncould inspire future exploration in mixing ML/RL training for neural\nsequence-to-sequence models.\n\n* The scale of experiments is larger than any previous works in modeling neural\nexecution and program induction. The results are impressive.\n\n* The paper is generally clear and well-written, although there are some points\nwhich might require further clarification (e.g., how do the keys ($v_i$'s in\nFig. 2) of variable tokens involved in computing action probabilities?\nConflicting notations: $v$ is used to refer to variables in Tab. 1 and memory\nkeys in Fig 1.).\n\nOverall, I like this paper and would like to see it in the conference.\n\n* Weaknesses\n\n* [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not\nusing the most popular WebQuestions (Berant et al., 2013) benchmark set? Since\nNSM only requires weak supervision, using WebQuestions would be more intuitive\nand straightforward, plus it could facilitate direct comparison with\nmain-stream QA research.\n\n* [Analysis of Compositionality] One of the contribution of this work is the\nusage of symbolic intermediate execution results to facilitate modeling\nlanguage compositionality. One interesting question is how well questions with\nvarious compositional depth are handled. Simple one-hop questions are the\neasiest to solve, while complex multi-hop ones that require filtering and\nsuperlative operations (argmax/min) would be highly non-trivial. The authors\nshould present detailed analysis regarding the performance on question sets\nwith different compositional depth.\n\n* [Missing References] I find some relevant papers in this field missing. For\nexample, the authors should cite previous RL-based methods for knowledge-based\nsemantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE\ntraining method of (Ranzato et al., 2016) which is closely related to augmented\nREINFORCE, and the neural enquirer work (Yin et al., 2016) which uses\ncontinuous differentiable memories for modeling neural execution.\n\n* Misc.\n\n* Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of\nusing parameters pre-trained with iterative ML?\n\n* What is KG server in Figure 5?",
        "partial_review": "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\u201cprogrammer\u201d) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a \u2018key variable\u2019 memory\ncomponent which stores (a) entities in the questions (b) values of intermediate\nvariables formed during execution of intermediate programs. These variables are\nreferred to further build the program.                    The model is also equipped\nwith\ncertain\ndiscrete operations (such as argmax or 'hop to next edges in a KB'). A separate\ncomponent (\"interpreter/computer\") executes these operations and stores\nintermediate values (as explained before). Since the \u2018programmer' is\ninherently a seq2seq model, the \"interpreter/computer\u201d also acts as a\nsyntax/type checker only allowing the decoder to generate valid tokens. For\nexample, the second argument to the \u201chop\u201d operation has to be a KB\npredicate. Finally the model is trained with weak supervision and directly\noptimizes the metric which is used to evaluate the performance (F score).\nBecause of the discrete operations and the non differentiable reward functions,\nthe model is trained with policy gradients (REINFORCE). Since gradients\nobtained through REINFORCE have high variance, it is common to first pretrain\nthe model with a max-likelihood objective or find some good sequences of\nactions trained through some auxiliary objective. This paper takes a latter\napproach in which it finds good sequences via an iterative maximum likelihood\napproach. The results and discussion sections are presented in a very nice way\nand the model achieves SOTA results on the WebQuestions dataset when compared\nto other weakly supervised model.\n\nThe paper is written clearly and is very easy to follow.\n\nThis paper presents a new and exciting direction and there is scope for a lot\nof future research in this direction. I would definitely love to see this\npresented in the conference.\n\nQuestions for the authors (important ones first)\n\n1. Another alternative way of training the model would be to bootstrap the\nparameters (\\theta) from the iterative ML method instead of adding pseudo gold\nprograms in the beam (Line 510 would be deleted). Did you try that and if so\nwhy do you think it didn\u2019t work?\n2. What was the baseline model in REINFORCE. Did you have a separate network\nwhich predicts the value function. This must be discussed in the paper in\ndetail.\n3. Were there programs which required multiple hop operations? Or were they\nlimited to single hops. If there were, can you provide an example? (I will\nunderstand if you are bound by word limit of the response)\n4. Can you give an example where the filter operation would be used?\n5. I did not follow the motivation behind replacing the entities in the\nquestion with special ENT symbol\n\nMinor comments:\nLine 161 describe -> describing\nLine 318 decoder reads \u2018)\u2019 -> decoder generates \u2018)'\nThis paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model\nequipped with discrete memory to facilitate symbolic execution. An NSM includes\nthree components: (1) a manager that provides weak supervision for learning,\n(2) a differentiable programmer based on neural sequence to sequence model,\nwhich encodes input instructions and predicts simplified Lisp programs using\npartial exe",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision \n\nHere is the abstract of the paper: Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge. \n\nHere is the partial review: This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n\u201cprogrammer\u201d) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a \u2018key variable\u2019 memory\ncomponent which stores (a) entities in the questions (b) values of intermediate\nvariables formed during execution of intermediate programs. These variables are\nreferred to further build the program.                    The model is also equipped\nwith\ncertain\ndiscrete operations (such as argmax or 'hop to next edges in a KB'). A separate\ncomponent (\"interpreter/computer\") executes these operations and stores\nintermediate values (as explained before). Since the \u2018programmer' is\ninherently a seq2seq model, the \"interpreter/computer\u201d also acts as a\nsyntax/type checker only allowing the decoder to generate valid tokens. For\nexample, the second argument to the \u201chop\u201d operation has to be a KB\npredicate. Finally the model is trained with weak supervision and directly\noptimizes the metric which is used to evaluate the performance (F score).\nBecause of the discrete operations and the non differentiable reward functions,\nthe model is trained with policy gradients (REINFORCE). Since gradients\nobtained through REINFORCE have high variance, it is common to first pretrain\nthe model with a max-likelihood objective or find some good sequences of\nactions trained through some auxiliary objective. This paper takes a latter\napproach in which it finds good sequences via an iterative maximum likelihood\napproach. The results and discussion sections are presented in a very nice way\nand the model achieves SOTA results on the WebQuestions dataset when compared\nto other weakly supervised model.\n\nThe paper is written clearly and is very easy to follow.\n\nThis paper presents a new and exciting direction and there is scope for a lot\nof future research in this direction. I would definitely love to see this\npresented in the conference.\n\nQuestions for the authors (important ones first)\n\n1. Another alternative way of training the model would be to bootstrap the\nparameters (\\theta) from the iterative ML method instead of adding pseudo gold\nprograms in the beam (Line 510 would be deleted). Did you try that and if so\nwhy do you think it didn\u2019t work?\n2. What was the baseline model in REINFORCE. Did you have a separate network\nwhich predicts the value function. This must be discussed in the paper in\ndetail.\n3. Were there programs which required multiple hop operations? Or were they\nlimited to single hops. If there were, can you provide an example? (I will\nunderstand if you are bound by word limit of the response)\n4. Can you give an example where the filter operation would be used?\n5. I did not follow the motivation behind replacing the entities in the\nquestion with special ENT symbol\n\nMinor comments:\nLine 161 describe -> describing\nLine 318 decoder reads \u2018)\u2019 -> decoder generates \u2018)'\nThis paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model\nequipped with discrete memory to facilitate symbolic execution. An NSM includes\nthree components: (1) a manager that provides weak supervision for learning,\n(2) a differentiable programmer based on neural sequence to sequence model,\nwhich encodes input instructions and predicts simplified Lisp programs using\npartial exe \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper introduces a new approach to semantic parsing using a Neural Symbolic Machine (NSM) that consists of a neural programmer and a symbolic computer. The neural programmer is a sequence-to-sequence model that maps language utterances to programs, and it utilizes a key-variable memory for compositionality. The symbolic computer is a Lisp interpreter that performs program execution and helps find good programs by pruning the search space. The authors apply REINFORCE to optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, they augment it with an iterative maximum-likelihood training process. The authors demonstrate that NSM outperforms the state-of-the-art on the WebQuestionsSP dataset without requiring any feature engineering or domain-specific knowledge. The paper is well-written and easy to follow. The results are presented in a clear manner. This work opens up new directions for future research. The authors also propose some interesting alternatives for training the model and raise important questions about the model and its components. However, there are a few minor comments to address in the text.\",\n}",
        "human_end_index": 3387
    },
    {
        "id": 614,
        "source": "PeerRead/acl_2017/",
        "title": "Clustering Paraphrases for Substitutability",
        "human_review": "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task by using these inventories to filter\ncandidates. To do so, the authors first propose a metric to measure the mutual\nsubstitutability of sense inventories with human judgments for the lexsub task,\nand empirically measure the substitutability of inventories from various\nsources such as WordNet and PPDB. Next, they propose clustering different\nparaphrases of a word from PPDB using a multi-view clustering approach, to\nautomatically generate a sense inventory instead of using the aforementioned\ninventories. Finally, they use these clusters with a naive (majority in top 5)\nWSD technique to filter existing ranked list of substitution candidates.\n\n- Strengths:\n\n* The key idea of marrying vector space model based approaches and sense\ninventories for the lexsub task is useful since these two techniques seem to\nhave complementary information, especially since the vector space models are\ntypically unaware of sense and polysemy.\n\n* The oracle evaluation is interesting as it gives a clear indication of how\nmuch gain can one expect in the best case, and while there is still a large gap\nbetween the oracle and actual scores, we can still argue for the usefulness of\nthe proposed approach due to the large difference between the unfiltered GAP\nand the oracle GAP.\n\n- Weaknesses:\n\n* I don't understand effectiveness of the multi-view clustering approach.\nAlmost all across the board, the paraphrase similarity view does significantly\nbetter than other views and their combination. What, then, do we learn about\nthe usefulness of the other views? There is one empirical example of how the\ndifferent views help in clustering paraphrases of the word 'slip', but there is\nno further analysis about how the different clustering techniques differ,\nexcept on the task directly. Without a more detailed analysis of differences\nand similarities between these views, it is hard to draw solid conclusions\nabout the different views.                                  \n\n* The paper is not fully clear on a first read. Specifically, it is not\nimmediately clear how the sections connect to each other, reading more like\ndisjoint pieces of work. For instance, I did not understand the connections\nbetween section 2.1 and section 4.3, so adding forward/backward pointer\nreferences to sections should be useful in clearing up things. Relatedly, the\nmulti-view clustering section (3.1) needs editing, since the subsections seem\nto be out of order, and citations seem to be missing (lines 392 and 393).\n\n* The relatively poor performance on nouns makes me uneasy. While I can expect\nTWSI to do really well due to its nature, the fact that the oracle GAP for\nPPDBClus is higher than most clustering approaches is disconcerting, and I\nwould like to understand the gap better. This also directly contradicts the\nclaim that the clustering approach is generalizable to all parts of speech\n(124-126), since the performance clearly isn't uniform.\n\n- General Discussion:\n\nThe paper is mostly straightforward in terms of techniques used and\nexperiments. Even then, the authors show clear gains on the lexsub task by\ntheir two-pronged approach, with potentially more to be gained by using\nstronger WSD algorithms.\n\nSome additional questions for the authors :\n\n* Lines 221-222 : Why do you add hypernyms/hyponyms?\n* Lines 367-368 : Why does X^{P} need to be symmetric?\n* Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed\narbitrary or is this a principled choice?\n* Is the high performance of SubstClus^{P} ascribable to the fact that the\nnumber of clusters was tuned based on this view? Would tuning the number of\nclusters based on other matrices affect the results and the conclusions?\n* What other related tasks could this approach possibly generalize to? Or is it\nonly specific to lexsub?\nStrengths:\nThe paper presents a new method that exploits word senses to improve the task\nof lexical substitutability.  Results show improvements over prior methods.\n\nWeaknesses:\nAs a reader of a ACL paper, I usually ask myself what important insight can I\ntake away from the paper, and from a big picture point of view, what does the\npaper add to the fields of natural language processing and computational\nlinguistics.  How does the task of lexical substitutability in general and this\npaper in particular help either in improving an NLP system or provide insight\nabout language?  I can't find a good answer answer to either question after\nreading this paper.\n\nAs a practitioner who wants to improve natural language understanding system, I\nam more focused on the first question -- does the lexical substitutability task\nand the improved results compared to prior work presented here help any end\napplication?  Given the current state of high performing systems, any discrete\nclustering of words (or longer utterances) often break down when compared to\ncontinuous representations words (see all the papers that utilitize discrete\nlexical semantics to achieve a task versus words' distributed representations\nused as an input to the same task; e.g. machine translation, question\nanswering, sentiment analysis, text classification and so forth).  How do the\nauthors motivate work on lexical substitutability given that discrete lexical\nsemantic representations often don't work well?  The introduction cites a few\npapers from several years back that are mostly set up in small data scenarios,\nand given that this word is based on English, I don't see why one would use\nthis method for any task.  I would be eager to see the authors' responses to\nthis general question of mine.\n\nAs a minor point, to further motivate this, consider the substitutes presented\nin Table 1.\n1. Tasha snatched it from him to rip away the paper.\n2. Tasha snatched it from him to rip away the sheet.\n\nTo me, these two sentences have varying meanings -- what if he was holding on\nto a paper bag?  In that scenario, can the word \"paper\" be substituted by\n\"sheet\"?  At least, in my understanding, it cannot.  Hence, there is so much\nsubjectivity in this task that lexical substitutes can completely alter the\nsemantics of the original sentence.\n\nMinor point(s):\n - Citations in Section 3.1.4 are missing.\n\nAddition: I have read the author response and I am sticking to my earlier\nevaluation of the paper.",
        "partial_review": "This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task by using these inventories to filter\ncandidates. To do so, the authors first propose a metric to measure the mutual\nsubstitutability of sense inventories with human judgments for the lexsub task,\nand empirically measure the substitutability of inventories from various\nsources such as WordNet and PPDB. Next, they propose clustering different\nparaphrases of a word from PPDB using a multi-view clustering approach, to\nautomatically generate a sense inventory instead of using the aforementioned\ninventories. Finally, they use these clusters with a naive (majority in top 5)\nWSD technique to filter existing ranked list of substitution candidates.\n\n- Strengths:\n\n* The key idea of marrying vector space model based approaches and sense\ninventories for the lexsub task is useful since these two techniques seem to\nhave complementary information, especially since the vector space models are\ntypically unaware of sense and polysemy.\n\n* The oracle evaluation is interesting as it gives a clear indication of how\nmuch gain can one expect in the best case, and while there is still a large gap\nbetween the oracle and actual scores, we can still argue for the usefulness of\nthe proposed approach due to the large difference between the unfiltered GAP\nand the oracle GAP.\n\n- Weaknesses:\n\n* I don't understand effectiveness of the multi-view clustering approach.\nAlmost all across the board, the paraphrase similarity view does significantly\nbetter than other views and their combination. What, then, do we learn about\nthe usefulness of the other views? There is one empirical example of how the\ndifferent views help in clustering paraphrases of the word 'slip', but there is\nno further analysis about how the different clustering techniques differ,\nexcept on the task directly. Without a more detailed analysis of differences\nand similarities between these views, it is hard to draw solid conclusions\nabout the different views.                                  \n\n* The paper is not fully clear on a first read. Specifically, it is not\nimmediately clear how the sections connect to each other, reading more like\ndisjoint pieces of work. For instance, I did not understand the connections\nbetween section 2.1 and section 4.3, so adding forward/backward pointer\nreferences to sections should be useful in clearing up things. Relatedly, the\nmulti-view clustering section (3.1) needs editing, since the subsections seem\nto be out of order, and citations seem to be missing (lines 392 and 393).\n\n* The relatively poor performance on nouns makes me uneasy. While I can expect\nTWSI to do really well due to its nature, the fact that the oracle GAP for\nPPDBClus is higher than most clustering approaches is disconcerting, and I\nwould like to understand the gap better. This also directly contradicts the\nclaim that the clustering approach is generalizable to all parts of speech\n(124-126), since the performance clearly isn't uniform.\n\n- General Discussion:\n\nThe paper is mostly straightforward in terms of techniques used and\nexperiments. Even then, the authors show clear gains on the lexsub task by\ntheir two-pronged approach, with pote",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Clustering Paraphrases for Substitutability \n\nHere is the abstract of the paper: Current state-of-the-art models for lexical substitution -- the task of nominating substitutes for a word in context -- ignore word sense, instead relying on powerful vector and embedded word representations to find good substitutes. We present a simple method for improving the lexical substitution rankings of existing models by integrating word sense inventories, filtering substitutes from the correct sense to the top of the rankings. To enable maximum coverage of our method, we also propose a novel method for clustering paraphrases by word sense with substitutability in mind. Our method results in sense clusters that are more substitutable and have wider coverage than existing sense inventories. They can be applied as a filter over lexical substitution rankings generated by existing vector- and embedding-based ranking models to significantly improve their performance. \n\nHere is the partial review: This paper proposes integrating word sense inventories into existing approaches\nfor the lexical substitution task by using these inventories to filter\ncandidates. To do so, the authors first propose a metric to measure the mutual\nsubstitutability of sense inventories with human judgments for the lexsub task,\nand empirically measure the substitutability of inventories from various\nsources such as WordNet and PPDB. Next, they propose clustering different\nparaphrases of a word from PPDB using a multi-view clustering approach, to\nautomatically generate a sense inventory instead of using the aforementioned\ninventories. Finally, they use these clusters with a naive (majority in top 5)\nWSD technique to filter existing ranked list of substitution candidates.\n\n- Strengths:\n\n* The key idea of marrying vector space model based approaches and sense\ninventories for the lexsub task is useful since these two techniques seem to\nhave complementary information, especially since the vector space models are\ntypically unaware of sense and polysemy.\n\n* The oracle evaluation is interesting as it gives a clear indication of how\nmuch gain can one expect in the best case, and while there is still a large gap\nbetween the oracle and actual scores, we can still argue for the usefulness of\nthe proposed approach due to the large difference between the unfiltered GAP\nand the oracle GAP.\n\n- Weaknesses:\n\n* I don't understand effectiveness of the multi-view clustering approach.\nAlmost all across the board, the paraphrase similarity view does significantly\nbetter than other views and their combination. What, then, do we learn about\nthe usefulness of the other views? There is one empirical example of how the\ndifferent views help in clustering paraphrases of the word 'slip', but there is\nno further analysis about how the different clustering techniques differ,\nexcept on the task directly. Without a more detailed analysis of differences\nand similarities between these views, it is hard to draw solid conclusions\nabout the different views.                                  \n\n* The paper is not fully clear on a first read. Specifically, it is not\nimmediately clear how the sections connect to each other, reading more like\ndisjoint pieces of work. For instance, I did not understand the connections\nbetween section 2.1 and section 4.3, so adding forward/backward pointer\nreferences to sections should be useful in clearing up things. Relatedly, the\nmulti-view clustering section (3.1) needs editing, since the subsections seem\nto be out of order, and citations seem to be missing (lines 392 and 393).\n\n* The relatively poor performance on nouns makes me uneasy. While I can expect\nTWSI to do really well due to its nature, the fact that the oracle GAP for\nPPDBClus is higher than most clustering approaches is disconcerting, and I\nwould like to understand the gap better. This also directly contradicts the\nclaim that the clustering approach is generalizable to all parts of speech\n(124-126), since the performance clearly isn't uniform.\n\n- General Discussion:\n\nThe paper is mostly straightforward in terms of techniques used and\nexperiments. Even then, the authors show clear gains on the lexsub task by\ntheir two-pronged approach, with pote \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"ntial to significantly improve the performance of existing vector- and embedding-based ranking models. The key idea of integrating word sense inventories with lexical substitution models is a useful one, as it takes advantage of the complementary information provided by these approaches. The oracle evaluation provides a clear indication of the potential improvement that can be achieved. However, there are some weaknesses in the paper. The effectiveness of the multi-view clustering approach is not well-explained, as the paraphrase similarity view consistently outperforms the other views. The lack of analysis on the differences between the views makes it difficult to draw solid conclusions about their usefulness. Additionally, the paper could benefit from clearer connections between the sections and better organization. The relatively poor performance on nouns raises concerns about the generalizability of the proposed clustering approach. Overall, the paper presents a promising approach for improving lexical substitution rankings, but further clarification and analysis are needed.\"\n}",
        "human_end_index": 3236
    },
    {
        "id": 619,
        "source": "PeerRead/acl_2017/",
        "title": "A Corpus of Annotated Revisions for Studying Argumentative Writing",
        "human_review": "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the corpus:\n\n1) Student Revision Behavior Analysis and 2) Automatic Revision Identification\n\nThe latter is essentially a text classification task using an SVM classifier\nand a variety of features. The authors state that the corpus will be freely\navailable for research purposes.\n\nThe paper is well-written and clear. A detailed annotation scheme was used by\ntwo\nannotators to annotate the corpus which added value to it. I believe the\nresource might be interesting to researcher working on writing process research\nand related topics. I also liked that you provided two very clear usage\nscenarios for the corpus. \n\nI have two major criticisms. The first could be easily corrected in case the\npaper is accepted, but the second requires more work.\n\n1) There are no statistics about the corpus in this paper. This is absolutely\nparamount. When you describe a corpus, there are some information that should\nbe there. \n\nI am talking about number of documents (I assume the corpus has 180 documents\n(60 essays x 3 drafts), is that correct?), number of tokens (around 400 words\neach essay?), number of sentences, etc. \n\nI assume we are talking about 60 unique essays x 400 words, so about 24,000\nwords in total. Is that correct? If we take the 3 drafts we end up with about\n72,000 words but probably with substantial overlap between drafts.\n\nA table with this information should be included in the paper.\n\n2) If the aforementioned figures are correct, we are talking about a very small\ncorpus. I understand the difficulty of producing hand-annotated data, and I\nthink this is one of the strengths of your work, but I am not sure about how\nhelpful this resource is for the NLP community as a whole. Perhaps such a\nresource would be better presented in a specialised workshop such as BEA or a\nspecialised conference on language resources like LREC instead of a general NLP\nconference like ACL.\n\nYou mentioned in the last paragraph that you would like to augment the corpus\nwith more annotation. Are you also willing to include more essays?\n\nComments/Minor:\n\n- As you have essays by native and non-native speakers, one further potential\napplication of this corpus is native language identification (NLI).\n\n- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\".\nBe more specific.\n\n- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant.",
        "partial_review": "This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the corpus:\n\n1) Student Revision Behavior Analysis and 2) Automatic Revision Identification\n\nThe latter is essentially a text classification task using an SVM classifier\nand a variety of features. The authors state that the corpus will be freely\navailable for research purposes.\n\nThe paper is well-written and clear. A detailed annotation scheme was used by\ntwo\nannotators to annotate the corpus which added value to it. I believe the\nresource might be interesting to researcher working on writing process research\nand related topics. I also liked that you provided two very clear usage\nscenarios for the corpus. \n\nI have two major criticisms. The first could be easily corrected in case the\npaper is accepted, but the second requires more work.\n\n1) There are no statistics about the corpus in this paper. This is absolutely\nparamount. When you describe a corpus, there are some information that should\nbe there. \n\nI am talking about number of documents (I assume the corpus has 180 documents\n(60 essays x 3 drafts), is that correct?), number of tokens (around 400 words\neach essay?), numbe",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A Corpus of Annotated Revisions for Studying Argumentative Writing \n\nHere is the abstract of the paper: This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer\u2019s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction. \n\nHere is the partial review: This paper presents a corpus of annotated essay revisions. \n\nIt includes two examples of application for the corpus:\n\n1) Student Revision Behavior Analysis and 2) Automatic Revision Identification\n\nThe latter is essentially a text classification task using an SVM classifier\nand a variety of features. The authors state that the corpus will be freely\navailable for research purposes.\n\nThe paper is well-written and clear. A detailed annotation scheme was used by\ntwo\nannotators to annotate the corpus which added value to it. I believe the\nresource might be interesting to researcher working on writing process research\nand related topics. I also liked that you provided two very clear usage\nscenarios for the corpus. \n\nI have two major criticisms. The first could be easily corrected in case the\npaper is accepted, but the second requires more work.\n\n1) There are no statistics about the corpus in this paper. This is absolutely\nparamount. When you describe a corpus, there are some information that should\nbe there. \n\nI am talking about number of documents (I assume the corpus has 180 documents\n(60 essays x 3 drafts), is that correct?), number of tokens (around 400 words\neach essay?), numbe \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a corpus of annotated essay revisions. It includes two examples of application for the corpus: 1) Student Revision Behavior Analysis and 2) Automatic Revision Identification. The latter is essentially a text classification task using an SVM classifier and a variety of features. The authors state that the corpus will be freely available for research purposes. The paper is well-written and clear. A detailed annotation scheme was used by two annotators to annotate the corpus which added value to it. I believe the resource might be interesting to researchers working on writing process research and related topics. I also liked that you provided two very clear usage scenarios for the corpus. However, I have two major criticisms. The first could be easily corrected in case the paper is accepted, but the second requires more work. 1) There are no statistics about the corpus in this paper. This is absolutely paramount. When you describe a corpus, there is some information that should be there. I am talking about the number of documents (I assume the corpus has 180 documents (60 essays x 3 drafts), is that correct?), the number of tokens (around 400 words each essay?), and the number... \"\n}",
        "human_end_index": 1195
    },
    {
        "id": 627,
        "source": "PeerRead/acl_2017/",
        "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access",
        "human_review": "This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly optimised using the reinforce algorithm. It learns from\ninteraction with a user simulator. There are two training phases. The first is\nan imitation learning phase where the system is initialised using supervising\nlearning from a rule-based model. Then there is a reinforcement learning phase\nwhere the system has jointly been optimised using the RL objective.\n\n- Strengths: This paper presents a framework where a differentiable access to\nthe KB is integrated in the joint optimisation. This is the biggest\ncontribution of the paper. \n\n- Weaknesses: Firstly, this is not a truly end-to-end system considering the\nresponse generation was handcrafted rather than learnt. Also, their E2E model\nactually overfits to the simulator and performs poorly in human evaluation.\nThis begs the question whether the authors are actually selling the idea of E2E\nlearning or the soft-KB access. The soft-KB access actually brings consistent\nimprovement, however the idea of end-to-end learning not so much. The authors\ntried to explain the merits of E2E in Figure 5 but I also fail to see the\ndifference. In addition, the authors didn't motivate the reason for using the\nreinforce algorithm which is known to suffer from high variance problem. They\ndidn't attempt to improve it by using a baseline or perhaps considering the\nnatural actor-critic algorithm which is known to perform better.\n\n- General Discussion: Apart from the mentioned weaknesses, I think the\nexperiments are solid and this is generally an acceptable paper. However, if\nthey crystallised the paper around the idea which actually improves the\nperformance (the soft KB access) but not the idea of E2E learning the paper\nwould be better.",
        "partial_review": "This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly optimised using the reinforce algorithm. It learns from\ninteraction with a user simulator. There are two training phases. The first is\nan imitation learning phase where the system is initialised using supervising\nlearning from a rule-based model. Then there is a reinforcement learning phase\nwhere the system has jointly been optimised using the RL objective.\n\n- Strengths: This paper presents a framework where a differentiable access to\nthe KB is integrated in the joint optimisation. This is the biggest\ncontribution of the paper. \n\n- Weaknesses: Firstly, this is not a truly end-to-end system considering the\nresponse generation was handcrafted rather than learnt. Also, their E2E model\nactually overfits to the simulator and performs poorly in human ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access \n\nHere is the abstract of the paper: This paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ``soft'' posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. \n\nHere is the partial review: This paper presents a dialogue agent where the belief tracker and the dialogue\nmanager are jointly optimised using the reinforce algorithm. It learns from\ninteraction with a user simulator. There are two training phases. The first is\nan imitation learning phase where the system is initialised using supervising\nlearning from a rule-based model. Then there is a reinforcement learning phase\nwhere the system has jointly been optimised using the RL objective.\n\n- Strengths: This paper presents a framework where a differentiable access to\nthe KB is integrated in the joint optimisation. This is the biggest\ncontribution of the paper. \n\n- Weaknesses: Firstly, this is not a truly end-to-end system considering the\nresponse generation was handcrafted rather than learnt. Also, their E2E model\nactually overfits to the simulator and performs poorly in human  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a dialogue agent where the belief tracker and the dialogue  manager are jointly optimised using the reinforce algorithm. It learns from interaction with a user simulator. There are two training phases. The first is an imitation learning phase where the system is initialised using supervising learning from a rule-based model. Then there is a reinforcement learning phase where the system has jointly been optimised using the RL objective.\\n\\n- Strengths: This paper presents a framework where a differentiable access to the KB is integrated in the joint optimisation. This is the biggest contribution of the paper.\\n\\n- Weaknesses: Firstly, this is not a truly end-to-end system considering the response generation was handcrafted rather than learnt. Also, their E2E model actually overfits to the simulator and performs poorly in human evaluation. In addition, the paper could benefit from providing more details on the experimental setup and results. Overall, while the integration of a differentiable access to the KB is a significant contribution, there are still limitations and areas for improvement in the paper.\"\n}",
        "human_end_index": 854
    },
    {
        "id": 636,
        "source": "PeerRead/acl_2017/",
        "title": "Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions",
        "human_review": "This work proposes to apply dilated convolutions for sequence tagging\n(specifically, named entity recognition). It also introduces some novel ideas\n(sharing the dilated convolution block, predicting the tags at each convolution\nlevel), which I think will prove useful to the community. The paper performs\nextensive ablation experiments to show the effectiveness of their approach.\nI found the writing to be very clear, and the experiments were exceptionally\nthorough.\n\nStrengths:  \n- Extensive experiments against various architectures (LSTM, LSTM + CRF)       \n- Novel architectural/training ideas (sharing blocks)  \n\nWeaknesses:  \n- Only applied to English NER--this is a big concern since the title of the\npaper seems to reference sequence-tagging directly.  \n- Section 4.1 could be clearer. For example, I presume there is padding to make\nsure the output resolution after each block is the same as the input\nresolution.  Might be good to mention this.  \n- I think an ablation study of number of layers vs perf might be interesting.\n\nRESPONSE TO AUTHOR REBUTTAL:\n\nThank you very much for a thoughtful response. Given that the authors have\nagreed to make the content be more specific to NER as opposed to\nsequence-tagging, I have revised my score upward.\n- Strengths:\n\nThe main strength promised by the paper is the speed advantage at the same\naccuracy level.\n\n- Weaknesses:\n\nPresentation of the approach leaves a lot to be desired. Sections 3 and 4 need\nto be much clearer, from concept definition to explaining the architecture and\nparameterization. In particular Section 4.1 and the parameter tieing used need\nto be crystal clear, since that is one of the main contributions of the paper.\n\nMore experiments supporting the vast speed improvements promised need to be\npresented. The results in Table 2 are good but not great. A speed-up of 4-6X is\nnothing all that transformative.\n\n- General Discussion:\n\nWhat exactly is \"Viterbi prediction\"? The term/concept is far from established;\nthe reader could guess but there must be a better way to phrase it.\n\nReference Weiss et al., 2015 has a typo.",
        "partial_review": "This work proposes to apply dilated convolutions for sequence tagging\n(specifically, named entity recognition). It also introduces some novel ideas\n(sharing the dilated convolution block, predicting the tags at each convolution\nlevel), which I think will prove useful to the community. The paper performs\nextensive ablation experiments to show the effectiveness of their approach.\nI found the writing to be very clear, and the experiments were exceptionally\nthorough.\n\nStrengths:  \n- Extensive experiments against various architectures (LSTM, LSTM + CRF)       \n- Novel architectural/training ideas (sharing blocks)  \n\nWeaknesses:  \n- Only applied to English NER--this is a big concern since the title of the\npaper seems to reference sequence-tagging directly.  \n- Section 4.1 could be clearer. For example, I presume there is padding to make\nsure the output resolution after each block is the same as the input\nresolution.  Might be good to mention this.  \n- I think an ablation study of number of layers vs perf might b",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions \n\nHere is the abstract of the paper: Bi-directional LSTMs have emerged as a standard method for obtaining per-token vector representations serving as input to various token labeling tasks (whether followed by Viterbi prediction or independent classification).  This paper proposes an alternative to Bi-LSTMs for this purpose: iterated dilated convolutional neural networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction.  We describe a distinct combination of network structure, parameter sharing and training procedures that is not only more accurate than Bi-LSTM-CRFs, but also 8x faster at test time on long sequences.  Moreover, ID-CNNs with independent classification enable a dramatic 14x test-time speedup, while still attaining accuracy comparable to the Bi-LSTM-CRF.  We further demonstrate the ability of ID-CNNs to combine evidence over long sequences by demonstrating their improved accuracy on whole-document (rather than per-sentence) inference.  Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, IDCNNs permit fixed-depth convolutions to run in parallel across entire documents.  Today when many companies run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. \n\nHere is the partial review: This work proposes to apply dilated convolutions for sequence tagging\n(specifically, named entity recognition). It also introduces some novel ideas\n(sharing the dilated convolution block, predicting the tags at each convolution\nlevel), which I think will prove useful to the community. The paper performs\nextensive ablation experiments to show the effectiveness of their approach.\nI found the writing to be very clear, and the experiments were exceptionally\nthorough.\n\nStrengths:  \n- Extensive experiments against various architectures (LSTM, LSTM + CRF)       \n- Novel architectural/training ideas (sharing blocks)  \n\nWeaknesses:  \n- Only applied to English NER--this is a big concern since the title of the\npaper seems to reference sequence-tagging directly.  \n- Section 4.1 could be clearer. For example, I presume there is padding to make\nsure the output resolution after each block is the same as the input\nresolution.  Might be good to mention this.  \n- I think an ablation study of number of layers vs perf might b \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This work proposes to apply dilated convolutions for sequence tagging (specifically, named entity recognition). It also introduces some novel ideas (sharing the dilated convolution block, predicting the tags at each convolution level), which I think will prove useful to the community. The paper performs extensive ablation experiments to show the effectiveness of their approach. I found the writing to be very clear, and the experiments were exceptionally thorough.\\n\\nStrengths:\\n- Extensive experiments against various architectures (LSTM, LSTM + CRF)\\n- Novel architectural/training ideas (sharing blocks)\\n\\nWeaknesses:\\n- Only applied to English NER--this is a big concern since the title of the paper seems to reference sequence-tagging directly.\\n- Section 4.1 could be clearer. For example, I presume there is padding to make sure the output resolution after each block is the same as the input resolution. Might be good to mention this.\\n- I think an ablation study of number of layers vs perf might be beneficial.\"\n}",
        "human_end_index": 1021
    },
    {
        "id": 649,
        "source": "PeerRead/acl_2017/",
        "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses",
        "human_review": "- Strengths:\nThis paper proposes an evaluation metric for automatically evaluating the\nquality of dialogue responses in non-task-oriented dialogue. The metric\noperates on continuous vector space representations obtained by using RNNs and\nit comprises two components: one that compares the context and the given\nresponse and the other that compares a reference response and the given\nresponse. The comparisons are conducted by means of dot product after\nprojecting the response into corresponding context and reference response\nspaces. These projection matrices are learned by minimizing the squared error\nbetween the model predictions and human annotations.\n\nI think this work gives a remarkable step forward towards the evaluation of\nnon-task-oriented dialogue systems. Different from previous works in this area,\nwhere pure semantic similarity was pursued, the authors are going beyond pure\nsemantic similarity in a very elegant manner by learning projection matrices\nthat transform the response vector into both context and reference space\nrepresentations. I am very curious on how your projection matrices M and N\ndiffer from the original identity initialization after training the models. I\nthink the paper will be more valuable if further discussion on this is\nintroduced, rather than focusing so much on resulting correlations. \n\n- Weaknesses:\n\nThe paper also leaves lots questions related to the implementation. For\ninstance, it is not clear whether the human scores used to train and evaluate\nthe system were single AMT annotations or the resulting average of few\nannotations. Also, it is not clear how the dataset was split into\ntrain/dev/test and whether n-fold cross validation was conducted or not. Also,\nit would be nice to better explain why in table 2 correlation for ADEM related\nscores are presented for the validation and test sets, while for the other\nscores they are presented for the full dataset and test set. The section on\npre-training with VHRED is also very clumsy and confusing, probably it is\nbetter to give less technical details but a better high level explanation of\nthe pre-training strategy and its advantages.\n\n- General Discussion:\n\n\u201cThere are many obvious cases where these metrics fail, as they are often\nincapable of considering the semantic similarity between responses (see Figure\n1).\u201d Be careful with statements like this one. This is not a problem of\nsemantic similarity! Opposite to it, the problem is that completely different\nsemantic cues might constitute pragmatically valid responses. Then, semantic\nsimilarity itself is not enough to evaluate a dialogue system response.\nDialogue system response evaluation must go beyond semantics (This is actually\nwhat your M and N matrices are helping to do!!!) \n\n\u201can accurate model that can evaluate dialogue response quality automatically\n\u2014 what could be considered an automatic Turing test \u2014\u201c The original\nintention of Turing test was to be a proxy to identify/define intelligent\nbehaviour. It actually proposes a test on intelligence based on an\n\u201cintelligent\u201d machine capability to imitate human behaviour in such a way\nthat it would be difficult for a common human to distinguish between such a\nmachine responses and actual human responses. It is of course related to\ndialogue system performance, but I think it is not correct to say that\nautomatically evaluating dialogue response quality is an automatic Turing test.\nActually, the title itself \u201cTowards an Automatic Turing Test\u201d is somehow\nmisleading!\n\n\u201cthe simplifying assumption that a \u2018good\u2019 chatbot is one whose responses\nare scored highly on appropriateness by human evaluators.\u201d This is certainly\nthe correct angle to introduce the problem of non-task-oriented dialogue\nsystems, rather than \u201cTuring Test\u201d. Regarding this, there has been related\nwork you might like to take a look at, as well as to make reference to, in the\nWOCHAT workshop series (see the shared task description and corresponding\nannotation guidelines).\n\nIn the discussion session: \u201cand has has been used\u201d -> \u201cand it has been\nused\u201d",
        "partial_review": "- Strengths:\nThis paper proposes an evaluation metric for automatically evaluating the\nquality of dialogue responses in non-task-oriented dialogue. The metric\noperates on continuous vector space representations obtained by using RNNs and\nit comprises two components: one that compares the context and the given\nresponse and the other that compares a reference response and the given\nresponse. The comparisons are conducted by means of dot product after\nprojecting the response into corresponding context and reference response\nspaces. These projection matrices are learned by minimizing the squared error\nbetween the model predictions and human annotations.\n\nI think this work gives a remarkable step forward towards the evaluation of\nnon-task-oriented dialogue systems. Different from previous works in this area,\nwhere pure semantic similarity was pursued, the authors are going beyond pure\nsemantic similarity in a very elegant manner by learning projection matrices\nthat transform the response vector into both context and reference space\nrepresentations. I am very curious on how your projection matrices M and N\ndiffer from the original identity initialization after training the models. I\nthink the paper will be more valuable if further discussion on this is\nintroduced, rather than focusing so much on resulting correlations. \n\n- Weaknesses:\n\nThe paper also leaves lots questions related to the implementation. For\ninstance, it is not clear whether the human scores used to train and evaluate\nthe system were single AMT annotations or the resulting average of few\nannotations. Also, it is not clear how the dataset was split into\ntrain/dev/test and whether n-fold cross validation was conducted or not. Also,\nit would be nice to better explain why in table 2 correlation for ADEM related\nscores are presented for the validation and test sets, while for the other\nscores they are presented for the full dataset and test set. The section on\npre-training with VHRED is also very clumsy and confusing, probably it is\nbetter to give less technical details but a better high level explanation of\nthe pre-training strategy and ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses \n\nHere is the abstract of the paper: Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem.  Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores.   We show that the ADEM model\u2019s predictions correlate significantly,  and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training,                    an important step for automatic dialogue evaluation. \n\nHere is the partial review: - Strengths:\nThis paper proposes an evaluation metric for automatically evaluating the\nquality of dialogue responses in non-task-oriented dialogue. The metric\noperates on continuous vector space representations obtained by using RNNs and\nit comprises two components: one that compares the context and the given\nresponse and the other that compares a reference response and the given\nresponse. The comparisons are conducted by means of dot product after\nprojecting the response into corresponding context and reference response\nspaces. These projection matrices are learned by minimizing the squared error\nbetween the model predictions and human annotations.\n\nI think this work gives a remarkable step forward towards the evaluation of\nnon-task-oriented dialogue systems. Different from previous works in this area,\nwhere pure semantic similarity was pursued, the authors are going beyond pure\nsemantic similarity in a very elegant manner by learning projection matrices\nthat transform the response vector into both context and reference space\nrepresentations. I am very curious on how your projection matrices M and N\ndiffer from the original identity initialization after training the models. I\nthink the paper will be more valuable if further discussion on this is\nintroduced, rather than focusing so much on resulting correlations. \n\n- Weaknesses:\n\nThe paper also leaves lots questions related to the implementation. For\ninstance, it is not clear whether the human scores used to train and evaluate\nthe system were single AMT annotations or the resulting average of few\nannotations. Also, it is not clear how the dataset was split into\ntrain/dev/test and whether n-fold cross validation was conducted or not. Also,\nit would be nice to better explain why in table 2 correlation for ADEM related\nscores are presented for the validation and test sets, while for the other\nscores they are presented for the full dataset and test set. The section on\npre-training with VHRED is also very clumsy and confusing, probably it is\nbetter to give less technical details but a better high level explanation of\nthe pre-training strategy and  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Strengths:\\nThis paper proposes an evaluation metric for automatically evaluating the quality of dialogue responses in non-task-oriented dialogue. The metric operates on continuous vector space representations obtained by using RNNs and it comprises two components: one that compares the context and the given response and the other that compares a reference response and the given response. The comparisons are conducted by means of dot product after projecting the response into corresponding context and reference response spaces. These projection matrices are learned by minimizing the squared error between the model predictions and human annotations.\\n\\nI think this work gives a remarkable step forward towards the evaluation of non-task-oriented dialogue systems. Different from previous works in this area, where pure semantic similarity was pursued, the authors are going beyond pure semantic similarity in a very elegant manner by learning projection matrices that transform the response vector into both context and reference space representations. I am very curious on how your projection matrices M and N differ from the original identity initialization after training the models. I think the paper will be more valuable if further discussion on this is introduced, rather than focusing so much on resulting correlations.\\n\\nWeaknesses:\\n\\nThe paper also leaves lots questions related to the implementation. For instance, it is not clear whether the human scores used to train and evaluate the system were single AMT annotations or the resulting average of few annotations. Also, it is not clear how the dataset was split into train/dev/test and whether n-fold cross validation was conducted or not. Also, it would be nice to better explain why in table 2 correlation for ADEM related scores are presented for the validation and test sets, while for the other scores they are presented for the full dataset and test set. The section on pre-training with VHRED is also very clumsy and confusing, probably it is better to give less technical details but a better high level explanation of the pre-training strategy and.\"\n}",
        "human_end_index": 2129
    },
    {
        "id": "654",
        "source": "PeerRead/acl_2017/",
        "title": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
        "human_review": "- General Discussion:\n\nThis paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling\nbased on deep BiLSTMs. In addition to applying recent best practice techniques,\nleading to further quantitative improvements, the authors provide an insightful\nqualitative analysis of their results. The paper is well written and has a\nclear structure. The authors provide a comprehensive overview of related work\nand compare results to a representative set of other SRL models that hace been\napplied on the same data sets.\n\nI found the paper to be interesting and convincing. It is a welcome research\ncontribution that not only shows that NNs work well, but also analyzes merits\nand shortcomings of an end-to-end learning approach.\n\n- Strengths:\n\nStrong model, insightful discussion/error analysis.\n\n- Weaknesses:\n\nLittle to no insights regarding the SRL task itself.\nThis paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nstate-of-the-art system (Zhou and Xu, 2015) with recent best practices for\ninitialization and regularization in the deep learning literature.\nThe model gives a 10% relative error reduction which is a big gain on this\ntask. The paper also gives in-depth empirical analyses to reveal the strengths\nand the remaining issues, that give a quite valuable information to the\nresearchers in this field. \n\nEven though I understand that the improvement of 3 point in F1 measure is a\nquite meaningful result from the engineering point of view, I think the main\ncontribution of the paper is on the extensive analysis in the experiment\nsection and a further in-depth investigation on analysis section. The detailed\nanalyses shown in Section 4 are performed in a quite reasonable way and give\nboth comparable results in SRL literature and novel information such as\nrelation between accuracies in syntactic parsing and SRL. This type of analysis\nhad often been omitted in recent papers. However, it is definitely important\nfor further improvement.\n\nThe paper is well-written and well-structured. \nI really enjoyed the paper and would like to see it accepted.",
        "partial_review": "- General Discussion:\n\nThis paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling\nbased on deep BiLSTMs. In addition to applying recent best practice techniques,\nleading to further quantitative improvements, the authors provide an insightful\nqualitative analysis of their results. The paper is well written and has a\nclear structure. The authors provide a comprehensive overview of related work\nand compare results to a representative set of other SRL models that hace been\napplied on the same data sets.\n\nI found the paper to be interesting and convincing. It is a welcome research\ncontribution that not only shows that NNs work well, but also analyzes merits\nand shortcomings of an end-to-end learning approach.\n\n- Strengths:\n\nStrong model, insightful discussion/error analysis.\n\n- Weaknesses:\n\nLittle to no insights regarding the SRL task itself.\nThis paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nstate-of-the-art system (Zhou and Xu, 2015) with recent best practices for\ninitialization and regularization in the deep learning ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Deep Semantic Role Labeling: What Works and What\u2019s Next \n\nHere is the abstract of the paper: We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results. \n\nHere is the partial review: - General Discussion:\n\nThis paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling\nbased on deep BiLSTMs. In addition to applying recent best practice techniques,\nleading to further quantitative improvements, the authors provide an insightful\nqualitative analysis of their results. The paper is well written and has a\nclear structure. The authors provide a comprehensive overview of related work\nand compare results to a representative set of other SRL models that hace been\napplied on the same data sets.\n\nI found the paper to be interesting and convincing. It is a welcome research\ncontribution that not only shows that NNs work well, but also analyzes merits\nand shortcomings of an end-to-end learning approach.\n\n- Strengths:\n\nStrong model, insightful discussion/error analysis.\n\n- Weaknesses:\n\nLittle to no insights regarding the SRL task itself.\nThis paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nstate-of-the-art system (Zhou and Xu, 2015) with recent best practices for\ninitialization and regularization in the deep learning  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a new state-of-the-art deep learning model for semantic role labeling (SRL) that significantly improves upon the previous state-of-the-art system. The authors provide a detailed analysis of their results, highlighting the strengths and limitations of their model. The paper is well-written and structured, with a comprehensive overview of related work and comparisons to other SRL models. The model itself is strong and the error analysis provides valuable insights. However, one weakness of the paper is the lack of insights regarding the SRL task itself. Overall, this paper is an interesting and convincing research contribution that not only demonstrates the effectiveness of neural networks but also provides a thorough analysis of their approach.\"\n}",
        "human_end_index": 1138
    },
    {
        "id": 657,
        "source": "PeerRead/acl_2017/",
        "title": "Interpreting Neural Networks to Understand Written Justifications in Values-Affirmation Essays",
        "human_review": "- Strengths: this paper addresses (in part) the problem of interpreting Long\nShort-Term Memory (LSTM) neural network models trained to categorize written\njustifications in values-affirmation essays. This is definitely an interesting\nresearch question. To do so, the authors want to rely on approaches that have\nare standard in experimental psychology. Furthermore, the authors also aim at\nvalidating sociological assumptions via this study.\n\n- Weaknesses: one of the main weaknesses of the paper lies in the fact that the\ngoals are not clear enough. One overall, ambitious goal put forward by the\nauthors is to use approaches from experimental psychology to interpret LSTMs.\nHowever, no clear methodology to do so is presented in the paper. On the other\nhand, if the goal is to validate sociological assumptions, then one should do\nso by studying the relationships between gender markers and the written\njustifications, independently on any model. The claim that \"expected gender\ndifferences (are) a function of theories of gendered self-construal\" is not\nproven in the study.\n\n- General Discussion: if the study is interesting, it suffers from several weak\narguments. First of all, the fact that the probability shift of a token in the\nLSTM network are correlated with the corresponding SVM coefficients is no proof\nthat \"these probabilities are valid ways to interpret the model\". Indeed, (a)\nSVM coefficients only reveal part of what is happening in the decision function\nof an SVM classifie and (b) it is not because one coefficient provides an\ninterpretation in one model that a correlated coefficient provides an\nexplanation in another model. Furthermore, the correlation coefficients are not\nthat high, so that the point put forward is not really backed up.\n\nAs mentioned before, another problem lies in the fact that the authors seem to\nhesitate between two goals. It would be better to clearly state one goal and\ndevelop it. Concerning the relation to experimental psychology, which is a\npriori an important part of the paper, it would be interesting to develop and\nbetter explain the multilevel bayesian models used to quantify the gender-based\nself-construal assumptions. It is very difficult to assess whether the\nmethodology used here is really appropriate without more details. As this is an\nimportant aspect of the method, it should be further detailed.\n- Strengths:\nThe paper is thoroughly written and discusses its approach compared to\nother approaches. The authors are aware that their findings are somewhat\nlimited regarding the mean F values.\n\n- Weaknesses:\nSome minor orthographical mistakes and some repetive clauses. In general the\npaper would benefit if the sections 1 and 2 would be shortened to allow the\nextension of sections 3 and 4.\nThe main goal is not laid out clearly enough, which may be a result of the\nambivalence of the paper's goals.\n\n- General Discussion:\nTable 1 should only be one column wide, while the figures, especially 3, 5, and\n6 would greatly benefit from a two column width.\nThe paper was not very easy to understand during first read. Major improvements\ncould be achieved by straightening up the content.",
        "partial_review": "- Strengths: this paper addresses (in part) the problem of interpreting Long\nShort-Term Memory (LSTM) neural network models trained to categorize written\njustifications in values-affirmation essays. This is definitely an interesting\nresearch question. To do so, the authors want to rely on approaches that have\nare standard in experimental psychology. Furthermore, the authors also aim at\nvalidating sociological assumptions via this study.\n\n- Weaknesses: one of the main weaknesses of the paper lies in the fact that the\ngoals are not clear enough. One overall, ambitious goal put forward by the\nauthors is to use approaches from experimental psychology to interpret LSTMs.\nHowever, no clear methodology to do so is presented in the paper. On the other\nhand, if the goal is to validate sociological assumptions, then one should do\nso by studying the relationships between gender markers and the written\njustifications, independently on any model. The claim that \"expected gender\ndifferences (are) a function of theories of gendered self-construal\" is not\nproven in the study.\n\n- General Discussion: if the study is interesting, it suffers from several weak\narguments. First of all, the fact that the probability shift of a token in the\nLSTM network are correlated with the corresponding SVM coefficients is no proof\nthat \"these probabilities are valid ways to interpret the model\". Indeed, (a)\nSVM coefficients only reveal part of what is happening in the decision function\nof an SVM classifie and (b) it is not because one coefficient provides an\ninterpretation in one model that a correlated coefficient provides an\nexplanation in a",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Interpreting Neural Networks to Understand Written Justifications in Values-Affirmation Essays \n\nHere is the abstract of the paper:  \n\nHere is the partial review: - Strengths: this paper addresses (in part) the problem of interpreting Long\nShort-Term Memory (LSTM) neural network models trained to categorize written\njustifications in values-affirmation essays. This is definitely an interesting\nresearch question. To do so, the authors want to rely on approaches that have\nare standard in experimental psychology. Furthermore, the authors also aim at\nvalidating sociological assumptions via this study.\n\n- Weaknesses: one of the main weaknesses of the paper lies in the fact that the\ngoals are not clear enough. One overall, ambitious goal put forward by the\nauthors is to use approaches from experimental psychology to interpret LSTMs.\nHowever, no clear methodology to do so is presented in the paper. On the other\nhand, if the goal is to validate sociological assumptions, then one should do\nso by studying the relationships between gender markers and the written\njustifications, independently on any model. The claim that \"expected gender\ndifferences (are) a function of theories of gendered self-construal\" is not\nproven in the study.\n\n- General Discussion: if the study is interesting, it suffers from several weak\narguments. First of all, the fact that the probability shift of a token in the\nLSTM network are correlated with the corresponding SVM coefficients is no proof\nthat \"these probabilities are valid ways to interpret the model\". Indeed, (a)\nSVM coefficients only reveal part of what is happening in the decision function\nof an SVM classifie and (b) it is not because one coefficient provides an\ninterpretation in one model that a correlated coefficient provides an\nexplanation in a \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Overall, this paper addresses an interesting research question but has several weaknesses. The main weakness lies in the lack of clear goals and methodology for interpreting the LSTM models. Additionally, the claim of validating sociological assumptions is not sufficiently supported. The study's arguments are weak, as the correlation between LSTM probabilities and SVM coefficients does not prove valid interpretation of the models. Overall, more clarity, robustness, and evidence are needed in this paper.\"\n}",
        "human_end_index": 1635
    },
    {
        "id": 66,
        "source": "PeerRead/acl_2017/",
        "title": "Generating Memorable Mnemonic Encodings of Numbers",
        "human_review": "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words constrained such that digits in the\noriginal sequence correspond to characters or digraphs in the output sequence\nof words; vowels added surrounding the consonant phonemes to form words are\nunconstrained. This paper describes several ways to encode your sequence of\ndigits such that the output sequence of words is more memorable, generally by\napplying syntactic constraints and heuristics.\n\nI found this application of natural language processing concepts somewhat\ninteresting, as I have not read an ACL paper on this topic before. However, I\nfound the paper and ideas presented here to have a rather old-school feel. With\nmuch of the focus on n-gram models for generation, frequent POS-tag sequences,\nand other heuristics, this paper really could have been written 15-20 years\nago. I am not sure that there is enough novelty in the ideas here to warrant\npublication in ACL in 2017. There is no contribution to NLP itself, e.g. in\nterms of modeling or search, and not a convincing contribution to the\napplication area which is just an instance of constrained generation. \n\nSince you start with one sequence and output another sequence with a very\nstraightforward monotonic mapping, it seems like a character-based\nsequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with\nNeural Networks; Sutskever et al. 2014) would work rather well here, very\nlikely with very fluent output and fewer moving parts (e.g. trigram models and\nPOS tag and scoring heuristics and postprocessing with a bigram model). You can\nuse large amounts of training from an arbitrary genre and do not need to rely\non an already-tagged corpus like in this paper, or worry about a parser. This\nwould be a 2017 paper.\n- Strengths:\nThis paper presents a sentence based approach to generating memorable mnemonics\nfor numbers. The evaluation study presented in the paper shows that the\nsentence based approach indeed produces memorable mnemonics for short 8-digit\nnumbers (e.g. 86101521 --> Officiate Wasteland).\nOverall the paper presents the problem, the background literature and the\nsolution in sufficient detail. Because memorizing numbers (e.g. phone numbers\nand account numbers) is sufficiently common, this is an interesting problem.\n\n- Weaknesses:\nThe proposed solution does not seem to scale-up well for longer numbers; seems\nto work well with 8-digit numbers though. But many numbers that people need to\nmemorize such as phone numbers and credit card numbers are longer than\n8-digits. Besides, a number may have a structure (e.g. a phone number has a\ncountry code + area code + personal number) which people exploit while\nmemorizing numbers. As stated above, this paper addresses an important problem\nbut the current solution needs to be improved further (several ideas have been\nlisted by the authors in section 6).\n\n- General Discussion:\nThe current presented approach, in comparison to existing approaches, is\npromising.\n- Strengths:\n\nTackles a not very explored task, with obvious practical application\nWell written and motivated\n\n- Weaknesses:\n\nThe only method of validation is a user study, which has several weaknesses.\n\n- Discussion:\n\nThe paper investigates various methods to generate memorable mnemonic encodings\nof numbers based on the \u201cMajor\u201d system. As opposed to other methods that\nrely on this system to encode sequences, the methods proposed in this work\nreturn a single sequence (instead of a set of candidates) which is selected to\nimprove memorability. Since \u201cmemorability\u201d is an ambiguous criterion to\noptimize for, the authors explore various syntactic approaches that aim for\nshort and likely sentences.  Their final model uses a POS template sampled form\na set of \u201cnice\u201d structures, and a tri-gram language model to fill in the\nslots of the template. \n\nThe proposed approach is well motivated: the section on existing tools places\nthis approach in the context of previous work on security and memorability. The\nauthors point to results showing that passwords based on mnemonic phrases offer\nthe best of both worlds in terms of security (vs random passwords) and\nmemorability (vs naive passwords). This solid motivation will appease those\nreaders initially skeptical about the importance/feasibility of such\ntechniques. \n\nIn terms of the proposed methods, the baselines and n-gram models\n(unsurprisingly) generate bad encodings. The results in table 2 show that\nindeed Chunk and Sentence produce shorter sentences, but for short digits such\nas this one, how relevant are the additional characteristics of these methods\n(eg. POS replacements, templates etc)? It seems that a simple n-gram model with\nthe number-of-digits-per-trigram reweighing could perform well here. \n\nThe evaluation is weaker than the rest of the paper. My main concern is that a\none-time memorization setting seems inadequate to test this framework. Mnemonic\ntechniques are meant to aid recall after repeated memorization exercises, not\njust a single \u201cpriming\u201d event. Thus, a more informative setting would have\nhad the users be reminded of the number and encoding daily over a period of\ntime, and after a \u201cbuffer period\u201d, test their recall. This would also more\nclosely resemble the real-life conditions in which such a technique would be\nused (e.g. for password memorization).\n\nIn terms of the results, the difference between (long term) recall and\nrecognition is interesting. Do the authors have some explanation for why in the\nformer most methods performed similarly, but in the latter \u201cSentence\u201d\nperforms better? Could it be that the use of not very likely words (e.g.\n\"officiate\", in the example provided) make the encodings hard to remember but\neasy to spot? If this were the case, it would somewhat defeat the purpose of\nthe approach.\n\nAlso, it would be useful for the reader if the paper provided  (e.g. in an\nappendix) some examples of the digits/encodings that the users were presented\nduring the study, to get a better sense of the difficulty of recall and the\nquality of the encodings. \n\n- Suggestions:\n\nIt would be nice to provide some background on the Major system for those not\nfamiliar with it, which I suspect might be many in the ACL audience, myself\nincluded. Where does it come from? What\u2019s the logic behind those\ndigit-phoneme maps?",
        "partial_review": "This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words constrained such that digits in the\noriginal sequence correspond to characters or digraphs in the output sequence\nof words; vowels added surrounding the consonant phonemes to form words are\nunconstrained. This paper describes several ways to encode your sequence of\ndigits such that the output sequence of words is more memorable, generally by\napplying syntactic constraints and heuristics.\n\nI found this application of natural language processing concepts somewhat\ninteresting, as I have not read an ACL paper on this topic before. However, I\nfound the paper and ideas presented here to have a rather old-school feel. With\nmuch of the focus on n-gram models for generation, frequent POS-tag sequences,\nand other heuristics, this paper really could have been written 15-20 years\nago. I am not sure that there is enough novelty in the ideas here to warrant\npublication in ACL in 2017. There is no contribution to NLP itself, e.g. in\nterms of modeling or search, and not a convincing contribution to the\napplication area which is just an instance of constrained generation. \n\nSince you start with one sequence and output another sequence with a very\nstraightforward monotonic mapping, it seems like a character-based\nsequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with\nNeural Networks; Sutskever et al. 2014) would work rather well here, very\nlikely with very fluent output and fewer moving parts (e.g. trigram models and\nPOS tag and scoring heuristics and postprocessing with a bigram model). You can\nuse large amounts of training from an arbitrary genre and do not need to rely\non an already-tagged corpus like in this paper, or worry about a parser. This\nwould be a 2017 paper.\n- Strengths:\nThis paper presents a sentence based approach to generating memorable mnemonics\nfor numbers. The evaluation study presented in the paper shows that the\nsentence based approach indeed produces memorable mnemonics for short 8-digit\nnumbers (e.g. 86101521 --> Officiate Wasteland).\nOverall the paper presents the problem, the background literature and the\nsolution in sufficient detail. Because memorizing numbers (e.g. phone numbers\nand account numbers) is sufficiently common, this is an interesting problem.\n\n- Weaknesses:\nThe proposed solution does not seem to scale-up well for longer numbers; seems\nto work well with 8-digit numbers though. But many numbers that people need to\nmemorize such as phone numbers and credit card numbers are longer than\n8-digits. Besides, a number may have a structure (e.g. a phone number has a\ncountry code + area code + personal number) which people exploit while\nmemorizing numbers. As stated above, this paper addresses an important problem\nbut the current solution needs to be improved further (several ideas have been\nlisted by the authors in section 6).\n\n- General Discussion:\nThe current presented approach, in comparison to exi",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Generating Memorable Mnemonic Encodings of Numbers \n\nHere is the abstract of the paper: The major system is a mnemonic system that can be used to memorize sequences of numbers. In this work, we present a method to automatically generate sentences that encode a given number. We propose several encoding models and compare the most promising ones in a password memorability study. The results of the study show that a model combining part-of-speech sentence templates with an n-gram language model produces the most memorable password representations. \n\nHere is the partial review: This paper describes several ways to encode arbitrarily long sequences of\ndigits using something called the major system. In the major system, each digit\nis mapped to one or more characters representing consonantal phonemes; the\npossible mappings between digit and phoneme are predefined. The output of an\nencoding is typically a sequence of words constrained such that digits in the\noriginal sequence correspond to characters or digraphs in the output sequence\nof words; vowels added surrounding the consonant phonemes to form words are\nunconstrained. This paper describes several ways to encode your sequence of\ndigits such that the output sequence of words is more memorable, generally by\napplying syntactic constraints and heuristics.\n\nI found this application of natural language processing concepts somewhat\ninteresting, as I have not read an ACL paper on this topic before. However, I\nfound the paper and ideas presented here to have a rather old-school feel. With\nmuch of the focus on n-gram models for generation, frequent POS-tag sequences,\nand other heuristics, this paper really could have been written 15-20 years\nago. I am not sure that there is enough novelty in the ideas here to warrant\npublication in ACL in 2017. There is no contribution to NLP itself, e.g. in\nterms of modeling or search, and not a convincing contribution to the\napplication area which is just an instance of constrained generation. \n\nSince you start with one sequence and output another sequence with a very\nstraightforward monotonic mapping, it seems like a character-based\nsequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with\nNeural Networks; Sutskever et al. 2014) would work rather well here, very\nlikely with very fluent output and fewer moving parts (e.g. trigram models and\nPOS tag and scoring heuristics and postprocessing with a bigram model). You can\nuse large amounts of training from an arbitrary genre and do not need to rely\non an already-tagged corpus like in this paper, or worry about a parser. This\nwould be a 2017 paper.\n- Strengths:\nThis paper presents a sentence based approach to generating memorable mnemonics\nfor numbers. The evaluation study presented in the paper shows that the\nsentence based approach indeed produces memorable mnemonics for short 8-digit\nnumbers (e.g. 86101521 --> Officiate Wasteland).\nOverall the paper presents the problem, the background literature and the\nsolution in sufficient detail. Because memorizing numbers (e.g. phone numbers\nand account numbers) is sufficiently common, this is an interesting problem.\n\n- Weaknesses:\nThe proposed solution does not seem to scale-up well for longer numbers; seems\nto work well with 8-digit numbers though. But many numbers that people need to\nmemorize such as phone numbers and credit card numbers are longer than\n8-digits. Besides, a number may have a structure (e.g. a phone number has a\ncountry code + area code + personal number) which people exploit while\nmemorizing numbers. As stated above, this paper addresses an important problem\nbut the current solution needs to be improved further (several ideas have been\nlisted by the authors in section 6).\n\n- General Discussion:\nThe current presented approach, in comparison to exi \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The current presented approach, in comparison to existing literature, is not very novel and lacks cutting-edge techniques. The focus on n-gram models, frequent POS-tag sequences, and heuristics makes the paper feel outdated and could have been written 15-20 years ago. It does not contribute significantly to the field of NLP or provide a convincing contribution to the application area. Moreover, while the proposed sentence-based approach shows promise for shorter numbers, it does not scale well for longer numbers, which limits its practical applicability. Additionally, the paper does not consider the structural characteristics of numbers that people often exploit for memorization. Overall, while the problem of generating memorable mnemonics for numbers is interesting, the current solution lacks novelty and needs further improvement.\"\n}",
        "human_end_index": 3236
    },
    {
        "id": 67,
        "source": "PeerRead/acl_2017/",
        "title": "Constructing Semantic Hierarchies via Fusion Learning Architecture",
        "human_review": "- Strengths:\n- The paper tackles an important issue, that is building ontologies or thesauri\n- The methods make sense and seem well chosen\n- Methods and setups are well detailed\n- It looks like the authors outperform the state-of-the-art approach (but see\nbelow for my concerns)\n\n- Weaknesses:\nThe main weaknesses for me are evaluation and overall presentation/writing.\n- The list of baselines is hard to understand. Some methods are really old and\nit doesn't seem justified to show them here (e.g., Mpttern).\n- Memb is apparently the previous state-of-the-art, but there is no mention to\nany reference.\n- While it looks like the method outperforms the previous best performing\napproach, the paper is not convincing enough. Especially, on the first dataset,\nthe difference between the new system and the previous state-of-the-art one is\npretty small.\n- The paper seriously lacks proofreading, and could not be published until this\nis fixed \u2013 for instance,\u00a0I noted 11 errors in the first column of page 2.\n- The CilinE hierarchy is very shallow (5 levels only). However apparently, it\nhas been used in the past by other authors. I would expect that the deeper the\nmore difficult it is to branch new hyponym-hypernyms. This can explain the very\nhigh results obtained (even by previous studies)...\n\n- General Discussion:\nThe approach itself is not really original or novel, but it is applied to a\nproblem that has not been addressed with deep learning yet. For this reason, I\nthink this paper is interesting, but there are two main flaws. The first and\neasiest to fix is the presentation. There are many errors/typos that need to be\ncorrected. I started listing them to help, but there are just too many of them.\nThe second issue is the evaluation, in my opinion. Technically, the\nperformances are better, but it does not feel convincing as explained above.\nWhat is Memb, is it the method from Shwartz et al 2016, maybe? If not, what\nperformance did this recent approach have? I think the authors need to\nreorganize the evaluation section, in order to properly list the baseline\nsystems, clearly show the benefit of their approach and where the others fail.\nSignificance tests  also seem necessary given the slight improvement on one\ndataset.\n- Strengths:\n\n  * Knowledge lean, language-independent approach\n\n- Weaknesses:\n\n  * Peculiar task/setting\n  * Marginal improvement over W_Emb (Fu et al, 2014)\n  * Waste of space\n  * Language not always that clear\n\n- General Discussion:\n\nIt seems to me that this paper is quite similar to (Fu et al, 2014) and only\nadds marginal improvements. It contains quite a lot of redundancy (e.g. related\nwork in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2),\nnot so useful descriptions of MLP and RNN, etc. A short paper might have been a\nbetter fit.\n\nThe task looks somewhat idiosyncratic to me. It is only useful if you already\nhave a method that gives you all and only the hypernyms of a given word. This\nseems to presuppose (Fu et al., 2013). \n\nFigure 4: why are the first two stars connected by conjunction and the last two\nstarts by disjunction?              Why is the output \"1\" (dark star) if the the\nthree\ninputs are \"0\" (white stars)?\n\nSec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the\ntest data (?) \n\nW_Emb is poorly explained (lines 650-652).\n\nSome parts of the text are puzzling. I can't make sense of the section titled\n\"Combined with Manually-Built Hierarchies\". Same for sec 4.4. What do the red\nand dashed lines mean?",
        "partial_review": "- Strengths:\n- The paper tackles an important issue, that is building ontologies or thesauri\n- The methods make sense and seem well chosen\n- Methods and setups are well detailed\n- It looks like the authors outperform the state-of-the-art approach (but see\nbelow for my concerns)\n\n- Weaknesses:\nThe main weaknesses for me are evaluation and overall presentation/writing.\n- The list of baselines is hard to understand. Some methods are really old and\nit doesn't seem justified to show them here (e.g., Mpttern).\n- Memb is apparently the previous state-of-the-art, but there is no mention to\nany reference.\n- While it looks like the method outperforms the previous best performing\napproach, the paper is not convincing enough. Especially, on the first dataset,\nthe difference between the new system and the previous state-of-the-art one is\npretty small.\n- The paper seriously lacks proofreading, and could not be published until this\nis fixed \u2013 for instance,\u00a0I noted 11 errors in the first column of page 2.\n- The CilinE hierarchy is very shallow (5 levels only). However apparently, it\nhas been used in the past by other authors. I would expect that the deeper the\nmore difficult it is to branch new hyponym-hypernyms. This can explain the very\nhigh results obtained (even by previous studies)...\n\n- General Discussion:\nThe approach itself is not really original or novel, but it is applied to a\nproblem that has not been addressed with deep learning yet. For this reason, I\nthink this paper is interesting, but there are two main flaws. The first and\neasiest to fix is the presentation. There are many errors/typos that need to be\ncorrected. I started listing them to help, but there are just too many of them.\nThe second issue is the evaluation,",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Constructing Semantic Hierarchies via Fusion Learning Architecture \n\nHere is the abstract of the paper: Semantic hierarchies construction means to build structure of concepts linked by hypernym-hyponym (``is-a'') relations. A major challenge for this task is the automatic discovery of hypernym-hyponym (``is-a'') relations. We propose a fusion learning architecture based on word embeddings for constructing semantic hierarchies, composed of discriminative generative fusion architecture and a very simple lexical structure rule for assisting, getting an F1-score of 74.20% with 91.60% precision-value, outperforming the state-of-the-art methods on a manually labeled test dataset. Subsequently, combining our method with manually-built hierarchies can further improve F1-score to 82.01%. Besides, the fusion learning architecture is language-independent. \n\nHere is the partial review: - Strengths:\n- The paper tackles an important issue, that is building ontologies or thesauri\n- The methods make sense and seem well chosen\n- Methods and setups are well detailed\n- It looks like the authors outperform the state-of-the-art approach (but see\nbelow for my concerns)\n\n- Weaknesses:\nThe main weaknesses for me are evaluation and overall presentation/writing.\n- The list of baselines is hard to understand. Some methods are really old and\nit doesn't seem justified to show them here (e.g., Mpttern).\n- Memb is apparently the previous state-of-the-art, but there is no mention to\nany reference.\n- While it looks like the method outperforms the previous best performing\napproach, the paper is not convincing enough. Especially, on the first dataset,\nthe difference between the new system and the previous state-of-the-art one is\npretty small.\n- The paper seriously lacks proofreading, and could not be published until this\nis fixed \u2013 for instance,\u00a0I noted 11 errors in the first column of page 2.\n- The CilinE hierarchy is very shallow (5 levels only). However apparently, it\nhas been used in the past by other authors. I would expect that the deeper the\nmore difficult it is to branch new hyponym-hypernyms. This can explain the very\nhigh results obtained (even by previous studies)...\n\n- General Discussion:\nThe approach itself is not really original or novel, but it is applied to a\nproblem that has not been addressed with deep learning yet. For this reason, I\nthink this paper is interesting, but there are two main flaws. The first and\neasiest to fix is the presentation. There are many errors/typos that need to be\ncorrected. I started listing them to help, but there are just too many of them.\nThe second issue is the evaluation, \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents a fusion learning architecture based on word embeddings for constructing semantic hierarchies. The strengths of the paper include the importance of the problem being tackled, the well-detailed methods and setups, and the outperformance of the state-of-the-art approaches. However, there are several weaknesses to be addressed. Firstly, the list of baselines is hard to understand, and some of the included methods are outdated. Additionally, there is no reference to the previous state-of-the-art method. The paper lacks convincing evidence of the superiority of the proposed approach, especially when comparing the results to the previous best performing system. Moreover, the paper requires improvement in terms of proofreading, as there are numerous errors and typos throughout. Furthermore, the shallow nature of the CilinE hierarchy raises questions about the generalizability of the results. Overall, while the paper addresses an interesting problem, the presentation and evaluation need to be improved for it to be publishable.\"\n}",
        "human_end_index": 1745
    },
    {
        "id": "676",
        "source": "PeerRead/acl_2017/",
        "title": "Neural Machine Translation via Binary Code Prediction",
        "human_review": "- Strengths:\nThe proposed methods can save memory and improve decoding speed on CPUs without\nlosing (or a little loss) performance. \n\n- Weaknesses:\nSince the determination of the convolutional codes of Algorithm 2 and Algorithm\n3 can affect the final performance, I think it would be better if the authors\ncan explore a good method for it. And I think the argument of \u201cExperiments\nshow the proposed model achieves translation accuracies that approach the\nsoftmax, while reducing memory usage on the order of 1/10 to 1/1000, and also\nimproving decoding speed on CPUs by x5 to x20.\u201d in the Abstract is not\nrigorous. As far as I know, your experiments setting with \u201cBinary\u201d and\n\u201cHybrid-512\u201d on ASPEC corpus show the improvements of decoding speed on\nCPUs by x20, but the BLEU scores are too low. So this is not a valid\nconclusion.\n\n- General Discussion:\nThis paper proposes an efficient prediction method for neural machine\ntranslation, which predicts a binary code for each word, to reduce the\ncomplexity of prediction. The authors also proposed to use the improved (error\ncorrection) binary codes method to improve the prediction accuracy and the\nhybrid softmax/binary model to balance the prediction accuracy and efficiency.\nThe proposed methods can save memory and improve decoding speed without losing\n(or a little loss) performance. I think this is a good paper.\n- Strengths:\n  This paper is well written, and with clear, well-designed\n  figures. The reader can easily understand the methodology even only\n  with those figures.\n\n  Predicting the binary code directly is a clever way to reduce the\n  parameter space, and the error-correction code just works\n  surprisingly well. I am really surprised by how 44 bits can achieve\n  26 out of 31 BLEU.  \n  The parameter reducing technique described in this work is\n  orthogonal to current existing methods: weight pruning and\n  sequence-level knowledge distilling.\n\n  The method here is not restricted by Neural Machine Translation, and\n  can be used in other tasks as long as there is a big output\n  vocabulary.  \n\n- Weaknesses:\n  The most annoying point to me is that in the relatively large\n  dataset (ASPEC), the best proposed model is still 1 BLEU point lower\n  than the softmax model. What about some even larger dataset, like\n  the French-English? There are at most 12 million sentences\n  there. Will the gap be even larger?\n\n  Similarly, what's the performance on some other language pairs ?\n\n  Maybe you should mention this paper,\n  https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by\n  10x and the BLEU loss is less than 0.5.  \n\n- General Discussion:\n\nThe paper describes a parameter reducing method for large vocabulary\nsoftmax. By applying the error-corrected code and hybrid with softmax,\nits BLEU approaches that of the orignal full vocab softmax model.\n\nOne quick question: what is the hidden dimension size of the models?\nI couldn't find this in the experiment setup.\n\nThe 44 bits can achieve 26 out of 31 BLEU on E2J, that was\nsurprisingly good. However, how could you increase the number of bits\nto increase the classification power ? 44 is too small, there's plenty\nof room to use more bits and the computation time on GPU won't even\nchange.\n\nAnother thing that is counter-intuitive is that by predicting the\nbinary code, the model is actually predicting the rank of the\nwords. So how should we interpret these bit-embeddings ? There seems\nno semantic relations of all the words that have odd rank. Is it\nbecause the model is so powerful that it just remembers the data ?\n- Strengths:\nThis paper has high originality, proposing a fundamentally different way of\npredicting words from a vocabulary that is more efficient than a softmax layer\nand has comparable performance on NMT. If successful, the approach could be\nimpactful because it speeds up prediction.\n\nThis paper is nice to read with great diagrams. it's very clearly presented --\nI like cross-referencing the models with the diagrams in Table 2. Including\nloss curves is appreciated.\n\n- Weaknesses:\nThough it may not be possible in the time remaining, it would be good to see a\ncomparison (i.e. BLEU scores) with previous related work like hierarchical\nsoftmax and differentiated softmax.\n\nThe paper is lacking a linguistic perspective on the proposed method. Compared\nto a softmax layer and hierarchical/differentiated softmax, is binary code\nprediction a natural way to predict words? Is it more or less similar to how a\nhuman might retrieve words from memory? Is there a theoretical reason to\nbelieve that binary code based approaches should be more or less suited to the\ntask than softmax layers?\n\nThough the paper promises faster training speeds in the introduction, Table 3\nshows only modest (less than x2) speedups for training. Presumably this is\nbecause much of the training iteration time is consumed by other parts of the\nnetwork. It would be useful to see the time needed for the output layer\ncomputation only.\n\n- General Discussion:\nIt would be nice if the survey of prior work in 2.2 explicitly related those\nmethods to the desiderata in the introduction (i.e. specify which they\nsatisfy).\n\nSome kind of analysis of the qualitative strengths and weaknesses of the binary\ncode prediction would be welcome -- what kind of mistakes does the system make,\nand how does this compare to standard softmax and/or hierarchical and\ndifferentiated softmax?\n\nLOW LEVEL COMMENTS\n\nEquation 5: what's the difference between id(w) = id(w') and w = w' ?\n\n335: consider defining GPGPU\n\nTable 3: Highlight the best BLEU scores in bold\n\nEquation 15: remind the reader that q is defined in equation 6 and b is a\nfunction of w. I was confused by this at first because w and h appear on the\nLHS but don't appear on the right, and I didn't know what b and q were.",
        "partial_review": "- Strengths:\nThe proposed methods can save memory and improve decoding speed on CPUs without\nlosing (or a little loss) performance. \n\n- Weaknesses:\nSince the determination of the convolutional codes of Algorithm 2 and Algorithm\n3 can affect the final performance, I think it would be better if the authors\ncan explore a good method for it. And I think the argument of \u201cExperiments\nshow the proposed model achieves translation accuracies that approach the\nsoftmax, while reducing memory usage on the order of 1/10 to 1/1000, and also\nimproving decoding speed on CPUs by x5 to x20.\u201d in the Abstract is not\nrigorous. As far as I know, your experiments setting with \u201cBinary\u201d and\n\u201cHybrid-512\u201d on ASPEC corpus show the improvements of decoding speed on\nCPUs by x20, but the BLEU scores are too low. So this is not a valid\nconclusion.\n\n- General Discussion:\nThis paper proposes an efficient prediction method for neural machine\ntranslation, which predicts a binary code for each word, to reduce the\ncomplexity of prediction. The authors also proposed to use the improved (error\ncorrection) binary codes method to improve the prediction accuracy and the\nhybrid softmax/binary model to balance the prediction accuracy and efficiency.\nThe proposed methods can save memory and improve decoding speed without losing\n(or a little loss) performance. I think this is a good paper.\n- Strengths:\n  This paper is well written, and with clear, well-designed\n  figures. The reader can easily understand the methodology even only\n  with those figures.\n\n  Predicting the binary code directly is a clever way to reduce the\n  parameter space, and the error-correction code just works\n  surprisingly well. I am really surprised by how 44 bits can achieve\n  26 out of 31 BLEU.  \n  The parameter reducing technique described in this work is\n  orthogonal to current existing methods: weight pruning and\n  sequence-level knowledge distilling.\n\n  The method here is not restricted by Neural Machine Translation, and\n  can be used in other tasks as long as there is a big output\n  vocabulary.  \n\n- Weaknesses:\n  The most annoying point to me is that in the relatively large\n  dataset (ASPEC), the best proposed model is still 1 BLEU point lower\n  than the softmax model. What about some even larger dataset, like\n  the French-English? There are at most 12 million sentences\n  there. Will the gap be even larger?\n\n  Similarly, what's the performance on some other language pairs ?\n\n  Maybe you should mention this paper,\n  https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by\n  10x and the BLEU loss is less than 0.5.  \n\n- General Discussion:\n\nThe paper describes a parameter reducing method for large vocabulary\nsoftmax. By applying the error-corrected code and hybrid with softmax,\nits BLEU approaches that of the orignal full vocab softmax model.\n\nOne quick question: what is the hidden dimension size of the mo",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Neural Machine Translation via Binary Code Prediction \n\nHere is the abstract of the paper: In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10. \n\nHere is the partial review: - Strengths:\nThe proposed methods can save memory and improve decoding speed on CPUs without\nlosing (or a little loss) performance. \n\n- Weaknesses:\nSince the determination of the convolutional codes of Algorithm 2 and Algorithm\n3 can affect the final performance, I think it would be better if the authors\ncan explore a good method for it. And I think the argument of \u201cExperiments\nshow the proposed model achieves translation accuracies that approach the\nsoftmax, while reducing memory usage on the order of 1/10 to 1/1000, and also\nimproving decoding speed on CPUs by x5 to x20.\u201d in the Abstract is not\nrigorous. As far as I know, your experiments setting with \u201cBinary\u201d and\n\u201cHybrid-512\u201d on ASPEC corpus show the improvements of decoding speed on\nCPUs by x20, but the BLEU scores are too low. So this is not a valid\nconclusion.\n\n- General Discussion:\nThis paper proposes an efficient prediction method for neural machine\ntranslation, which predicts a binary code for each word, to reduce the\ncomplexity of prediction. The authors also proposed to use the improved (error\ncorrection) binary codes method to improve the prediction accuracy and the\nhybrid softmax/binary model to balance the prediction accuracy and efficiency.\nThe proposed methods can save memory and improve decoding speed without losing\n(or a little loss) performance. I think this is a good paper.\n- Strengths:\n  This paper is well written, and with clear, well-designed\n  figures. The reader can easily understand the methodology even only\n  with those figures.\n\n  Predicting the binary code directly is a clever way to reduce the\n  parameter space, and the error-correction code just works\n  surprisingly well. I am really surprised by how 44 bits can achieve\n  26 out of 31 BLEU.  \n  The parameter reducing technique described in this work is\n  orthogonal to current existing methods: weight pruning and\n  sequence-level knowledge distilling.\n\n  The method here is not restricted by Neural Machine Translation, and\n  can be used in other tasks as long as there is a big output\n  vocabulary.  \n\n- Weaknesses:\n  The most annoying point to me is that in the relatively large\n  dataset (ASPEC), the best proposed model is still 1 BLEU point lower\n  than the softmax model. What about some even larger dataset, like\n  the French-English? There are at most 12 million sentences\n  there. Will the gap be even larger?\n\n  Similarly, what's the performance on some other language pairs ?\n\n  Maybe you should mention this paper,\n  https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by\n  10x and the BLEU loss is less than 0.5.  \n\n- General Discussion:\n\nThe paper describes a parameter reducing method for large vocabulary\nsoftmax. By applying the error-corrected code and hybrid with softmax,\nits BLEU approaches that of the orignal full vocab softmax model.\n\nOne quick question: what is the hidden dimension size of the mo \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper is well written, and with clear, well-designed figures. The reader can easily understand the methodology even only with those figures.\\n\\nPredicting the binary code directly is a clever way to reduce the parameter space, and the error-correction code just works surprisingly well. I am really surprised by how 44 bits can achieve 26 out of 31 BLEU.\\n\\nThe parameter reducing technique described in this work is orthogonal to current existing methods: weight pruning and sequence-level knowledge distilling.\\n\\nThe method here is not restricted by Neural Machine Translation, and can be used in other tasks as long as there is a big output vocabulary.\\n\\nThe most annoying point to me is that in the relatively large dataset (ASPEC), the best proposed model is still 1 BLEU point lower than the softmax model. What about some even larger dataset, like the French-English? There are at most 12 million sentences there. Will the gap be even larger?\\n\\nSimilarly, what's the performance on some other language pairs ?\\n\\nMaybe you should mention this paper,\\nhttps://arxiv.org/abs/1610.00072. It speeds up the decoding speed by 10x and the BLEU loss is less than 0.5.\\n\\nThe paper describes a parameter reducing method for large vocabulary softmax. By applying the error-corrected code and hybrid with softmax, its BLEU approaches that of the orignal full vocab softmax model.\\n\\nOne quick question: what is the hidden dimension size of the mo\"\n}",
        "human_end_index": 2898
    },
    {
        "id": 68,
        "source": "PeerRead/acl_2017/",
        "title": "A New Formula for Vietnamese Text Readability Assessment",
        "human_review": "- Strengths: The paper broadens the applicability of readability scores to an\nadditional language, and produces a well-validated applicability score for\nVietnamese. \n\n- Weaknesses: The greatest weaknesses, with respect to ACL are that 1)\nreadability scores are of limited interest within the field of computational\nlinguistics. While they are somewhat useful in educational and public\ncommunication fields, their impact on the progress of computational linguistics\nis limited.  A minor weakness is in the writing: the paper has numerous minor\ngrammatical errors.\nAlthough the discussion compares the performance of the PDS1 and PDW1 features\nfrom the previous work, it is unclear how poorly the previous readability\nmeasures perform, relevant to the one developed here, for practical purposes.\n\n- General Discussion: This paper would be a stronger candidate for inclusion if\nthe corpus (and importantly, labels developed) were released. It could be used\nmore widely than the development of scalar readability metrics, and would\nenable (e.g.) investigation of application of more powerful feature-selection\nmethods.\n- Strengths:\n New Dataset, \n NLP on Resource poor language\n\n- Weaknesses:\n Incomplete related work references, \n No comparison with recent methods and approaches, \n Lack of technical contribution, \n Weak experiments,\n\n- General Discussion:\n\nIn this paper the authors present a simple formula for readability assessment\nof Vietnamese Text. Using a combination of features such as word count,\nsentence length etc they train a simple regression model to estimate the\nreadability of the documents. \n\nOne of the major weaknesses of the paper its lack of technical contribution -\nwhile early work in readability assessment employed simple methods like the one\noutlined in this paper, recent work on predicting readability uses more robust\nmethods that rely on language models for instance (Eg :\nhttp://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf,\nhttp://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10\n-camera.pdf). A comparison with such methods could be a useful contribution and\nmake the paper stronger especially if simple methods such as those outlined in\nthis paper can compete with more complicated models. \n\nBaseline experiments with SMOG, Gunning Fog index etc should also be presented\nas well as the other Vietnamese metrics and datasets that the authors cite. \n\nAnother problem is that while previous readability indices were more selective\nand classified content into granular levels corresponding to grade levels (for\ninstance), the authors use a coarse classification scheme to label documents as\neasy, medium and hard which makes the metric uninteresting. (Also, why not use\na classifier?)\n\nThe work is probably a bit too pre-mature and suffers from significant\nweaknesses to be accepted at this stage. I would encourage the authors to\nincorporate suggested feedback to make it better. \n\nThe paper also has quite a few grammatical errors which should be addressed in\nany future submission.\nThe authors present a new formula for assessing readability of Vietnamese\ntexts. The formula is developed based on a multiple regression analysis with\nthree features. Furthermore, the authors have developed and annotated a new\ntext corpus with three readability classes (easy, middle, hard).\n\nResearch on languages other than English is interesting and important,\nespecially when it comes to low-resource languages. Therefore, the corpus might\nbe a nice additional resource for research (but it seems that the authors will\nnot publish it - is that right?). However, I don't think the paper is\nconvincing in its current shape or will influence future research. Here are my\nreasons:\n\n- The authors provide no reasons why there is a need for delevoping a new\nformula for readability assessments, given that there already exist two\nformulas for Vietnamese with almost the same features. What are the\ndisadvantages of those formulas and why is the new formula presented in this\npaper better?\n\n- In general, the experimental section lacks comparisons with previous work and\nanalysis of results. The authors claim that the accuracy of their formula (81%\non their corpus) is \"good and can be applied in practice\". What would be the\naccuracy of other formulas that already exist and what are the pros and cons of\nthose existing formulas compared to the new one?\n\n- As mentioned before, an analysis of results is missing, e.g. which word /\nsentence lengths / number of difficult words are considered as easy/middle/hard\nby their model?\n\n- A few examples how their formula could be applied in a practical application\nwould be nice as well.\n\n- The related work section is rather a \"background\" section since it only\npresents previously published formulas. What I'm missing is a more general\ndiscussion of related work. There are some papers that might be interesting for\nthat, e.g., DuBay 2004: \"The principles of readability\", or Rabin 1988:\n\"Determining difficulty levels of text written in languages other than English\"\n\n- Since Vietnamese is syllable-based and not word-based, I'm wondering how the\nauthors get \"words\" in their study. Do they use a particular approach for\nmerging syllables? And if yes, which approach do they use and what's the\naccuracy of the approach?\n\n- All in all, the content of the paper (experiments, comparisons, analysis,\ndiscussion, related work) is not enough for a long paper.\n\nAdditional remarks:\n\n- The language needs improvements\n\n- Equations: The usage of parentheses and multiplying operators is inconsistent\n\n- Related works section: The usage of capitalized first letters is inconsistent",
        "partial_review": "- Strengths: The paper broadens the applicability of readability scores to an\nadditional language, and produces a well-validated applicability score for\nVietnamese. \n\n- Weaknesses: The greatest weaknesses, with respect to ACL are that 1)\nreadability scores are of limited interest within the field of computational\nlinguistics. While they are somewhat useful in educational and public\ncommunication fields, their impact on the progress of computational linguistics\nis limited.  A minor weakness is in the writing: the paper has numerous minor\ngrammatical errors.\nAlthough the discussion compares the performance of the PDS1 and PDW1 features\nfrom the previous work, it is unclear how poorly the previous readability\nmeasures perform, relevant to the one developed here, for practical purposes.\n\n- General Discussion: This paper would be a stronger candidate for inclusion if\nthe corpus (and importantly, labels developed) were released. It could be used\nmore widely than the development of scalar readability metrics, and would\nenable (e.g.) investigation of application of more powerful feature-selection\nmethods.\n- Strengths:\n New Dataset, \n NLP on Resource poor language\n\n- Weaknesses:\n Incomplete related work references, \n No comparison with recent methods and approaches, \n Lack of technical contribution, \n Weak experiments,\n\n- General Discussion:\n\nIn this paper the authors present a simple formula for readability assessment\nof Vietnamese Text. Using a combination of features such as word count,\nsentence length etc they train a simple regression model to estimate the\nreadability of the documents. \n\nOne of the major weaknesses of the paper its lack of technical contribution -\nwhile early work in readability assessment employed simple methods like the one\noutlined in this paper, recent work on predicting readability uses more robust\nmethods that rely on language models for instance (Eg :\nhttp://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf,\nhttp://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10\n-camera.pdf). A comparison with such methods could be a useful contribution and\nmake the paper stronger especially if simple methods such as those outlined in\nthis paper can compete with more complicated models. \n\nBaseline experiments with SMOG, Gunning Fog index etc should also be presented\nas well as the other Vietnamese metrics and datasets that the authors cite. \n\nAnother problem is that while previous readability indices were more selective\nand classified content into granular levels corresponding to grade levels (for\ninstance), the authors use a coarse classification scheme to label documents as\neasy, medium and hard which makes the metric uninteresting. (Also, why not use\na classifier?)\n\nThe work is probably a bit too pre-mature and suffers from significant\nweaknesses to ",
        "task3_prompt": "\nComplete a partially written peer review about the paper: A New Formula for Vietnamese Text Readability Assessment \n\nHere is the abstract of the paper: Text readability has an important role in text drafting and document selecting. Researches on the readability of the text have been made long ago for English and some common languages. There are few researches in Vietnamese text readability and most of them are performed from more than two decades ago on very small corpora. In this paper, we build a new and larger corpus and use it to create a newer formula to predict the difficulty of Vietnamese text. The experimental results show that the new formula can predict the readability of Vietnamese documents with over 80% accuracy. \n\nHere is the partial review: - Strengths: The paper broadens the applicability of readability scores to an\nadditional language, and produces a well-validated applicability score for\nVietnamese. \n\n- Weaknesses: The greatest weaknesses, with respect to ACL are that 1)\nreadability scores are of limited interest within the field of computational\nlinguistics. While they are somewhat useful in educational and public\ncommunication fields, their impact on the progress of computational linguistics\nis limited.  A minor weakness is in the writing: the paper has numerous minor\ngrammatical errors.\nAlthough the discussion compares the performance of the PDS1 and PDW1 features\nfrom the previous work, it is unclear how poorly the previous readability\nmeasures perform, relevant to the one developed here, for practical purposes.\n\n- General Discussion: This paper would be a stronger candidate for inclusion if\nthe corpus (and importantly, labels developed) were released. It could be used\nmore widely than the development of scalar readability metrics, and would\nenable (e.g.) investigation of application of more powerful feature-selection\nmethods.\n- Strengths:\n New Dataset, \n NLP on Resource poor language\n\n- Weaknesses:\n Incomplete related work references, \n No comparison with recent methods and approaches, \n Lack of technical contribution, \n Weak experiments,\n\n- General Discussion:\n\nIn this paper the authors present a simple formula for readability assessment\nof Vietnamese Text. Using a combination of features such as word count,\nsentence length etc they train a simple regression model to estimate the\nreadability of the documents. \n\nOne of the major weaknesses of the paper its lack of technical contribution -\nwhile early work in readability assessment employed simple methods like the one\noutlined in this paper, recent work on predicting readability uses more robust\nmethods that rely on language models for instance (Eg :\nhttp://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf,\nhttp://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10\n-camera.pdf). A comparison with such methods could be a useful contribution and\nmake the paper stronger especially if simple methods such as those outlined in\nthis paper can compete with more complicated models. \n\nBaseline experiments with SMOG, Gunning Fog index etc should also be presented\nas well as the other Vietnamese metrics and datasets that the authors cite. \n\nAnother problem is that while previous readability indices were more selective\nand classified content into granular levels corresponding to grade levels (for\ninstance), the authors use a coarse classification scheme to label documents as\neasy, medium and hard which makes the metric uninteresting. (Also, why not use\na classifier?)\n\nThe work is probably a bit too pre-mature and suffers from significant\nweaknesses to  \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"Strengths: The paper broadens the applicability of readability scores to an additional language, and produces a well-validated applicability score for Vietnamese. The use of a new dataset for this purpose is also a strength. In addition, the paper addresses the lack of recent research in Vietnamese text readability. \\n\\nWeaknesses: In terms of technical contribution, the paper lacks novelty as it employs a simple regression model and does not compare with recent methods and approaches in readability assessment. The experiments are also considered weak, and the paper lacks discussion on the performance of previous readability measures. In terms of writing, there are numerous minor grammatical errors throughout the paper. Furthermore, the paper does not provide a comprehensive review of related work and fails to cite recent papers on readability assessment. It also uses a relatively coarse classification scheme for labeling documents, which limits the interest of the metric. Overall, the paper lacks depth and could benefit from addressing these weaknesses.\\n\\nGeneral Discussion: The authors could strengthen the paper by including a comparison with recent methods in readability assessment, as well as baseline experiments with other Vietnamese metrics and datasets mentioned in the paper. Additionally, releasing the corpus and labels developed would enhance the potential impact of the paper. This could enable further investigations and allow for the application of more powerful feature-selection methods. Overall, the paper has potential but needs significant improvements to be considered for inclusion.\"\n}",
        "human_end_index": 2830
    },
    {
        "id": 684,
        "source": "PeerRead/acl_2017/",
        "title": "Gated-Attention Readers for Text Comprehension",
        "human_review": "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to get it published in ACL.\n\n- The proposed GA mechanism looks promising, but not enough to convince the\nimportance of this technique over other state-of-the-art systems, because\nengineering tricks presented 3.1.4 boost a lot on accuracy and are blended in\nthe result.\n\n- Incomplete bibliography: Nearly all published work in reference section\nrefers arxiv preprint version. \nThis makes me (and future readers) suspicious if this work thoroughly compares\nwith prior work. Please make them complete if the published version is\navailable. \n\n- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned\nas previous work that is unpublished preprint. \nI don't think this is necessary at all. Alternately, I would like the author to\nreplace it with vanilla GA (or variant of the proposed model for baseline). \nIt doesn't make sense that result from the preprint which will end up being the\nsame as this ACL submission is presented in the same manuscript. \nFor fair blind-review, I didn't search on arvix archive though.\n\n- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,\nand GA (fix L(w)) is for K=3 in table 2. \nDoes this mean that GA-- is actually AS Reader? \nIt's not clear that GA-- is re-implementation of AS. \nI assumed K=1 (AS) in table 2 uses also GloVe initialization and\ntoken-attention, but it doesn't seem in GA--. \n\n- I wish the proposed method compared with prior work in related work section\n(i.e. what's differ from related work).\n\n- Fig 2 shows benefit of gated attention (which translates multi-hop\narchitecture), and it's very impressive. It would be great to see any\nqualitative example with comparison.\nThis paper presents an interesting model for reading comprehension, by\ndepicting the multiplicative interactions between the query and local\ninformation around a word in a document, and the authors proposed a new\ngated-attention strategy to characterize the relationship. The work is quite\nsolid, with almost state of art result on the whole four cloze-style datasets\nachieved. Some of the further improvement can be helpful for the similar tasks.\n\n\nNevertheless, I have some concerns on the following aspect:\n\n1. The authors have referred many papers from arXiv, but I think some really\nrelated works are not included. Such as the works from Caiming Xiong, et al.\nhttps://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et\nal. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on\nenhancing the attention operation to modeling the interaction between documents\nand queries. Although these works are not evaluated on the cloze-style corpus\nbut the SQuAD, an experimental or fundamental comparison may be necessary.\n\n2. There have been some studies that adopts attention mechanism or its variants\nspecially designed for the Reading Comprehension tasks, and the work actually\nshare the similar ideas with this paper. My suggestion is to conduct some\ncomparisons with such work to enhance the experiments of this paper.\n- Strengths:\n\n* Paper is very well-written and every aspect of the model is well-motivated\nand clearly explained.\n* The authors have extensively covered the previous work in the area.\n* The approach achieves state-of-the-art results across several text\ncomprehension data sets. In addition, the experimental evaluation is very\nthorough.\n\n- Weaknesses:\n\n* Different variants of the model achieve state-of-the-art performance across\nvarious data sets. However, the authors do provide an explanation for this\n(i.e. size of data set and text anonymization patterns).\n\n- General Discussion:\n\nThe paper describes an approach to text comprehension which uses gated\nattention modules to achieve state-of-the-art performance. Compared to previous\nattention mechanisms, the gated attention reader uses the query embedding and\nmakes multiple passes (multi-hop architecture) over the document and applies\nmultiplicative updates to the document token vectors before finally producing a\nclassification output regarding the answer. This technique somewhat mirrors how\nhumans solve text comprehension problems. Results show that the approach\nperforms well on large data sets such as CNN and Daily Mail. For the CBT data\nset, some additional feature engineering is needed to achieve state-of-the-art\nperformance. \n\nOverall, the paper is very well-written and model is novel and well-motivated.\nFurthermore, the approach achieves state-of-the-art performance on several data\nsets. \n\nI had only minor issues with the evaluation. The experimental results section\ndoes not mention whether the improvements (e.g. in Table 3) are statistically\nsignificant and if so, which test was used and what was the p-value. Also I\ncouldn't find an explanation for the performance on CBT-CN data set where the\nvalidation performance is superior to NSE but test performance is significantly\nworse.",
        "partial_review": "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to get it published in ACL.\n\n- The proposed GA mechanism looks promising, but not enough to convince the\nimportance of this technique over other state-of-the-art systems, because\nengineering tricks presented 3.1.4 boost a lot on accuracy and are blended in\nthe result.\n\n- Incomplete bibliography: Nearly all published work in reference section\nrefers arxiv preprint version. \nThis makes me (and future readers) suspicious if this work thoroughly compares\nwith prior work. Please make them complete if the published version is\navailable. \n\n- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned\nas previous work that is unpublished preprint. \nI don't think this is necessary at all. Alternately, I would like the author to\nreplace it with vanilla GA (or variant of the proposed model for baseline). \nIt doesn't make sense that result from the preprint which will end up being the\nsame as this ACL submission is presented in the same manuscript. \nFor fair blind-review, I didn't search on arvix archive though.\n\n- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,\nand GA (fix L(w)) is for K=3 in table 2. \nDoes this mean that GA-- is actually AS Reader? \nIt's not clear that GA-- is re-implementation of AS. \nI assumed K=1 (AS) in table 2 uses also GloVe initialization and\ntoken-attention, but it doesn't seem in GA--. \n\n- I wish the proposed method compared with prior work in related work section\n(i.e. what's differ from related work).\n\n- Fig 2 shows benefit of gated attention (which translates multi-hop\narchitecture), and it's very impressive. It would be great to see any\nqualitative example with comparison.\nThis paper presents an interesting model for reading comprehension, by\ndepicting the multiplicative interactions between the query and local\ninformation around a word in a document, and the authors proposed a new\ngated-attention strategy to characterize the relationship. The work is quite\nsolid, with almost state of art result on the whole four cloze-style datasets\nachieved. Some of the further improvement can be helpful for the similar tasks.\n\n\nNevertheless, I have some concerns on the following aspect:\n\n1. The authors have referred many papers from arXiv, but I think some really\nrelated works are not include",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Gated-Attention Readers for Text Comprehension \n\nHere is the abstract of the paper: In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. \n\nHere is the partial review: This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to get it published in ACL.\n\n- The proposed GA mechanism looks promising, but not enough to convince the\nimportance of this technique over other state-of-the-art systems, because\nengineering tricks presented 3.1.4 boost a lot on accuracy and are blended in\nthe result.\n\n- Incomplete bibliography: Nearly all published work in reference section\nrefers arxiv preprint version. \nThis makes me (and future readers) suspicious if this work thoroughly compares\nwith prior work. Please make them complete if the published version is\navailable. \n\n- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned\nas previous work that is unpublished preprint. \nI don't think this is necessary at all. Alternately, I would like the author to\nreplace it with vanilla GA (or variant of the proposed model for baseline). \nIt doesn't make sense that result from the preprint which will end up being the\nsame as this ACL submission is presented in the same manuscript. \nFor fair blind-review, I didn't search on arvix archive though.\n\n- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,\nand GA (fix L(w)) is for K=3 in table 2. \nDoes this mean that GA-- is actually AS Reader? \nIt's not clear that GA-- is re-implementation of AS. \nI assumed K=1 (AS) in table 2 uses also GloVe initialization and\ntoken-attention, but it doesn't seem in GA--. \n\n- I wish the proposed method compared with prior work in related work section\n(i.e. what's differ from related work).\n\n- Fig 2 shows benefit of gated attention (which translates multi-hop\narchitecture), and it's very impressive. It would be great to see any\nqualitative example with comparison.\nThis paper presents an interesting model for reading comprehension, by\ndepicting the multiplicative interactions between the query and local\ninformation around a word in a document, and the authors proposed a new\ngated-attention strategy to characterize the relationship. The work is quite\nsolid, with almost state of art result on the whole four cloze-style datasets\nachieved. Some of the further improvement can be helpful for the similar tasks.\n\n\nNevertheless, I have some concerns on the following aspect:\n\n1. The authors have referred many papers from arXiv, but I think some really\nrelated works are not include \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper presents a gated attention mechanism for machine reading that extends the Attention Sum Reader to multi-hop reasoning. The proposed GA mechanism shows promise, but the engineering tricks in Section 3.1.4 are not enough to convincingly demonstrate the importance of this technique over other state-of-the-art systems. Additionally, the incomplete bibliography raises suspicions about the thoroughness of the comparison with prior work. It is important to include complete references if published versions are available. The use of unpublished work as a baseline in Table 1 and Table 3 is unnecessary and can be replaced with a variant of the proposed model for a fair blind review. There is also a conflict regarding GA-- in Table 1 and K=1(AS) in Table 2, and it is not clear if GA-- is a re-implementation of AS Reader. The proposed method should also be compared to prior work in the related work section to establish its differences. While Figure 2 shows the benefits of gated attention, it would be helpful to include qualitative examples for comparison. Overall, the model is interesting and achieves impressive results but can be improved upon for similar tasks.\"\n}",
        "human_end_index": 2629
    },
    {
        "id": 691,
        "source": "PeerRead/acl_2017/",
        "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment",
        "human_review": "- Overview:\n\nThe paper proposes a new model for training sense embeddings grounded in a\nlexical-semantic resource (in this case WordNet). There is no direct evaluation\nthat the learned sense vectors are meaningful; instead, the sense vectors are\ncombined back into word embeddings, which are evaluated in a downstream task:\nPP attachment prediction.\n\n- Strengths:\n\nPP attachment results seem solid.\n\n- Weaknesses:\n\nWhether the sense embeddings are meaningful remains uninvestigated. \n\nThe probabilistic model has some details that are hard to understand. Are the\n\\lambda_w_i hyperparameters or trained? Where does \u201crank\u201d come from, is\nthis taken from the sense ranks in WordNet?\n\nRelated work: the idea of expressing embeddings of words as a convex\ncombination of sense embeddings has been proposed a number of times previously.\nFor instance, Johansson and Nieto Pi\u00f1a \u201cEmbedding a semantic network in a\nword space\u201d (NAACL, 2015) decomposed word embeddings into ontology-grounded\nsense embeddings based on this idea. Also in unsupervised sense vector training\nthis idea has been used, for instance by Arora et al \u201cLinear Algebraic\nStructure of Word Senses, with Applications to Polysemy\u201d.\n\nMinor comments:\n\nno need to define types and tokens, this is standard terminology\n\nwhy is the first \\lamba_w_i in equation 4 needed if the probability is\nunnormalized?\n\n- General Discussion:",
        "partial_review": "- Overview:\n\nThe paper proposes a new model for training sense embeddings grounded in a\nlexical-semantic resource (in this case WordNet). There is no direct evaluation\nthat the learned sense vectors are meaningful; instead, the sense vectors are\ncombined back into word embeddings, which are evaluated in a downstream task:\nPP attachment prediction.\n\n- Strengths:\n\nPP attachment results seem solid.\n\n- Weaknesses:\n\nWhether the sense embeddings are meaningful remains uninvestigated. \n\nThe probabilistic model has some details that are hard to understand. Are the\n\\lambda_w_i hyperparameters or trained? Where does \u201crank\u201d come from, is\nthis taken from the sense ranks i",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment \n\nHere is the abstract of the paper: Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors. \n\nHere is the partial review: - Overview:\n\nThe paper proposes a new model for training sense embeddings grounded in a\nlexical-semantic resource (in this case WordNet). There is no direct evaluation\nthat the learned sense vectors are meaningful; instead, the sense vectors are\ncombined back into word embeddings, which are evaluated in a downstream task:\nPP attachment prediction.\n\n- Strengths:\n\nPP attachment results seem solid.\n\n- Weaknesses:\n\nWhether the sense embeddings are meaningful remains uninvestigated. \n\nThe probabilistic model has some details that are hard to understand. Are the\n\\lambda_w_i hyperparameters or trained? Where does \u201crank\u201d come from, is\nthis taken from the sense ranks i \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"The paper presents a novel approach to training sense embeddings using WordNet and applies them to improve the accuracy of prepositional phrase (PP) attachment prediction. The results for the PP attachment task are solid, indicating the effectiveness of the proposed model. However, the meaningfulness of the learned sense vectors is not investigated directly, which is a potential weakness of the paper. Additionally, the probabilistic model used in the approach lacks clarity in certain aspects. It is unclear whether the \\lambda_w_i hyperparameters are pre-defined or trained, and the origin of the 'rank' parameter is not explained. Overall, the paper provides valuable insights into ontology-aware token embeddings, but further investigation and clarification are needed in certain areas.\"\n}",
        "human_end_index": 668
    },
    {
        "id": "699",
        "source": "PeerRead/acl_2017/",
        "title": "Deep Keyphrase Generation",
        "human_review": "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed model outperforms other\nbaselines if supervised data is available.\n\n- Strengths:\nThe paper is well-organized and easy to follow (the intuition of the proposed\nmethod is clear). It includes enough details to replicate experiments. Although\nthe application of an encoder-decoder (+ copy mechanism) is straightforward,\nexperimental results are reasonable and support the claim (generation of absent\nkeyphrases) presented in this paper.\n\n- Weaknesses:\nAs said above, there is little surprise in the proposed approach. Also, as\ndescribed in Section 5.3, the trained model does not transfer well to new\ndomain (it goes below unsupervised models). One of the contribution of this\npaper is to maintain training corpora in good quantity and quality, but it is\nnot (explicitly) stated.\n\n- General Discussion:\nI like to read the paper and would be pleased to see it accepted. I would like\nto know how the training corpus (size and variation) affects the performance of\nthe proposed method. Also, it would be beneficial to see the actual values of\np_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my\nexperience in running the CopyNet, the copying mechanism sometimes works\nunexpectedly (not sure why this happens).\nThis paper divides the keyphrases into two types: (1) Absent key phrases (such\nphrases do not match any contiguous subsequences of the source document) and\n(2) Present key phrases (such key phrases fully match a part of the text). The\nauthors used RNN based generative models (discussed as RNN and Copy RNN) for\nkeyphrase prediction and copy mechanism in RNN to predict the already occurred\nphrases. \n\nStrengths:\n\n1. The formation and extraction of key phrases, which are absent in the current\ndocument is an interesting idea of significant research interests. \n\n2. The paper is easily understandable.\n\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep\nrecurrent neural networks are already used in keyphrase extraction (shows very\ngood performance also), so, it will be interesting to have a proper motivation\nto justify the use of  RNN and Copy RNN over deep recurrent neural networks. \n\nWeaknesses:\n\n1. Some discussions are required on the convergence of the proposed joint\nlearning process (for RNN and CopyRNN), so that readers can understand, how the\nstable points in probabilistic metric space are obtained? Otherwise, it may be\ntough to repeat the results.\n\n2. The evaluation process shows that the current system (which extracts 1.\nPresent and 2. Absent both kinds of keyphrases) is evaluated against baselines\n(which contains only \"present\" type of keyphrases). Here there is no direct\ncomparison of the performance of the current system w.r.t. other\nstate-of-the-arts/benchmark systems on only \"present\" type of key phrases. It\nis important to note that local phrases (keyphrases) are also important for the\ndocument. The experiment does not discuss it explicitly. It will be interesting\nto see the impact of the RNN and Copy RNN based model on automatic extraction\nof local or \"present\" type of key phrases.\n\n3. The impact of document size in keyphrase extraction is also an important\npoint. It is found that the published results of [1], (see reference below)\nperforms better than (with a sufficiently high difference) the current system\non Inspec (Hulth, 2003) abstracts dataset. \n\n4. It is reported that current system uses 527,830 documents for training,\nwhile 40,000 publications are held out for training baselines. Why are all\npublications not used in training the baselines? Additionally,        The topical\ndetails of the dataset (527,830 scientific documents) used in training RNN and\nCopy RNN are also missing. This may affect the chances of repeating results.\n\n5. As the current system captures the semantics through RNN based models. So,\nit would be better to compare this system, which also captures semantics. Even,\nRef-[2] can be a strong baseline to compare the performance of the current\nsystem.\n\nSuggestions to improve:\n\n1. As, per the example, given in the Figure-1, it seems that all the \"absent\"\ntype of key phrases are actually \"Topical phrases\". For example: \"video\nsearch\", \"video retrieval\", \"video indexing\" and \"relevance ranking\", etc.\nThese all define the domain/sub-domain/topics of the document. So, In this\ncase, it will be interesting to see the results (or will be helpful in\nevaluating \"absent type\" keyphrases): if we identify all the topical phrases of\nthe entire corpus by using tf-idf and relate the document to the high-ranked\nextracted topical phrases (by using Normalized Google Distance, PMI, etc.). As\nsimilar efforts are already applied in several query expansion techniques (with\nthe aim to relate the document with the query, if matching terms are absent in\ndocument).\n\nReference:\n1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to\nfind exemplar terms for keyphrase extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Language Processing, pages\n257\u2013266.\n\n2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction\nusing deep recurrent neural networks on Twitter. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing (pp. 836-845).\n- Strengths:\n\nNovel model.  I particularly like the ability to generate keyphrases not\npresent in the source text.\n\n- Weaknesses:\n\n Needs to be explicit whether all evaluated models are trained and tested on\nthe same data sets.  Exposition of the copy mechanism not quite\nclear/convincing.\n\n- General Discussion:\n\nThis paper presents a supervised neural network approach for keyphrase\ngeneration.  The model uses an encoder-decoder architecture that\nfirst encodes input text with a RNN, then uses an attention mechanism to\ngenerate keyphrases from\nthe hidden states.  There is also a more advanced variant of the\ndecoder which has an attention mechanism that conditions on the\nkeyphrase generated in the previous time step.\n\nThe model is interesting and novel. And I think the ability to\ngenerate keyphrases not in the source text is particularly\nappealing.  My main concern is with the evaluation:  Are all\nevaluated models trained with the same amount of data and evaluated\non the same test sets?              It's not very clear.  For example, on the\nNUS data set, Section 4.2 line 464 says that the supervised baselines\nare evaluated with cross validation.\n\nOther comments:\n\nThe paper is mostly clearly written and easy to follow.  However,\nsome parts are unclear:\n\n- Absent keyphrases vs OOV.  I think there is a need to distinguish\n  between the two, and the usage meaning of OOV should be consistent.  The RNN\nmodels\n  use the most frequent 50000 words as the vocabulary (Section 3.4\n  line 372, Section 5.1 line 568), so I suppose OOV are words not in\n  this 50K vocabulary.              In line 568, do you mean OOV or absent\n  words/keyphrases?  Speaking of this, I'm wondering how many\n  keyphrases fall outside of this 50K?              The use of \"unknown words\"\n  in line 380 is also ambiguous.  I think it's probably clearer to say that\n the RNN models can generate words not present in the source text as long as\nthey appear\nsomewhere else in the corpus (and the 50K vocabulary)\n\n- Exposition of the copy mechanism (section 3.4).  This mechanism has a\n  more specific locality than the attention model in basic RNN model.\n  However, I find the explanation of the intuition misleading.              If I\n  understand correctly, the \"copy mechanism\" is conditioned on the\n  source text locations that matches the keyphrase in the previous\n  time step y_{t-1}.  So maybe it has a higher tendency to generate n-grams\nseen source text (Figure 1).  I buy the argument that the more sophisticated\n  attention model probably makes CopyRNN better than the RNN\n  overall, but why is the former model particularly better for absent\n  keyphrases?  It is as if both models perform equally well on present\nkeyphrases.\n\n- How are the word embeddings initialized?",
        "partial_review": "This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed model outperforms other\nbaselines if supervised data is available.\n\n- Strengths:\nThe paper is well-organized and easy to follow (the intuition of the proposed\nmethod is clear). It includes enough details to replicate experiments. Although\nthe application of an encoder-decoder (+ copy mechanism) is straightforward,\nexperimental results are reasonable and support the claim (generation of absent\nkeyphrases) presented in this paper.\n\n- Weaknesses:\nAs said above, there is little surprise in the proposed approach. Also, as\ndescribed in Section 5.3, the trained model does not transfer well to new\ndomain (it goes below unsupervised models). One of the contribution of this\npaper is to maintain training corpora in good quantity and quality, but it is\nnot (explicitly) stated.\n\n- General Discussion:\nI like to read the paper and would be pleased to see it accepted. I would like\nto know how the training corpus (size and variation) affects the performance of\nthe proposed method. Also, it would be beneficial to see the actual values of\np_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my\nexperience in running the CopyNet, the copying mechanism sometimes works\nunexpectedly (not sure why this happens).\nThis paper divides the keyphrases into two types: (1) Absent key phrases (such\nphrases do not match any contiguous subsequences of the source document) and\n(2) Present key phrases (such key phrases fully match a part of the text). The\nauthors used RNN based generative models (discussed as RNN and Copy RNN) for\nkeyphrase prediction and copy mechanism in RNN to predict the already occurred\nphrases. \n\nStrengths:\n\n1. The formation and extraction of key phrases, which are absent in the current\ndocument is an interesting idea of significant research interests. \n\n2. The paper is easily understandable.\n\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep\nrecurrent neural networks are already used in keyphrase extraction (shows very\ngood performance also), so, it will be interesting to have a proper motivation\nto justify the use of  RNN and Copy RNN over deep recurrent neural networks. \n\nWeaknesses:\n\n1. Some discussions are required on the convergence of the proposed joint\nlearning process (for RNN and CopyRNN), so that readers can understand, how the\nstable points in probabilistic metric space are obtained? Otherwise, it may be\ntough to repeat the results.\n\n2. The evaluation process shows that the current system (which extracts 1.\nPresent and 2. Absent both kinds of keyphrases) is evaluated against baselines\n(which contains only \"present\" type of keyphrases). Here there is no direct\ncomparison of the performance of the current system w.r.t. other\nstate-of-the-arts/benchmark systems on only \"present\" type of key phrases. It\nis important to note that local phrases (keyphrases) are also important for the\ndocument. The experiment does not discuss it explicitly. It will be interesting\nto see the impact of the RNN and Copy RNN based model on automatic extraction\nof local or \"present\" type of key phrases.\n\n3. The impact of document size in keyphrase extraction is also an important\npoint. It is found that the published results of [1], (see reference below)\nperforms better than (with a sufficiently high difference) the current system\non Inspec (Hulth, 2003) abstracts dataset. \n\n4. It is reported that current system uses 527,830 documents for training,\nwhile 40,000 publications are held out for training baselines. Why are all\npublications not used in training the baselines? Additionally,        The topical\ndetails of the dataset (527,830 scientific documents) used in training RNN and\nCopy RNN are also missing. This may affect the chances of repeating results.\n\n5. As the current system captures the semantics through RNN based models. So,\nit would be better to compare this system, which also captures semantics. Even,\nRef-[2] can be a strong baseline to compare the pe",
        "task3_prompt": "\nComplete a partially written peer review about the paper: Deep Keyphrase Generation \n\nHere is the abstract of the paper: Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks.  We name it as \\textit{deep keyphrase generation} since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase. \n\nHere is the partial review: This paper proposes to use an encoder-decoder framework for keyphrase\ngeneration. Experimental results show that the proposed model outperforms other\nbaselines if supervised data is available.\n\n- Strengths:\nThe paper is well-organized and easy to follow (the intuition of the proposed\nmethod is clear). It includes enough details to replicate experiments. Although\nthe application of an encoder-decoder (+ copy mechanism) is straightforward,\nexperimental results are reasonable and support the claim (generation of absent\nkeyphrases) presented in this paper.\n\n- Weaknesses:\nAs said above, there is little surprise in the proposed approach. Also, as\ndescribed in Section 5.3, the trained model does not transfer well to new\ndomain (it goes below unsupervised models). One of the contribution of this\npaper is to maintain training corpora in good quantity and quality, but it is\nnot (explicitly) stated.\n\n- General Discussion:\nI like to read the paper and would be pleased to see it accepted. I would like\nto know how the training corpus (size and variation) affects the performance of\nthe proposed method. Also, it would be beneficial to see the actual values of\np_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my\nexperience in running the CopyNet, the copying mechanism sometimes works\nunexpectedly (not sure why this happens).\nThis paper divides the keyphrases into two types: (1) Absent key phrases (such\nphrases do not match any contiguous subsequences of the source document) and\n(2) Present key phrases (such key phrases fully match a part of the text). The\nauthors used RNN based generative models (discussed as RNN and Copy RNN) for\nkeyphrase prediction and copy mechanism in RNN to predict the already occurred\nphrases. \n\nStrengths:\n\n1. The formation and extraction of key phrases, which are absent in the current\ndocument is an interesting idea of significant research interests. \n\n2. The paper is easily understandable.\n\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep\nrecurrent neural networks are already used in keyphrase extraction (shows very\ngood performance also), so, it will be interesting to have a proper motivation\nto justify the use of  RNN and Copy RNN over deep recurrent neural networks. \n\nWeaknesses:\n\n1. Some discussions are required on the convergence of the proposed joint\nlearning process (for RNN and CopyRNN), so that readers can understand, how the\nstable points in probabilistic metric space are obtained? Otherwise, it may be\ntough to repeat the results.\n\n2. The evaluation process shows that the current system (which extracts 1.\nPresent and 2. Absent both kinds of keyphrases) is evaluated against baselines\n(which contains only \"present\" type of keyphrases). Here there is no direct\ncomparison of the performance of the current system w.r.t. other\nstate-of-the-arts/benchmark systems on only \"present\" type of key phrases. It\nis important to note that local phrases (keyphrases) are also important for the\ndocument. The experiment does not discuss it explicitly. It will be interesting\nto see the impact of the RNN and Copy RNN based model on automatic extraction\nof local or \"present\" type of key phrases.\n\n3. The impact of document size in keyphrase extraction is also an important\npoint. It is found that the published results of [1], (see reference below)\nperforms better than (with a sufficiently high difference) the current system\non Inspec (Hulth, 2003) abstracts dataset. \n\n4. It is reported that current system uses 527,830 documents for training,\nwhile 40,000 publications are held out for training baselines. Why are all\npublications not used in training the baselines? Additionally,        The topical\ndetails of the dataset (527,830 scientific documents) used in training RNN and\nCopy RNN are also missing. This may affect the chances of repeating results.\n\n5. As the current system captures the semantics through RNN based models. So,\nit would be better to compare this system, which also captures semantics. Even,\nRef-[2] can be a strong baseline to compare the pe \n\nOutput in the following json format:\n{\n    \"complete_review\": \"\"\n}\n\n\n",
        "complete_review": "{\n    \"complete_review\": \"This paper proposes to use an encoder-decoder framework for keyphrase generation. Experimental results show that the proposed model outperforms other baselines if supervised data is available.\\n\\nStrengths:\\nThe paper is well-organized and easy to follow (the intuition of the proposed method is clear). It includes enough details to replicate experiments. Although the application of an encoder-decoder (+ copy mechanism) is straightforward, experimental results are reasonable and support the claim (generation of absent keyphrases) presented in this paper.\\n\\nWeaknesses:\\nAs said above, there is little surprise in the proposed approach. Also, as described in Section 5.3, the trained model does not transfer well to new domain (it goes below unsupervised models). One of the contribution of this paper is to maintain training corpora in good quantity and quality, but it is not (explicitly) stated.\\n\\nGeneral Discussion:\\nI like to read the paper and would be pleased to see it accepted. I would like to know how the training corpus (size and variation) affects the performance of the proposed method. Also, it would be beneficial to see the actual values of p_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my experience in running the CopyNet, the copying mechanism sometimes works unexpectedly (not sure why this happens).\\nThis paper divides the keyphrases into two types: (1) Absent key phrases (such phrases do not match any contiguous subsequences of the source document) and (2) Present key phrases (such key phrases fully match a part of the text). The authors used RNN based generative models (discussed as RNN and Copy RNN) for keyphrase prediction and copy mechanism in RNN to predict the already occurred phrases.\\n\\nStrengths:\\n\\n1. The formation and extraction of key phrases, which are absent in the current document is an interesting idea of significant research interests. \\n\\n2. The paper is easily understandable.\\n\\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep recurrent neural networks are already used in keyphrase extraction (shows very good performance also), so, it will be interesting to have a proper motivation to justify the use of  RNN and Copy RNN over deep recurrent neural networks. \\n\\nWeaknesses:\\n\\n1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\\n\\n2. The evaluation process shows that the current system (which extracts 1. Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only \\\"present\\\" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only \\\"present\\\" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or \\\"present\\\" type of key phrases.\\n\\n3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. \\n\\n4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally,        The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.\\n\\n5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the pe \"\n}",
        "human_end_index": 4074
    }
]